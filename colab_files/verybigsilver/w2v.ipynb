{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILaOeXq1_aIF",
        "outputId": "f2b377cf-6e34-4f18-b024-622ad617fabb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/fortmp/w2v\n",
            "현재 작업 위치: /content/drive/MyDrive/fortmp/w2v\n"
          ]
        }
      ],
      "source": [
        "# 1. 구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. 작업 폴더 생성 및 이동 (VUDENC 프로젝트를 저장할 위치)\n",
        "# 경로에 본인의 구글 드라이브 ID나 원하는 폴더명을 넣으셔도 됩니다.\n",
        "import os\n",
        "workspace_path = '/content/drive/MyDrive/fortmp/w2v'\n",
        "os.makedirs(workspace_path, exist_ok=True)\n",
        "%cd {workspace_path}\n",
        "\n",
        "print(f\"현재 작업 위치: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. VUDENC GitHub 저장소 클론 (이미 있다면 실행 X)\n",
        "if not os.path.exists('VulnerabilityDetection'):\n",
        "    !git clone https://github.com/LauraWartschinski/VulnerabilityDetection.git\n",
        "else:\n",
        "    print(\"VulnerabilityDetection 폴더가 이미 존재합니다.\")\n",
        "\n",
        "# 2. 프로젝트 폴더로 이동\n",
        "%cd VulnerabilityDetection\n",
        "\n",
        "# 3. 필요 라이브러리 설치\n",
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5F3k4eg_bbL",
        "outputId": "7e5f31fc-9885-4b20-96b4-d8d0eb40ee30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VulnerabilityDetection 폴더가 이미 존재합니다.\n",
            "/content/drive/MyDrive/fortmp/w2v/VulnerabilityDetection\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjFhx29HCNM-",
        "outputId": "c0644dc7-b105-4779-bd61-938b9ce63d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/fortmp/w2v/VulnerabilityDetection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 훈련 스크립트가 있는 폴더로 이동\n",
        "%cd Code\n",
        "\n",
        "# 2. Zenodo에서 토큰화된 코퍼스 zip 파일 다운로드\n",
        "# (pythontraining_withString_X.py.zip 파일, 약 1.4GB)\n",
        "# !wget \"https://zenodo.org/record/3732235/files/pythontraining_withString_X.py.zip?download=1\" -O pythontraining_withString_X.py.zip\n",
        "\n",
        "# 로컬에 있는 압축 파일 경로\n",
        "local_rar_path = '/content/drive/MyDrive/fortmp/w2v/VulnerabilityDetection/Code/w2v/pythontraining_withString.rar'\n",
        "output_dir = '/content/drive/MyDrive/fortmp/w2v/VulnerabilityDetection/Code/w2v' # 압축 해제할 경로\n",
        "\n",
        "# 3. 다운로드한 zip 파일 압축 해제 (rar 파일이므로 unrar 사용)\n",
        "# unrar 명령어가 설치되어 있지 않다면 설치 필요: !apt-get install unrar\n",
        "# 압축 해제 전, unrar 설치 여부 확인 및 설치\n",
        "!apt-get update && apt-get install -y unrar\n",
        "\n",
        "# 압축 해제 실행 (대상 경로 지정 -t 옵션)\n",
        "# -x 옵션은 경로를 유지하면서 압축을 해제합니다.\n",
        "!unrar x {local_rar_path} {output_dir}\n",
        "\n",
        "print(\"\\n토큰화된 코퍼스 준비 완료!\")\n",
        "!ls -lh {output_dir} # 파일 목록과 크기 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGqe9dmA_lZe",
        "outputId": "21bdb6ce-6141-42f7-9659-4450a8084125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/fortmp/w2v/VulnerabilityDetection/Code\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Fetched 257 kB in 22s (11.5 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unrar is already the newest version (1:6.1.5-1ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "\n",
            "UNRAR 6.11 beta 1 freeware      Copyright (c) 1993-2022 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/drive/MyDrive/fortmp/w2v/VulnerabilityDetection/Code/w2v/pythontraining_withString.rar\n",
            "\n",
            "\n",
            "Would you like to replace the existing file /content/drive/MyDrive/fortmp/w2v/VulnerabilityDetection/Code/w2v/pythontraining_withString_X\n",
            "337300692 bytes, modified on 2019-10-23 00:41\n",
            "with a new one\n",
            "337300692 bytes, modified on 2019-10-23 00:41\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit ㅛ\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit Y\n",
            "\n",
            "Extracting  /content/drive/MyDrive/fortmp/w2v/VulnerabilityDetection/Code/w2v/pythontraining_withString_X     \b\b\b\b  0%\b\b\b\b  1%\b\b\b\b  2%\b\b\b\b  3%\b\b\b\b  4%\b\b\b\b  5%\b\b\b\b  6%\b\b\b\b  7%\b\b\b\b  8%\b\b\b\b  9%\b\b\b\b 10%\b\b\b\b 11%\b\b\b\b 12%\b\b\b\b 13%\b\b\b\b 14%\b\b\b\b 15%\b\b\b\b 16%\b\b\b\b 17%\b\b\b\b 18%\b\b\b\b 19%\b\b\b\b 20%\b\b\b\b 21%\b\b\b\b 22%\b\b\b\b 23%\b\b\b\b 24%\b\b\b\b 25%\b\b\b\b 26%\b\b\b\b 27%\b\b\b\b 28%\b\b\b\b 29%\b\b\b\b 30%\b\b\b\b 31%\b\b\b\b 32%\b\b\b\b 33%\b\b\b\b 34%\b\b\b\b 35%\b\b\b\b 36%\b\b\b\b 37%\b\b\b\b 38%\b\b\b\b 39%\b\b\b\b 40%\b\b\b\b 41%\b\b\b\b 42%\b\b\b\b 43%\b\b\b\b 44%\b\b\b\b 45%\b\b\b\b 46%\b\b\b\b 47%\b\b\b\b 48%\b\b\b\b 49%\b\b\b\b 50%\b\b\b\b 51%\b\b\b\b 52%\b\b\b\b 53%\b\b\b\b 54%\b\b\b\b 55%\b\b\b\b 56%\b\b\b\b 57%\b\b\b\b 58%\b\b\b\b 59%\b\b\b\b 60%\b\b\b\b 61%\b\b\b\b 62%\b\b\b\b 63%\b\b\b\b 64%\b\b\b\b 65%\b\b\b\b 66%\b\b\b\b 67%\b\b\b\b 68%\b\b\b\b 69%\b\b\b\b 70%\b\b\b\b 71%\b\b\b\b 72%\b\b\b\b 73%\b\b\b\b 74%\b\b\b\b 75%\b\b\b\b 76%\b\b\b\b 77%\b\b\b\b 78%\b\b\b\b 79%\b\b\b\b 80%\b\b\b\b 81%\b\b\b\b 82%\b\b\b\b 83%\b\b\b\b 84%\b\b\b\b 85%\b\b\b\b 86%\b\b\b\b 87%\b\b\b\b 88%\b\b\b\b 89%\b\b\b\b 90%\b\b\b\b 91%\b\b\b\b 92%\b\b\b\b 93%\b\b\b\b 94%\b\b\b\b 95%\b\b\b\b 96%\b\b\b\b 97%\b\b\b\b 98%\b\b\b\b 99%\b\b\b\b100%\b\b\b\b\b  OK \n",
            "All OK\n",
            "\n",
            "토큰화된 코퍼스 준비 완료!\n",
            "total 531M\n",
            "-rw------- 1 root root  52M Sep  5 08:29 othermodels.rar\n",
            "-rw------- 1 root root  95M Sep  5 08:29 python-corpus.rar\n",
            "-rw------- 1 root root  23M Sep  5 08:29 pythontraining-withoutString.rar\n",
            "-rw------- 1 root root  32M Sep  5 08:29 pythontraining_withString.rar\n",
            "-rw------- 1 root root 322M Oct 23  2019 pythontraining_withString_X\n",
            "-rw------- 1 root root 8.7M Sep  5 08:29 word2vec_withString10-300-200.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYrUDp6DDt5c",
        "outputId": "44391723-9e61-436e-a8fd-ec3d7baa06c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/fortmp/w2v/VulnerabilityDetection/Code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9367a906"
      },
      "source": [
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab/english/')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "    print(\"NLTK 'punkt_tab' 리소스 다운로드 완료.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import os.path\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "all_words = []\n",
        "\n",
        "mode = \"withString\" #default\n",
        "\n",
        "\n",
        "\n",
        "# Loading the training corpus\n",
        "print(\"Loading \" + mode)\n",
        "with open('w2v/pythontraining' + '_'+mode+\"_X\", 'r') as file:\n",
        "    pythondata = file.read().lower().replace('\\n', ' ')\n",
        "\n",
        "print(\"Length of the training file: \" + str(len(pythondata)) + \".\")\n",
        "print(\"It contains \" + str(pythondata.count(\" \")) + \" individual code tokens.\")\n",
        "\n",
        "# Preparing the dataset (or loading already processed dataset to not do everything again)\n",
        "if (os.path.isfile('data/pythontraining_processed_' + mode)):\n",
        "  with open ('data/pythontraining_processed_' + mode, 'rb') as fp:\n",
        "    all_words = pickle.load(fp)\n",
        "  print(\"loaded processed model.\")\n",
        "else:\n",
        "  print(\"now processing...\")\n",
        "  processed = pythondata\n",
        "  all_sentences = nltk.sent_tokenize(processed)\n",
        "  all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
        "  print(\"saving\")\n",
        "  with open('data/pythontraining_processed_' + mode, 'wb') as fp:\n",
        "    pickle.dump(all_words, fp)\n",
        "\n",
        "print(\"processed.\\n\")\n",
        "\n",
        "# for 반복문 시작 직전에 이 코드를 추가하여 all_words 변수를 확인합니다.\n",
        "\n",
        "print(\"\\n--- [진단 시작] all_words 변수 상태 확인 ---\")\n",
        "print(f\"'all_words' 변수의 타입: {type(all_words)}\")\n",
        "\n",
        "if isinstance(all_words, list) and len(all_words) > 0:\n",
        "    print(f\"인식된 문장의 총 개수: {len(all_words)}\")\n",
        "    print(f\"첫 번째 문장의 일부 (토큰 10개): {all_words[0][:10]}\")\n",
        "    print(\"진단 결과: 'all_words'가 정상적으로 보입니다.\")\n",
        "else:\n",
        "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "    print(\"!!! [문제 발견] 'all_words' 변수가 비어있거나 리스트가 아닙니다. !!!\")\n",
        "    print(\"!!! Word2Vec 모델을 만들 재료가 없어 .npy 파일이 생성되지 않은 것 같습니다. !!!\")\n",
        "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "\n",
        "print(\"--- [진단 종료] ---\\n\")\n",
        "\n",
        "\n",
        "# 원래 코드 시작\n",
        "#trying out different parameters\n",
        "for mincount in [10]:\n",
        "  for iterationen in [6]:\n",
        "    for s in [100]:\n",
        "\n",
        "      print(\"\\n\\n\" + mode + \" W2V model with min count \" + str(mincount) + \" and \" + str(iterationen) + \" Iterationen and size \" + str(s))\n",
        "      fname = \"w2v/word2vec_\"+mode+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s)+ \".model\"\n",
        "\n",
        "      if (os.path.isfile(fname)):\n",
        "        print(\"model already exists.\")\n",
        "        continue\n",
        "\n",
        "      else:\n",
        "        print(\"calculating model...\")\n",
        "        # training the model\n",
        "        model = Word2Vec(all_words, vector_size=s, min_count=mincount, epochs=iterationen, workers = 4)\n",
        "        #vocabulary = model.wv.key_to_index\n",
        "\n",
        "        #print some examples\n",
        "\n",
        "        #words = [\"import\", \"true\", \"while\", \"if\", \"try\", \"in\", \"+\", \"x\", \"=\", \":\", \"[\", \"print\", \"str\", \"count\", \"len\", \"where\", \"join\", \"split\", \"==\", \"raw_input\"]\n",
        "        #for similar in words:\n",
        "        #  try:\n",
        "        #    print(\"\\n\")\n",
        "        #    print(similar)\n",
        "        #    sim_words = model.wv.most_similar(similar)\n",
        "        #    print(sim_words)\n",
        "        #    print(\"\\n\")\n",
        "        #  except Exception as e:\n",
        "        #    print(e)\n",
        "        #    print(\"\\n\")\n",
        "\n",
        "        #saving the model\n",
        "        model.save(fname)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSPigW-f_oke",
        "outputId": "7a15529d-97a0-4f79-aa15-d76fbebccea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading withString\n",
            "Length of the training file: 337300692.\n",
            "It contains 78328049 individual code tokens.\n",
            "loaded processed model.\n",
            "processed.\n",
            "\n",
            "\n",
            "--- [진단 시작] all_words 변수 상태 확인 ---\n",
            "'all_words' 변수의 타입: <class 'list'>\n",
            "인식된 문장의 총 개수: 3769517\n",
            "첫 번째 문장의 일부 (토큰 10개): ['__revision__', '=', '``', '$', 'id', '$', \"''\", 'distutils_all', '=', '[']\n",
            "진단 결과: 'all_words'가 정상적으로 보입니다.\n",
            "--- [진단 종료] ---\n",
            "\n",
            "\n",
            "\n",
            "withString W2V model with min count 10 and 6 Iterationen and size 100\n",
            "calculating model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import os\n",
        "\n",
        "# 모델 파일 경로 (pSPigW-f_oke 셀에서 생성된 파일명 사용)\n",
        "# 모델 훈련이 완료되지 않았다면 FileNotFoundError가 발생할 수 있습니다.\n",
        "model_fname = \"w2v/word2vec_withString10-6-100.model\"\n",
        "model_path = os.path.join(os.getcwd(), model_fname)\n",
        "\n",
        "\n",
        "print(f\"모델 파일 경로: {model_path}\")\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    try:\n",
        "        # Word2Vec 모델 로드\n",
        "        model = Word2Vec.load(model_path)\n",
        "        print(\"\\n--- 모델 테스트 ---\")\n",
        "\n",
        "        # 테스트 단어 목록\n",
        "        test_words = [\"import\", \"true\", \"while\", \"if\", \"try\", \"in\", \"+\", \"=\", \"[\", \"print\", \"str\", \"count\", \"len\", \"split\", \"==\"]\n",
        "\n",
        "        for word in test_words:\n",
        "            try:\n",
        "                if word in model.wv:\n",
        "                    print(f\"\\n'{word}'와(과) 유사한 단어:\")\n",
        "                    sim_words = model.wv.most_similar(word, topn=5)\n",
        "                    for sim_word, score in sim_words:\n",
        "                        print(f\"  - {sim_word}: {score:.4f}\")\n",
        "                else:\n",
        "                    print(f\"'{word}'는(은) 모델의 어휘에 없습니다.\")\n",
        "            except Exception as e:\n",
        "                print(f\"'{word}' 테스트 중 오류 발생: {e}\")\n",
        "\n",
        "        print(\"\\n--- 모델 테스트 완료 ---\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n모델 로드 중 오류 발생: {e}\")\n",
        "else:\n",
        "    print(\"\\n[오류] 모델 파일을 찾을 수 없습니다.\")\n",
        "    print(\"Word2Vec 모델 훈련이 완료되었는지 확인해주세요.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OezdNfJytnDk",
        "outputId": "57f41992-abb6-43fe-873e-221ef4c78145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델 파일 경로: /content/drive/MyDrive/fortmp/w2v/VulnerabilityDetection/Code/w2v/word2vec_withString10-6-100.model\n",
            "\n",
            "--- 모델 테스트 ---\n",
            "\n",
            "'import'와(과) 유사한 단어:\n",
            "  - from: 0.5426\n",
            "  - werkzeug: 0.5283\n",
            "  - __all__: 0.5145\n",
            "  - print_function: 0.5143\n",
            "  - pyasn1: 0.4988\n",
            "\n",
            "'true'와(과) 유사한 단어:\n",
            "  - false: 0.9104\n",
            "  - ,: 0.5298\n",
            "  - none: 0.5112\n",
            "  - ): 0.4936\n",
            "  - :: 0.4891\n",
            "\n",
            "'while'와(과) 유사한 단어:\n",
            "  - break: 0.6164\n",
            "  - -=: 0.4976\n",
            "  - if: 0.4898\n",
            "  - +=: 0.4760\n",
            "  - line_newline_stripped: 0.4665\n",
            "\n",
            "'if'와(과) 유사한 단어:\n",
            "  - elif: 0.8061\n",
            "  - assert: 0.6203\n",
            "  - and: 0.5914\n",
            "  - or: 0.5539\n",
            "  - continue: 0.4978\n",
            "\n",
            "'try'와(과) 유사한 단어:\n",
            "  - else: 0.6119\n",
            "  - finally: 0.5561\n",
            "  - except: 0.4203\n",
            "  - doesnotexist: 0.3936\n",
            "  - check_mode: 0.3873\n",
            "\n",
            "'in'와(과) 유사한 단어:\n",
            "  - get_language_info: 0.5516\n",
            "  - task=-1: 0.5255\n",
            "  - purposes: 0.5166\n",
            "  - incidental: 0.5126\n",
            "  - sleeping: 0.4964\n",
            "\n",
            "'+'와(과) 유사한 단어:\n",
            "  - +=: 0.7072\n",
            "  - -=: 0.5748\n",
            "  - \\'date: 0.5560\n",
            "  - /: 0.5413\n",
            "  - 0-9a-fa-f: 0.5380\n",
            "\n",
            "'='와(과) 유사한 단어:\n",
            "  - ,: 0.7453\n",
            "  - ]: 0.6684\n",
            "  - [: 0.6562\n",
            "  - ): 0.6508\n",
            "  - :: 0.6461\n",
            "\n",
            "'['와(과) 유사한 단어:\n",
            "  - ]: 0.7726\n",
            "  - ': 0.7011\n",
            "  - ,: 0.6661\n",
            "  - =: 0.6562\n",
            "  - 1: 0.6340\n",
            "\n",
            "'print'와(과) 유사한 단어:\n",
            "  - debug: 0.5878\n",
            "  - printf: 0.5633\n",
            "  - ...: 0.5614\n",
            "  - \\\\n\\\\t: 0.5412\n",
            "  - t-sne: 0.5133\n",
            "\n",
            "'str'와(과) 유사한 단어:\n",
            "  - bool: 0.6353\n",
            "  - int: 0.6163\n",
            "  - dict: 0.5286\n",
            "  - conditionset: 0.5108\n",
            "  - tuple: 0.4952\n",
            "\n",
            "'count'와(과) 유사한 단어:\n",
            "  - limit: 0.6026\n",
            "  - total: 0.5739\n",
            "  - counter: 0.5729\n",
            "  - position: 0.5573\n",
            "  - sum: 0.5466\n",
            "\n",
            "'len'와(과) 유사한 단어:\n",
            "  - pad_len: 0.5387\n",
            "  - nlevels: 0.5227\n",
            "  - count: 0.5125\n",
            "  - cols: 0.5020\n",
            "  - dims: 0.5011\n",
            "\n",
            "'split'와(과) 유사한 단어:\n",
            "  - rsplit: 0.7108\n",
            "  - lstrip: 0.7099\n",
            "  - rfind: 0.6933\n",
            "  - rstrip: 0.6620\n",
            "  - endswith: 0.6169\n",
            "\n",
            "'=='와(과) 유사한 단어:\n",
            "  - !: 0.7302\n",
            "  - else: 0.4813\n",
            "  - <: 0.4577\n",
            "  - >: 0.4566\n",
            "  - then\\\\n: 0.4310\n",
            "\n",
            "--- 모델 테스트 완료 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 1. 결과물이 생성된 폴더 경로\n",
        "# 모델 파일은 w2v 폴더에 생성됩니다.\n",
        "output_dir = \"w2v\"\n",
        "\n",
        "# 2. 구글 드라이브에 최종 모델을 저장할 경로\n",
        "drive_save_dir = \"/content/drive/MyDrive/fortmp/w2v/trained_w2v_model\"\n",
        "os.makedirs(drive_save_dir, exist_ok=True)\n",
        "\n",
        "# 3. 생성된 모델 파일 목록 확인\n",
        "try:\n",
        "    generated_files = os.listdir(output_dir)\n",
        "    print(f\"'{output_dir}' 폴더의 생성된 파일 목록:\")\n",
        "    for f in generated_files:\n",
        "        print(f\"- {f}\")\n",
        "\n",
        "    # 4. 생성된 모든 파일을 구글 드라이브로 복사\n",
        "    # w2v 폴더 자체가 아닌 w2v 폴더 안의 내용물을 복사합니다.\n",
        "    for item in os.listdir(output_dir):\n",
        "        s = os.path.join(output_dir, item)\n",
        "        d = os.path.join(drive_save_dir, item)\n",
        "        if os.path.isdir(s):\n",
        "            # 하위 폴더가 있다면 재귀적으로 복사\n",
        "            !cp -r {s} {d}\n",
        "        else:\n",
        "            # 파일 복사\n",
        "            !cp {s} {d}\n",
        "\n",
        "\n",
        "    print(f\"\\n모델 파일들을 '{drive_save_dir}' 경로에 안전하게 저장했습니다!\")\n",
        "\n",
        "    # 5. 저장된 모델 테스트 (선택 사항)\n",
        "    from gensim.models import Word2Vec\n",
        "\n",
        "    # 드라이브에 저장된 모델 경로를 사용\n",
        "    # 파일명은 실제 저장된 파일명과 일치해야 합니다. 위 ls 결과에서 확인하세요.\n",
        "    model_filename = \"word2vec_withString10-6-100.model\" # 모델 훈련 설정에 따른 파일명\n",
        "    model_path = os.path.join(drive_save_dir, model_filename)\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        try:\n",
        "            model = Word2Vec.load(model_path)\n",
        "            print(\"\\n--- 저장된 모델 로드 및 테스트 ---\")\n",
        "            # 테스트 단어는 필요에 따라 변경 가능\n",
        "            test_word = \"if\"\n",
        "            if test_word in model.wv:\n",
        "                 print(f\"'{test_word}'와 유사한 단어:\", model.wv.most_similar(test_word, topn=5))\n",
        "            else:\n",
        "                 print(f\"'{test_word}'는(은) 저장된 모델의 어휘에 없습니다.\")\n",
        "\n",
        "            test_word_2 = \"split\"\n",
        "            if test_word_2 in model.wv:\n",
        "                 print(f\"'{test_word_2}'와 유사한 단어:\", model.wv.most_similar(test_word_2, topn=5))\n",
        "            else:\n",
        "                 print(f\"'{test_word_2}'는(은) 저장된 모델의 어휘에 없습니다.\")\n",
        "\n",
        "            print(\"\\n--- 저장된 모델 테스트 완료 ---\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n저장된 모델 로드 또는 테스트 중 오류 발생: {e}\")\n",
        "    else:\n",
        "        print(f\"\\n[오류] 저장된 모델 파일을 찾을 수 없습니다. '{model_path}' 경로와 파일명을 확인해주세요.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\n[오류] '{output_dir}' 폴더를 찾을 수 없습니다. 모델 파일이 생성되지 않았거나 경로가 잘못되었습니다.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n파일 저장 또는 테스트 중 예상치 못한 오류 발생: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "shuX7f-c_pLn",
        "outputId": "56493e8f-aac2-454f-db69-ca6663c23703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'models'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1595672614.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 3. 생성된 모델 파일 목록 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgenerated_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"생성된 파일 목록:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 새 토크나이저 시험"
      ],
      "metadata": {
        "id": "thNnCW0rg9sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def custom_tokenizer(code):\n",
        "    # 코드를 소문자로 변환\n",
        "    code = code.lower()\n",
        "\n",
        "    # 조건문 패턴 정의 (예: if, while, for 뒤에 오는 조건 부분)\n",
        "    # 이 부분은 더 정교하게 만들 필요가 있습니다.\n",
        "    condition_pattern = re.compile(r'\\b(if|while|for)\\s+([^:]+):')\n",
        "\n",
        "    # 식별자 패턴 정의 (알파벳으로 시작하고 알파벳, 숫자, 언더스코어가 이어지는 패턴)\n",
        "    # 파이썬 키워드는 제외합니다.\n",
        "    python_keywords = {'False', 'None', 'True', 'and', 'as', 'assert', 'async', 'await', 'break', 'class', 'continue', 'def', 'del', 'elif', 'else', 'except', 'finally', 'for', 'from', 'global', 'if', 'import', 'in', 'is', 'lambda', 'nonlocal', 'not', 'or', 'pass', 'raise', 'return', 'try', 'while', 'with', 'yield'}\n",
        "    identifier_pattern = re.compile(r'\\b[a-z_][a-z0-9_]*\\b')\n",
        "\n",
        "    # 조건을 먼저 대체\n",
        "    condition_count = 0\n",
        "    def replace_condition(match):\n",
        "        nonlocal condition_count\n",
        "        condition_count += 1\n",
        "        # 조건 부분을 ' condX' 형태로 대체하고, 원래 조건문 키워드와 콜론은 유지\n",
        "        # 예: 'if not a:' -> 'if cond1:'\n",
        "        return f'{match.group(1)} cond{condition_count}:'\n",
        "\n",
        "    processed_code = condition_pattern.sub(replace_condition, code)\n",
        "\n",
        "\n",
        "    # 식별자를 대체\n",
        "    identifiers = {}\n",
        "    identifier_count = 0\n",
        "\n",
        "    def replace_identifier(match):\n",
        "        nonlocal identifier_count\n",
        "        word = match.group(0)\n",
        "        if word in python_keywords or word.startswith('cond'): # 키워드나 이미 대체된 조건은 건너뛰기\n",
        "            return word\n",
        "        if word not in identifiers:\n",
        "            identifier_count += 1\n",
        "            identifiers[word] = f'a{identifier_count}'\n",
        "        return identifiers[word]\n",
        "\n",
        "    # 코드를 공백 기준으로 분할하여 식별자 대체 적용\n",
        "    tokens = []\n",
        "    # 조건을 대체한 코드에 대해 식별자 대체 적용\n",
        "    for token in processed_code.split():\n",
        "         # 각 토큰에 대해 식별자 패턴을 찾아 대체\n",
        "         replaced_token = identifier_pattern.sub(replace_identifier, token)\n",
        "         tokens.append(replaced_token)\n",
        "\n",
        "\n",
        "    # 토큰들을 다시 문자열로 결합하거나 리스트 형태로 반환\n",
        "    return \" \".join(tokens) # 예시로 공백으로 다시 결합\n",
        "\n",
        "# 테스트\n",
        "code_snippet = \"\"\"\n",
        "def example_function(data):\n",
        "    if not data:\n",
        "        print(\"Data is empty\")\n",
        "    else:\n",
        "        count = 0\n",
        "        for item in data:\n",
        "            if item > 10:\n",
        "                count += item\n",
        "        while count > 0:\n",
        "            count -= 1\n",
        "            print(f\"Remaining: {count}\")\n",
        "    return count\n",
        "\"\"\"\n",
        "\n",
        "tokenized_code = custom_tokenizer(code_snippet)\n",
        "print(\"원본 코드:\")\n",
        "print(code_snippet)\n",
        "print(\"\\n토큰화된 코드:\")\n",
        "print(tokenized_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YLZA6B6g8KI",
        "outputId": "d3cf8670-c9a2-4cd2-c360-0970d55b739b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본 코드:\n",
            "\n",
            "def example_function(data):\n",
            "    if not data:\n",
            "        print(\"Data is empty\")\n",
            "    else:\n",
            "        count = 0\n",
            "        for item in data:\n",
            "            if item > 10:\n",
            "                count += item\n",
            "        while count > 0:\n",
            "            count -= 1\n",
            "            print(f\"Remaining: {count}\")\n",
            "    return count\n",
            "\n",
            "\n",
            "토큰화된 코드:\n",
            "def a1(a2): if cond1: a3(\"a2 is a4\") else: a5 = 0 for cond2: if cond3: a5 += a6 while cond4: a5 -= 1 a3(a7\"a8: {a5}\") return a5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03113632"
      },
      "source": [
        "import nltk\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import os.path\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "# 기존의 custom_tokenizer 함수를 여기에 복사하거나,\n",
        "# 이미 위의 셀에 정의되어 있다면 이 부분은 생략합니다.\n",
        "# from your_tokenizer_file import custom_tokenizer # 만약 다른 파일에 저장했다면\n",
        "\n",
        "all_words = []\n",
        "\n",
        "mode = \"withString_custom_tokenized\" # 새로운 모드 이름\n",
        "\n",
        "# Loading the training corpus\n",
        "print(\"Loading \" + mode)\n",
        "# 원본 파일 로드\n",
        "with open('w2v/pythontraining' + '_withString_X', 'r') as file:\n",
        "    pythondata = file.read().lower().replace('\\n', ' ')\n",
        "\n",
        "print(\"Length of the training file: \" + str(len(pythondata)) + \".\")\n",
        "print(\"It contains \" + str(pythondata.count(\" \")) + \" individual code tokens before custom tokenization.\")\n",
        "\n",
        "# Preparing the dataset (or loading already processed dataset to not do everything again)\n",
        "processed_data_path = 'data/pythontraining_processed_' + mode\n",
        "if (os.path.isfile(processed_data_path)):\n",
        "  with open (processed_data_path, 'rb') as fp:\n",
        "    all_words = pickle.load(fp)\n",
        "  print(\"loaded processed model.\")\n",
        "else:\n",
        "  print(\"now processing with custom tokenizer...\")\n",
        "  # 새로운 토크나이저 적용\n",
        "  tokenized_text = custom_tokenizer(pythondata)\n",
        "\n",
        "  # 토큰화된 문자열을 띄어쓰기 기준으로 분할하여 리스트로 만듦\n",
        "  # Word2Vec은 'list of lists' 형태의 입력이 필요하므로,\n",
        "  # 여기서는 각 문장을 하나의 '문서'로 간주하고 전체를 하나의 리스트로 만듭니다.\n",
        "  # 만약 문장 단위로 학습하고 싶다면, custom_tokenizer 함수를 수정하여 문장 단위로 토큰화하도록 해야 합니다.\n",
        "  all_words = [tokenized_text.split()] # 전체를 하나의 문장(리스트)으로 처리\n",
        "\n",
        "  print(\"saving processed data...\")\n",
        "  os.makedirs('data', exist_ok=True) # 'data' 폴더가 없으면 생성\n",
        "  with open(processed_data_path, 'wb') as fp:\n",
        "    pickle.dump(all_words, fp)\n",
        "\n",
        "print(\"processed.\\n\")\n",
        "\n",
        "# for 반복문 시작 직전에 이 코드를 추가하여 all_words 변수를 확인합니다.\n",
        "\n",
        "print(\"\\n--- [진단 시작] all_words 변수 상태 확인 ---\")\n",
        "print(f\"'all_words' 변수의 타입: {type(all_words)}\")\n",
        "\n",
        "if isinstance(all_words, list) and len(all_words) > 0:\n",
        "    print(f\"인식된 문장의 총 개수 (여기서는 전체 파일이 1개의 문장): {len(all_words)}\")\n",
        "    if len(all_words[0]) > 10:\n",
        "        print(f\"첫 번째 문장의 일부 (토큰 10개): {all_words[0][:10]}\")\n",
        "    else:\n",
        "         print(f\"첫 번째 문장의 일부 (전체 토큰): {all_words[0]}\")\n",
        "    print(\"진단 결과: 'all_words'가 정상적으로 보입니다.\")\n",
        "else:\n",
        "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "    print(\"!!! [문제 발견] 'all_words' 변수가 비어있거나 리스트가 아닙니다. !!!\")\n",
        "    print(\"!!! Word2Vec 모델을 만들 재료가 없어 학습이 어렵습니다. !!!\")\n",
        "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "\n",
        "print(\"--- [진단 종료] ---\\n\")\n",
        "\n",
        "\n",
        "# trying out different parameters\n",
        "for mincount in [10]:\n",
        "  for iterationen in [6]:\n",
        "    for s in [100]:\n",
        "\n",
        "      print(\"\\n\\n\" + mode + \" W2V model with min count \" + str(mincount) + \" and \" + str(iterationen) + \" Iterationen and size \" + str(s))\n",
        "      fname = \"w2v/word2vec_\"+mode+\"_\" + str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s)+ \".model\"\n",
        "      model_save_path = os.path.join(os.getcwd(), 'w2v', fname)\n",
        "\n",
        "\n",
        "      if (os.path.isfile(model_save_path)):\n",
        "        print(\"model already exists.\")\n",
        "        continue\n",
        "\n",
        "      else:\n",
        "        print(\"calculating model...\")\n",
        "        # training the model\n",
        "        # Word2Vec 모델은 'list of lists' 형태를 기대합니다.\n",
        "        # 우리의 all_words는 현재 [전체_토큰_리스트] 형태이므로, 그대로 사용합니다.\n",
        "        model = Word2Vec(all_words, vector_size=s, min_count=mincount, epochs=iterationen, workers = 4)\n",
        "\n",
        "        #saving the model\n",
        "        model.save(model_save_path)\n",
        "        print(f\"Model saved to {model_save_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}