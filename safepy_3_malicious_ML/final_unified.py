"""
í†µí•©ëœ Python íŒ¨í‚¤ì§€ ë³´ì•ˆ ë¶„ì„ ë„êµ¬ (Final Unified Version)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì„ í†µí•©í•©ë‹ˆë‹¤:
1. ë©”íƒ€ë°ì´í„° íŒŒì¼ íŒŒì‹± í›„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì €ì¥
2. ëª¨ë“  ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ ë° ì „ì²˜ë¦¬
3. LSTM ê¸°ë°˜ ì½”ë“œ ì·¨ì•½ì  ë¶„ì„
4. XGBoost ëª¨ë¸ì„ ì´ìš©í•œ ìµœì¢… ì•…ì„± íŒ¨í‚¤ì§€ íŒë‹¨
5. ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±

ì‚¬ìš©ë²•:
    python final_unified.py

ì¶œë ¥:
- merged_sourceCode.csv: ë³‘í•©ëœ ì†ŒìŠ¤ì½”ë“œ
- pypi_typo_analysis5.csv: ë¶„ì„ ê²°ê³¼ ë°ì´í„°
- package_vulnerability_analysis.csv: LSTM ë¶„ì„ ê²°ê³¼
- pypi_malicious_reason_report.txt: ìµœì¢… íŒë‹¨ ë¦¬í¬íŠ¸
"""

import os
import csv
import re
import zipfile
import pickle
import sys
import numpy as np
import pandas as pd
import gc
import math
import time
import requests
from typing import Optional, Dict, List, Tuple, Any
from collections import Counter
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from google.cloud import bigquery
from google.oauth2 import service_account
from google.api_core import exceptions as gcp_exceptions

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # TensorFlow ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # oneDNN ìµœì í™” ë¹„í™œì„±í™”
import warnings
warnings.filterwarnings('ignore')  # ëª¨ë“  ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°

# LSTM ê´€ë ¨ import
try:
    import chardet
    HAS_CHARDET = True
except ImportError:
    HAS_CHARDET = False

from tensorflow.keras import backend as K

# preprocess import ì‹œ ì¶œë ¥ ë©”ì‹œì§€ ì„ì‹œ ìˆ¨ê¸°ê¸°
import sys
from io import StringIO

# stdoutì„ ì„ì‹œë¡œ ë¦¬ë””ë ‰ì…˜í•˜ì—¬ Word2Vec ë¡œë“œ ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
old_stdout = sys.stdout
sys.stdout = StringIO()
try:
    from preprocess import tokenize_python, embed_sequences, w2v_model
finally:
    sys.stdout = old_stdout

# Levenshtein distance import (ì¡°ìš©íˆ ì²˜ë¦¬)
try:
    from Levenshtein import distance as levenshtein_distance
except ImportError:
    # ì¡°ìš©íˆ ëŒ€ì²´ í•¨ìˆ˜ ì‚¬ìš©
    def levenshtein_distance(a, b):
        return abs(len(a) - len(b))  # ê°„ë‹¨í•œ ëŒ€ì²´ í•¨ìˆ˜

# Get the current directory
current_dir = os.path.dirname(os.path.abspath(__file__))
model_save_dir = os.path.join(current_dir, 'model')
result_dir = os.path.join(current_dir, 'result')

# Global variables for models
model_mal = None
label_encoder_mal = None
xgboost_model = None

class FinalUnifiedAnalyzer:
    def __init__(self):
        self.current_dir = current_dir
        self.model_save_dir = model_save_dir
        self.result_dir = result_dir
        self.meta_datas = []
        self.df = None
        self.lstm_results = None
        
    def remove_comments(self, code):
        """ì†ŒìŠ¤ ì½”ë“œì—ì„œ ì£¼ì„ ì œê±°"""
        # ì—¬ëŸ¬ ì¤„ ì£¼ì„ ì œê±°
        code = re.sub(r"'''(.*?)'''", '', code, flags=re.DOTALL)
        code = re.sub(r'"""(.*?)"""', '', code, flags=re.DOTALL)
        # í•œ ì¤„ ì£¼ì„ ì œê±°
        code = re.sub(r'#.*', '', code)
        return code

    def process_directory(self, root_path):
        """ë””ë ‰í† ë¦¬ë¥¼ ì²˜ë¦¬í•˜ì—¬ ì†ŒìŠ¤ì½”ë“œë¥¼ ì¶”ì¶œí•˜ê³  ë³‘í•©"""
        rows = []

        # root_path ë‚´ë¶€ì˜ ëª¨ë“  í•˜ìœ„ ë””ë ‰í„°ë¦¬ íƒìƒ‰
        for dir_name in os.listdir(root_path):
            dir_path = os.path.join(root_path, dir_name)
            if os.path.isdir(dir_path):
                merged_code = ''
                for root, _, files in os.walk(dir_path):
                    for file in files:
                        if file.endswith('.py'):
                            file_path = os.path.join(root, file)
                            try:
                                with open(file_path, 'r', encoding='utf-8') as f:
                                    raw_code = f.read()
                                    cleaned_code = self.remove_comments(raw_code)
                                    merged_code += cleaned_code + '\n'
                            except Exception as e:
                                print(f"âš ï¸ {file_path} ì½ê¸° ì‹¤íŒ¨: {e}")
                if merged_code.strip():
                    rows.append([dir_name, merged_code.strip()])
        return rows

    def save_to_csv(self, data, output_file='merged_sourceCode.csv'):
        """ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        # result í´ë”ì— ì €ì¥í•˜ë„ë¡ ê²½ë¡œ ìˆ˜ì •
        if not os.path.isabs(output_file):
            os.makedirs(self.result_dir, exist_ok=True)
            output_file = os.path.join(self.result_dir, output_file)
        
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Directory', 'MergedCodeWithoutComments'])
            writer.writerows(data)

    def extract_zip_and_process_source(self):
        """ZIP íŒŒì¼ ì••ì¶• í•´ì œ ë° ì†ŒìŠ¤ì½”ë“œ ì²˜ë¦¬"""
        zip_dir = "./python-packages-1757531529324.zip"
        extract_dir = "./extracted_files"
        
        if not os.path.exists(zip_dir):
            print(f"Warning: ZIP íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {zip_dir}")
            return None
        
        # ì••ì¶• í•´ì œ
        with zipfile.ZipFile(zip_dir, 'r') as zip_ref:
            zip_ref.extractall(extract_dir)
        
        # ì†ŒìŠ¤ì½”ë“œ ì²˜ë¦¬
        root_path = './extracted_files/source'
        if os.path.exists(root_path):
            data = self.process_directory(root_path)
            self.save_to_csv(data)
            print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {len(data)}ê°œ ë””ë ‰í„°ë¦¬ ì²˜ë¦¬ë¨")
            return data
        else:
            print(f"Warning: ì†ŒìŠ¤ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {root_path}")
            return None

    def parse_name_email(self, text):
        """ì´ë¦„ê³¼ ì´ë©”ì¼ íŒŒì‹±"""
        match = re.match(r"(.*)<(.*@.*)>", text)
        if match:
            name = match.group(1).strip()
            email = match.group(2).strip()
            return name, email
        return None, None

    def parse_metadata(self, file_path):
        """ë©”íƒ€ë°ì´í„° íŒŒì‹±"""
        target_keys = {
            "name", "summary", "author", "author-email", "version",
            "maintainer", "maintainer-email"
        }
        metadata = {}

        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    line = line.strip()
                    if ':' in line:
                        key, value = map(str.strip, line.split(':', 1))
                        key_lower = key.lower()
                        
                        if key_lower in target_keys:
                            metadata[key_lower] = value

            # authorê°€ ì—†ê±°ë‚˜ ê°’ì´ ë¹„ì–´ ìˆì„ ê²½ìš°
            if not metadata.get("author"):
                if metadata.get("author-email"):
                    name, email = self.parse_name_email(metadata["author-email"])
                    if name and email:
                        metadata["author"] = name
                        metadata["author-email"] = email

            # author_emailì´ ì—†ê±°ë‚˜ ê°’ì´ ë¹„ì–´ ìˆì„ ê²½ìš°
            if not metadata.get("author-email"):
                if metadata.get("maintainer-email") and metadata["maintainer-email"].strip():
                    metadata["author-email"] = metadata["maintainer-email"]

            # authorê°€ ì—†ê±°ë‚˜ ê°’ì´ ë¹„ì–´ ìˆì„ ê²½ìš° â†’ maintainerë¡œ ëŒ€ì²´
            if not metadata.get("author"):
                if metadata.get("maintainer") and metadata["maintainer"].strip():
                    metadata["author"] = metadata["maintainer"]
                    
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜ {file_path}: {e}")
            
        return metadata

    def extract_and_parse_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° íŒŒì‹±"""
        extract_dir = "./extracted_files"
        metadata_dir = os.path.join(extract_dir, "metadata")
        
        if not os.path.exists(metadata_dir):
            print(f"Warning: ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {metadata_dir}")
            return []
            
        meta_datas = []
        for file in os.listdir(metadata_dir):
            if file.endswith(".txt"):
                metadata_path = os.path.join(metadata_dir, file)
                metadata = self.parse_metadata(metadata_path)
                if metadata:
                    meta_datas.append(metadata)
        
        self.meta_datas = meta_datas
        print(f"âœ… ë©”íƒ€ë°ì´í„° íŒŒì‹± ì™„ë£Œ: {len(meta_datas)}ê°œ")
        return meta_datas

    def get_pepy_downloads(self, package_name, api_key):
        """PePy.tech APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìš´ë¡œë“œ ìˆ˜ ì¡°íšŒ"""
        url = f"https://api.pepy.tech/api/v2/projects/{package_name}"
        headers = {"X-API-Key": api_key}

        try:
            response = requests.get(url, headers=headers)
            if response.status_code == 200:
                data = response.json()
                return data.get("total_downloads", -1)
            else:
                return -1
        except Exception as e:
            return -1

    def get_download_count_bq(self, package_name, service_account_json):
        """BigQueryë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìš´ë¡œë“œ ìˆ˜ ì¡°íšŒ"""
        try:
            client = bigquery.Client.from_service_account_json(service_account_json)

            query = """
            SELECT COUNT(*) AS total_downloads
            FROM `bigquery-public-data.pypi.file_downloads`
            WHERE file.project = @pkg
            """

            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("pkg", "STRING", package_name)
                ]
            )

            query_job = client.query(query, job_config=job_config)
            result = query_job.result()

            for row in result:
                return int(row["total_downloads"])
            return 0
        except Exception as e:
            return -1

    def download_unified(self, package_name):
        """í†µí•©ëœ ë‹¤ìš´ë¡œë“œ ìˆ˜ ì¡°íšŒ"""
        download_count = self.get_pepy_downloads(package_name, "0SRbc/jRFsHYxOShwIQ/N0jtrKf1syMW")
        if download_count == -1:
            #download_count = self.get_download_count_bq(package_name, "./plated-mantis-471407-m4-b14f1b3e761d.json")
            download_count = 0  # BigQuery ì ‘ê·¼ì´ ë¶ˆê°€ëŠ¥í•  ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •
        return download_count

    def shannon_entropy(self, s):
        """ë¬¸ìì—´ì˜ Shannon ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        if not s:
            return 0
        prob = [v / len(s) for v in Counter(s).values()]
        return -sum(p * math.log2(p) for p in prob)

    def is_valid_version(self, v):
        """ë²„ì „ í˜•ì‹ ê²€ì¦"""
        return bool(re.match(r"^\d+\.\d+\.\d+$", str(v).strip()))

    def get_pypi_top_packages(self):
        """PyPI ìƒìœ„ íŒ¨í‚¤ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            url = "https://hugovk.github.io/top-pypi-packages/top-pypi-packages-30-days.json"
            response = requests.get(url)
            data = response.json()
            return [pkg['project'] for pkg in data['rows']]
        except Exception as e:
            print(f"ì¸ê¸° íŒ¨í‚¤ì§€ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def extract_core_name(self, name):
        """í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ (ì ‘ë‘/ì ‘ë¯¸ì–´ ì œê±°)"""
        return re.split(r"[-_.]", name.lower())[0]

    def is_typo_like(self, pkg_name, legit_list):
        """ì˜¤íƒ€ ê¸°ë°˜ ìœ ì‚¬ì„± íŒë³„"""
        name = self.extract_core_name(pkg_name)
        for legit in legit_list:
            legit_core = self.extract_core_name(legit)
            if levenshtein_distance(name, legit_core) == 1 and abs(len(name) - len(legit_core)) <= 1:
                return True
        return False

    def preprocess_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬"""
        # ë‹¤ìš´ë¡œë“œ ìˆ˜ ìˆ˜ì§‘
        for meta_data in self.meta_datas:
            package_name = meta_data.get("name")
            if package_name:
                download_count = self.download_unified(package_name)
                meta_data["download"] = download_count

        # DataFrame ë³€í™˜
        df = pd.DataFrame(self.meta_datas)

        # ê¸°ë³¸ ì „ì²˜ë¦¬
        df["download"] = df["download"].fillna(0).astype(int)
        df["download_log"] = df["download"].apply(lambda x: np.log1p(x))

        scaler = StandardScaler()
        df["download_scaled"] = scaler.fit_transform(df[["download_log"]])

        # ì„¤ëª… ë¶„ì„
        df["summary"] = df["summary"].fillna("")
        df["summary_length"] = df["summary"].apply(len)
        df["summary_too_short"] = df["summary_length"] < 10
        df["summary_too_long"] = df["summary_length"] > 300
        df["summary_entropy"] = df["summary"].apply(self.shannon_entropy)
        df["summary_low_entropy"] = df["summary_entropy"] < 3.5

        # ë²„ì „ ê²€ì¦
        df["version_valid"] = df["version"].apply(self.is_valid_version)

        # ì˜¤íƒ€ ê¸°ë°˜ íƒì§€
        pypi_packages = self.get_pypi_top_packages()
        df["is_typo_like"] = df["name"].apply(lambda x: self.is_typo_like(x, pypi_packages))

        # ì¶”ê°€ í”¼ì²˜
        df["download_too_low"] = df["download_log"] < df["download_log"].quantile(0.05)
        df["download_too_high"] = df["download_log"] > df["download_log"].quantile(0.95)
        df["is_disposable"] = False

        # MinMaxScalerë¡œ download_log ì •ê·œí™”
        scaler2 = MinMaxScaler()
        df["download_log_scaled"] = 1 - scaler2.fit_transform(df[["download_log"]])

        self.df = df
        return df

    # LSTM ë¶„ì„ ê´€ë ¨ ë©”ì„œë“œë“¤
    def detect_encoding(self, file_path):
        """íŒŒì¼ ì¸ì½”ë”© ê°ì§€"""
        if HAS_CHARDET:
            try:
                with open(file_path, 'rb') as f:
                    sample = f.read(10000)
                    result = chardet.detect(sample)
                    if result['confidence'] > 0.7:
                        return [result['encoding']]
            except Exception as e:
                print(f"ì¸ì½”ë”© ê°ì§€ ì˜¤ë¥˜: {e}")
        
        return ['cp949', 'euc-kr', 'utf-8', 'utf-16', 'latin-1', 'iso-8859-1']

    def read_csv_data(self, csv_file_path):
        """CSV íŒŒì¼ ì½ê¸° (ì¸ì½”ë”© ìë™ ê°ì§€)"""
        print(f"CSV íŒŒì¼ ì½ê¸° ì‹œë„: {csv_file_path}")
        
        encodings = self.detect_encoding(csv_file_path)
        
        for encoding in encodings:
            try:
                print(f"ì¸ì½”ë”© ì‹œë„: {encoding}")
                df = pd.read_csv(csv_file_path, encoding=encoding)
                
                if 'package' not in df.columns or 'code' not in df.columns:
                    # Directoryì™€ MergedCodeWithoutComments ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš° ë³€í™˜
                    if 'Directory' in df.columns and 'MergedCodeWithoutComments' in df.columns:
                        df = df.rename(columns={'Directory': 'package', 'MergedCodeWithoutComments': 'code'})
                    else:
                        print(f"í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {list(df.columns)}")
                        return None
                        
                print(f"CSV íŒŒì¼ ë¡œë“œ ì„±ê³µ: {csv_file_path} ({encoding})")
                print(f"í–‰ ìˆ˜: {len(df)}, ì»¬ëŸ¼: {list(df.columns)}")
                return df
                
            except UnicodeDecodeError:
                continue
            except Exception as e:
                print(f"CSV ì½ê¸° ì˜¤ë¥˜ ({encoding}): {e}")
                continue
        
        print(f"ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨: {encodings}")
        return None

    def load_lstm_models(self):
        """LSTM ëª¨ë¸ê³¼ ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ"""
        global model_mal, label_encoder_mal
        
        try:
            model_path = os.path.join(self.model_save_dir, 'model_mal.pkl')
            with open(model_path, 'rb') as f:
                model_mal = pickle.load(f)
            print("LSTM ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            
            # GPU ìµœì í™” ì„¤ì • (ì¡°ìš©íˆ ì²˜ë¦¬)
            try:
                import tensorflow as tf
                if tf.config.list_physical_devices('GPU'):
                    gpus = tf.config.experimental.list_physical_devices('GPU')
                    if gpus:
                        for gpu in gpus:
                            tf.config.experimental.set_memory_growth(gpu, True)
            except Exception:
                pass  # ì¡°ìš©íˆ ë¬´ì‹œ
            
        except Exception as e:
            print(f"LSTM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
        
        try:
            encoder_path = os.path.join(self.model_save_dir, 'label_encoder_mal.pkl')
            with open(encoder_path, 'rb') as f:
                label_encoder_mal = pickle.load(f)
            print("ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ ì„±ê³µ")
            return True
        except Exception as e:
            print(f"ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def analyze_single_code(self, source_code, package_name):
        """ë‹¨ì¼ ì½”ë“œ LSTM ë¶„ì„"""
        try:
            tokenized_code = tokenize_python(source_code)
            
            if not tokenized_code:
                return {
                    'vulnerability_status': 'Error',
                    'cwe_label': 'parsing_error',
                    'confidence': 0.0
                }
            
            if w2v_model is None:
                return {
                    'vulnerability_status': 'Error',
                    'cwe_label': 'model_error',
                    'confidence': 0.0
                }
            
            embedded_code = embed_sequences([tokenized_code], w2v_model)
            
            if not embedded_code or len(embedded_code) == 0 or embedded_code[0].size == 0:
                return {
                    'vulnerability_status': 'Error',
                    'cwe_label': 'embedding_error',
                    'confidence': 0.0
                }
            
            # ì‹œí€€ìŠ¤ íŒ¨ë”©
            max_sequence_length = 100
            embedding_dim = w2v_model.vector_size
            padded_code = np.zeros((max_sequence_length, embedding_dim))
            
            embedded_sequence = embedded_code[0]
            if embedded_sequence.shape[0] > 0:
                if embedded_sequence.shape[0] < max_sequence_length:
                    padded_code[:embedded_sequence.shape[0], :] = embedded_sequence
                else:
                    padded_code = embedded_sequence[:max_sequence_length, :]
            
            padded_code = np.expand_dims(padded_code, axis=0)
            
            # ëª¨ë¸ ì˜ˆì¸¡
            prediction = model_mal.predict(padded_code, verbose=0)
            
            if prediction.ndim == 2 and prediction.shape[1] == 1:
                confidence = float(prediction[0][0])
                predicted_index = int((prediction > 0.5).astype(int)[0][0])
            else:
                predicted_index = int(np.argmax(prediction, axis=1)[0])
                confidence = float(prediction[0][predicted_index])
            
            try:
                decoded_label = label_encoder_mal.inverse_transform([predicted_index])[0]
            except Exception as e:
                return {
                    'vulnerability_status': 'Error',
                    'cwe_label': 'label_decode_error',
                    'confidence': confidence
                }
            
            benign_aliases = {"Benign", "benign", "Not Vulnerable", "Normal", "Safe", "0", 0}
            is_vulnerable = decoded_label not in benign_aliases
            
            vulnerability_status = 'Vulnerable' if is_vulnerable else 'Not Vulnerable'
            cwe_label = str(decoded_label) if is_vulnerable else 'Benign'
            
            return {
                'vulnerability_status': vulnerability_status,
                'cwe_label': cwe_label,
                'confidence': confidence
            }
            
        except Exception as e:
            print(f"ì½”ë“œ ë¶„ì„ ì˜¤ë¥˜ ({package_name}): {e}")
            return {
                'vulnerability_status': 'Error',
                'cwe_label': 'analysis_error',
                'confidence': 0.0
            }

    def analyze_lstm_codes(self, source_csv='merged_sourceCode.csv'):
        """CSV íŒŒì¼ì˜ ëª¨ë“  ì½”ë“œë¥¼ LSTMìœ¼ë¡œ ë¶„ì„"""
        csv_path = os.path.join(self.result_dir, source_csv)  # result í´ë”ì—ì„œ ì°¾ë„ë¡ ìˆ˜ì •
        
        if not os.path.exists(csv_path):
            print(f"ì†ŒìŠ¤ì½”ë“œ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
            return None
            
        df = self.read_csv_data(csv_path)
        if df is None:
            return None
        
        print(f"\n=== LSTM ë¶„ì„ ì‹œì‘: {len(df)}ê°œ íŒ¨í‚¤ì§€ ===")
        start_time = time.time()
        
        results = []
        
        for idx, row in df.iterrows():
            package_name = row['package']
            source_code = row['code']
            
            print(f"LSTM ë¶„ì„ ì¤‘ ({idx+1}/{len(df)}): {package_name}")
            
            if pd.isna(source_code) or str(source_code).strip() == '':
                result_row = {
                    'package': package_name,
                    'vulnerability_status': 'Error',
                    'cwe_label': 'Empty Code',
                    'confidence': 0.0
                }
            else:
                analysis_result = self.analyze_single_code(str(source_code), package_name)
                result_row = {
                    'package': package_name,
                    'vulnerability_status': analysis_result['vulnerability_status'],
                    'cwe_label': analysis_result['cwe_label'],
                    'confidence': analysis_result['confidence']
                }
            
            results.append(result_row)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        result_df = pd.DataFrame(results)
        
        print(f"\n=== LSTM ë¶„ì„ ì™„ë£Œ ===")
        print(f"ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"íŒ¨í‚¤ì§€ë‹¹ í‰ê·  ì‹œê°„: {total_time/len(df):.2f}ì´ˆ")
        
        # ê²°ê³¼ ì €ì¥
        os.makedirs(self.result_dir, exist_ok=True)
        output_path = os.path.join(self.result_dir, 'package_vulnerability_analysis.csv')
        result_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"LSTM ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_path}")
        
        self.lstm_results = result_df
        return result_df

    def integrate_lstm_results(self):
        """LSTM ê²°ê³¼ë¥¼ ë©”ì¸ DataFrameì— í†µí•©"""
        if self.lstm_results is None or self.df is None:
            print("LSTM ê²°ê³¼ ë˜ëŠ” ë©”ì¸ DataFrameì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        # LSTM ê²°ê³¼ë¥¼ ë©”ì¸ DataFrameê³¼ ë³‘í•©
        # vulnerability_statusë¥¼ ìˆ«ìë¡œ ë³€í™˜
        vulnerability_map = {'Vulnerable': 1, 'Not Vulnerable': 0, 'Error': -1}
        self.lstm_results['vulnerability_status_numeric'] = self.lstm_results['vulnerability_status'].map(vulnerability_map)
        
        # CWE ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜ (ê°„ë‹¨í•œ ë°©ì‹)
        cwe_map = {'Benign': 0, 'Empty Code': -1, 'parsing_error': -1, 'model_error': -1, 
                   'embedding_error': -1, 'label_decode_error': -1, 'analysis_error': -1}
        
        # CWE ê°’ë“¤ì„ ìˆ«ìë¡œ ë§¤í•‘ (ê¸°íƒ€ëŠ” 1ë¡œ ì„¤ì •)
        self.lstm_results['cwe_label_numeric'] = self.lstm_results['cwe_label'].apply(
            lambda x: cwe_map.get(x, 1) if x in cwe_map else 1
        )
        
        # íŒ¨í‚¤ì§€ ì´ë¦„ì„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
        merged_df = pd.merge(self.df, 
                            self.lstm_results[['package', 'vulnerability_status_numeric', 'cwe_label_numeric', 'confidence']], 
                            left_on='name', 
                            right_on='package', 
                            how='left')
        
        # ë³‘í•©ë˜ì§€ ì•Šì€ í•­ëª©ì€ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
        merged_df['vulnerability_status_numeric'] = merged_df['vulnerability_status_numeric'].fillna(0)
        merged_df['cwe_label_numeric'] = merged_df['cwe_label_numeric'].fillna(0)
        merged_df['confidence'] = merged_df['confidence'].fillna(0.0)
        
        # ìƒˆë¡œìš´ í”¼ì²˜ ìƒì„± (ë…¸ì´ì¦ˆ ì¶”ê°€ëœ ë²„ì „)
        merged_df['vulnerability_status_noisy'] = merged_df['vulnerability_status_numeric']
        merged_df['cwe_label_noisy'] = merged_df['cwe_label_numeric'] 
        merged_df['threat_level_noisy'] = merged_df.apply(self.combined_threat, axis=1)
        merged_df['download_log_scaled_noisy'] = merged_df['download_log_scaled']
        
        self.df = merged_df
        
        # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥ (result í´ë”ì—)
        os.makedirs(self.result_dir, exist_ok=True)
        output_path = os.path.join(self.result_dir, 'pypi_typo_analysis5.csv')
        self.df.to_csv(output_path, index=False)
        print(f"í†µí•©ëœ ë¶„ì„ ë°ì´í„° ì €ì¥: {output_path}")
        
        return True

    def combined_threat(self, row):
        """ìœ„í˜‘ ìˆ˜ì¤€ ê³„ì‚°"""
        vuln_status = row.get('vulnerability_status_numeric', 0)
        cwe_label = row.get('cwe_label_numeric', 0)
        
        if vuln_status == 1 and cwe_label == 1:
            return 2
        elif vuln_status == 1 or cwe_label == 1:
            return 1
        else:
            return 0

    def load_xgboost_model(self):
        """XGBoost ëª¨ë¸ ë¡œë“œ"""
        global xgboost_model
        
        try:
            model_path = os.path.join(self.current_dir, "xgboost_model.pkl")
            with open(model_path, "rb") as f:
                xgboost_model = pickle.load(f)
            print("XGBoost ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            return True
        except Exception as e:
            print(f"XGBoost ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def predict_malicious(self):
        """XGBoost ëª¨ë¸ë¡œ ì•…ì„± íŒ¨í‚¤ì§€ ì˜ˆì¸¡"""
        if self.df is None:
            print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        if xgboost_model is None:
            print("XGBoost ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        # í”¼ì²˜ ì„ íƒ
        features = [
            "is_disposable", 
            "summary_length", "summary_too_short", "summary_too_long",
            "summary_entropy", "summary_low_entropy", "version_valid",
            "is_typo_like",
            "download_log_scaled_noisy",
            "vulnerability_status_noisy", "threat_level_noisy", "cwe_label_noisy"
        ]
        
        # ëˆ„ë½ëœ í”¼ì²˜ê°€ ìˆëŠ”ì§€ í™•ì¸
        available_features = [f for f in features if f in self.df.columns]
        missing_features = [f for f in features if f not in self.df.columns]
        
        if missing_features:
            print(f"ëˆ„ë½ëœ í”¼ì²˜: {missing_features}")
            # ëˆ„ë½ëœ í”¼ì²˜ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›€
            for feature in missing_features:
                self.df[feature] = 0
        
        X = self.df[features]
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        try:
            self.df["is_malicious"] = xgboost_model.predict(X)
            print("ì•…ì„± íŒ¨í‚¤ì§€ ì˜ˆì¸¡ ì™„ë£Œ")
            print(self.df[["name", "is_malicious"]].head(10))
            return True
        except Exception as e:
            print(f"ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def get_malicious_reasons(self, row):
        """ì•…ì„± íŒë‹¨ ì´ìœ  ìƒì„±"""
        reasons = []
        if row.get("summary_too_short"): reasons.append("ìš”ì•½ì´ ë„ˆë¬´ ì§§ìŒ")
        if row.get("summary_too_long"): reasons.append("ìš”ì•½ì´ ë„ˆë¬´ ê¹€")
        if row.get("summary_low_entropy"): reasons.append("ìš”ì•½ì´ ìë™ ìƒì„±ëœ ê²ƒìœ¼ë¡œ ì˜ì‹¬ë¨")
        if row.get("download_too_low"): reasons.append("ë‹¤ìš´ë¡œë“œ ìˆ˜ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ë‚®ìŒ")
        if row.get("download_too_high"): reasons.append("ë‹¤ìš´ë¡œë“œ ìˆ˜ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ìŒ")
        if not row.get("version_valid"): reasons.append("ë²„ì „ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŒ")
        if row.get("is_disposable"): reasons.append("ì¼íšŒìš© ì´ë©”ì¼ ì‚¬ìš© ì˜ì‹¬")
        if row.get("is_typo_like"): reasons.append("ì¸ê¸° íŒ¨í‚¤ì§€ ì´ë¦„ê³¼ ìœ ì‚¬í•œ ì˜¤íƒ€ ê¸°ë°˜ ì´ë¦„")
        if row.get("vulnerability_status_noisy", 0) == 1: reasons.append("LSTM ë¶„ì„ì—ì„œ ì·¨ì•½ì  ë°œê²¬")
        return reasons

    def get_normal_reasons(self, row):
        """ì •ìƒ íŒë‹¨ ì´ìœ  ìƒì„±"""
        reasons = []
        if not row.get("summary_too_short"): reasons.append("ìš”ì•½ ê¸¸ì´ ì ì ˆí•¨")
        if not row.get("summary_too_long"): reasons.append("ìš”ì•½ì´ ë„ˆë¬´ ê¸¸ì§€ ì•ŠìŒ")
        if not row.get("summary_low_entropy"): reasons.append("ìš”ì•½ì´ ì‚¬ëŒì´ ì‘ì„±í•œ ê²ƒìœ¼ë¡œ ë³´ì„")
        if not row.get("download_too_low"): reasons.append("ë‹¤ìš´ë¡œë“œ ìˆ˜ê°€ ì¶©ë¶„í•¨")
        if not row.get("download_too_high"): reasons.append("ë‹¤ìš´ë¡œë“œ ìˆ˜ê°€ ê³¼ë„í•˜ì§€ ì•ŠìŒ")
        if row.get("version_valid"): reasons.append("ë²„ì „ í˜•ì‹ì´ ì˜¬ë°”ë¦„")
        if not row.get("is_disposable"): reasons.append("ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì´ë©”ì¼ ì‚¬ìš©")
        if not row.get("is_typo_like"): reasons.append("ì´ë¦„ì´ ì¸ê¸° íŒ¨í‚¤ì§€ì™€ ìœ ì‚¬í•˜ì§€ ì•ŠìŒ")
        if row.get("vulnerability_status_noisy", 0) == 0: reasons.append("LSTM ë¶„ì„ì—ì„œ ì·¨ì•½ì  ì—†ìŒ")
        return reasons

    def generate_final_report(self):
        """ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        if self.df is None or 'is_malicious' not in self.df.columns:
            print("ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ì–´ì„œ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        os.makedirs(self.result_dir, exist_ok=True)
        report_path = os.path.join(self.result_dir, "pypi_malicious_reason_report.txt")
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("=== Python íŒ¨í‚¤ì§€ ë³´ì•ˆ ë¶„ì„ ìµœì¢… ë¦¬í¬íŠ¸ ===\n\n")
            
            total_packages = len(self.df)
            malicious_count = self.df['is_malicious'].sum()
            normal_count = total_packages - malicious_count
            
            f.write(f"ì´ ë¶„ì„ íŒ¨í‚¤ì§€ ìˆ˜: {total_packages}\n")
            f.write(f"ì•…ì„± íŒ¨í‚¤ì§€: {malicious_count}ê°œ\n")
            f.write(f"ì •ìƒ íŒ¨í‚¤ì§€: {normal_count}ê°œ\n")
            f.write(f"ì•…ì„± ë¹„ìœ¨: {malicious_count/total_packages*100:.2f}%\n\n")
            
            for _, row in self.df.iterrows():
                pkg_name = row.get("name", "unknown")
                label = row.get("is_malicious", 0)

                if label == 1:
                    reasons = self.get_malicious_reasons(row)
                    status = "âŒ ì•…ì„±"
                else:
                    reasons = self.get_normal_reasons(row)
                    status = "âœ… ì •ìƒ"

                reason_text = " / ".join(reasons) if reasons else "íŒë‹¨ ê¸°ì¤€ ì—†ìŒ"
                f.write(f"{status} ğŸ“¦ {pkg_name}\n")
                f.write(f"â†’ íŒë‹¨ ì´ìœ : {reason_text}\n\n")

        print(f"ğŸ“„ ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        return True

    def save_comprehensive_results(self):
        """ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ í¬í•¨í•œ í†µí•© CSV íŒŒì¼ ìƒì„±"""
        if self.df is None:
            print("ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ì„œ í†µí•© CSVë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.result_dir, exist_ok=True)
        
        # í†µí•© ê²°ê³¼ DataFrame ì¤€ë¹„
        comprehensive_df = self.df.copy()
        
        # LSTM ê²°ê³¼ì™€ ë³‘í•© (ì´ë¯¸ í†µí•©ë˜ì–´ ìˆì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ í‘œì‹œ)
        if self.lstm_results is not None:
            # LSTM ê²°ê³¼ë¥¼ ë” ëª…í™•í•˜ê²Œ í‘œì‹œ
            lstm_merge_df = self.lstm_results[['package', 'vulnerability_status', 'cwe_label', 'confidence']].rename(columns={
                'vulnerability_status': 'lstm_vulnerability_status',
                'cwe_label': 'lstm_cwe_label', 
                'confidence': 'lstm_confidence'
            })
            
            comprehensive_df = pd.merge(comprehensive_df, lstm_merge_df, 
                                       left_on='name', right_on='package', 
                                       how='left', suffixes=('', '_lstm'))
            
            # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
            if 'package_lstm' in comprehensive_df.columns:
                comprehensive_df = comprehensive_df.drop('package_lstm', axis=1)
        
        # ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ì»¬ëŸ¼ ì´ë¦„ ëª…í™•í™”
        if 'is_malicious' in comprehensive_df.columns:
            comprehensive_df = comprehensive_df.rename(columns={'is_malicious': 'xgboost_prediction'})
        
        # ì¤‘ìš”í•œ ì»¬ëŸ¼ë“¤ì„ ì•ìª½ìœ¼ë¡œ ì¬ë°°ì¹˜
        priority_columns = [
            'name',  # íŒ¨í‚¤ì§€ ì´ë¦„
            'xgboost_prediction',  # XGBoost ìµœì¢… ì˜ˆì¸¡
            'lstm_vulnerability_status',  # LSTM ì·¨ì•½ì  ìƒíƒœ
            'lstm_cwe_label',  # LSTM CWE ë¼ë²¨
            'lstm_confidence',  # LSTM ì‹ ë¢°ë„
            'summary',  # íŒ¨í‚¤ì§€ ì„¤ëª…
            'author',  # ì‘ì„±ì
            'author-email',  # ì‘ì„±ì ì´ë©”ì¼
            'version',  # ë²„ì „
            'download',  # ë‹¤ìš´ë¡œë“œ ìˆ˜
            'download_log',  # ë¡œê·¸ ë³€í™˜ëœ ë‹¤ìš´ë¡œë“œ ìˆ˜
        ]
        
        # ìš°ì„ ìˆœìœ„ ì»¬ëŸ¼ë“¤ì´ ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ ì„ íƒ
        available_priority_cols = [col for col in priority_columns if col in comprehensive_df.columns]
        
        # ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ë“¤
        remaining_cols = [col for col in comprehensive_df.columns if col not in available_priority_cols]
        
        # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬
        final_columns = available_priority_cols + remaining_cols
        comprehensive_df = comprehensive_df[final_columns]
        
        # í†µí•© CSV íŒŒì¼ ì €ì¥
        comprehensive_csv_path = os.path.join(self.result_dir, 'comprehensive_analysis_results.csv')
        comprehensive_df.to_csv(comprehensive_csv_path, index=False, encoding='utf-8-sig')
        
        print(f"ğŸ“Š í†µí•© ë¶„ì„ ê²°ê³¼ CSV ì €ì¥: {comprehensive_csv_path}")
        print(f"   - ì´ íŒ¨í‚¤ì§€ ìˆ˜: {len(comprehensive_df)}")
        print(f"   - ì´ ì»¬ëŸ¼ ìˆ˜: {len(comprehensive_df.columns)}")
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        if 'xgboost_prediction' in comprehensive_df.columns:
            malicious_count = comprehensive_df['xgboost_prediction'].sum()
            print(f"   - XGBoost ì˜ˆì¸¡ ì•…ì„±: {malicious_count}ê°œ")
            print(f"   - XGBoost ì˜ˆì¸¡ ì •ìƒ: {len(comprehensive_df) - malicious_count}ê°œ")
        
        if 'lstm_vulnerability_status' in comprehensive_df.columns:
            lstm_vulnerable = (comprehensive_df['lstm_vulnerability_status'] == 'Vulnerable').sum()
            print(f"   - LSTM ì·¨ì•½ì  ë°œê²¬: {lstm_vulnerable}ê°œ")
        
        # ì£¼ìš” ì»¬ëŸ¼ ëª©ë¡ ì¶œë ¥
        print("   - ì£¼ìš” ì»¬ëŸ¼ë“¤:")
        for i, col in enumerate(available_priority_cols[:10]):  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
            print(f"     {i+1}. {col}")
        
        if len(available_priority_cols) > 10:
            print(f"     ... ì™¸ {len(final_columns) - 10}ê°œ ì»¬ëŸ¼")
        
        return comprehensive_csv_path

    def cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        global model_mal, label_encoder_mal, xgboost_model
        
        try:
            K.clear_session()
        except:
            pass
        
        # ì „ì—­ ë³€ìˆ˜ ì •ë¦¬
        for obj_name in ['model_mal', 'label_encoder_mal', 'xgboost_model']:
            if obj_name in globals():
                try:
                    del globals()[obj_name]
                except:
                    pass
        
        gc.collect()
        print("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=== Python íŒ¨í‚¤ì§€ ë³´ì•ˆ ë¶„ì„ ë„êµ¬ (Final Unified) ===\n")
    
    analyzer = FinalUnifiedAnalyzer()
    
    try:
        # 1. ZIP íŒŒì¼ í•´ì œ ë° ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ
        print("1ï¸âƒ£ ZIP íŒŒì¼ í•´ì œ ë° ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ...")
        source_data = analyzer.extract_zip_and_process_source()
        if source_data is None:
            print("âŒ ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ ì‹¤íŒ¨")
            return
        
        # 2. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° íŒŒì‹±
        print("\n2ï¸âƒ£ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° íŒŒì‹±...")
        meta_data = analyzer.extract_and_parse_metadata()
        if not meta_data:
            print("âŒ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨")
            return
        
        # 3. ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬
        print("\n3ï¸âƒ£ ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬...")
        df = analyzer.preprocess_metadata()
        if df is None:
            print("âŒ ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨")
            return
        
        # 4. LSTM ëª¨ë¸ ë¡œë“œ
        print("\n4ï¸âƒ£ LSTM ëª¨ë¸ ë¡œë“œ...")
        if not analyzer.load_lstm_models():
            print("âŒ LSTM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            return
        
        # 5. LSTM ì½”ë“œ ë¶„ì„
        print("\n5ï¸âƒ£ LSTM ì½”ë“œ ë¶„ì„...")
        lstm_results = analyzer.analyze_lstm_codes()
        if lstm_results is None:
            print("âŒ LSTM ë¶„ì„ ì‹¤íŒ¨")
            return
        
        # 6. LSTM ê²°ê³¼ í†µí•©
        print("\n6ï¸âƒ£ LSTM ê²°ê³¼ í†µí•©...")
        if not analyzer.integrate_lstm_results():
            print("âŒ ê²°ê³¼ í†µí•© ì‹¤íŒ¨")
            return
        
        # 7. XGBoost ëª¨ë¸ ë¡œë“œ
        print("\n7ï¸âƒ£ XGBoost ëª¨ë¸ ë¡œë“œ...")
        if not analyzer.load_xgboost_model():
            print("âŒ XGBoost ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            return
        
        # 8. ìµœì¢… ì•…ì„± ì˜ˆì¸¡
        print("\n8ï¸âƒ£ ìµœì¢… ì•…ì„± íŒ¨í‚¤ì§€ ì˜ˆì¸¡...")
        if not analyzer.predict_malicious():
            print("âŒ ì˜ˆì¸¡ ì‹¤íŒ¨")
            return
        
        # 9. ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
        print("\n9ï¸âƒ£ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±...")
        if not analyzer.generate_final_report():
            print("âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨")
            return
        
        # 10. í†µí•© CSV íŒŒì¼ ìƒì„±
        print("\nğŸ”Ÿ í†µí•© ë¶„ì„ ê²°ê³¼ CSV ìƒì„±...")
        comprehensive_csv = analyzer.save_comprehensive_results()
        if not comprehensive_csv:
            print("âŒ í†µí•© CSV ìƒì„± ì‹¤íŒ¨")
            return
        
        print("\nâœ… ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("\nìƒì„±ëœ íŒŒì¼ë“¤ (./result í´ë”):")
        print("- result/merged_sourceCode.csv: ë³‘í•©ëœ ì†ŒìŠ¤ì½”ë“œ")
        print("- result/pypi_typo_analysis5.csv: í†µí•© ë¶„ì„ ë°ì´í„°")
        print("- result/package_vulnerability_analysis.csv: LSTM ë¶„ì„ ê²°ê³¼")
        print("- result/comprehensive_analysis_results.csv: ëª¨ë“  ê²°ê³¼ í†µí•© CSV")
        print("- result/pypi_malicious_reason_report.txt: ìµœì¢… íŒë‹¨ ë¦¬í¬íŠ¸")
        
    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # 11. ë©”ëª¨ë¦¬ ì •ë¦¬
        print("\nğŸ”§ ë©”ëª¨ë¦¬ ì •ë¦¬...")
        analyzer.cleanup()

if __name__ == "__main__":
    main()