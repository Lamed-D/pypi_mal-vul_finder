"""
í†µí•©ëœ Python íŒ¨í‚¤ì§€ ë³´ì•ˆ ë¶„ì„ ë„êµ¬ (Final Unified Version)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì„ í†µí•©í•©ë‹ˆë‹¤:
1. ë©”íƒ€ë°ì´í„° íŒŒì¼ íŒŒì‹± í›„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì €ì¥
2. ëª¨ë“  ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ ë° ì „ì²˜ë¦¬
3. LSTM ê¸°ë°˜ ì½”ë“œ ì·¨ì•½ì  ë¶„ì„
4. XGBoost ëª¨ë¸ì„ ì´ìš©í•œ ìµœì¢… ì•…ì„± íŒ¨í‚¤ì§€ íŒë‹¨
5. ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±

ì‚¬ìš©ë²•:
    python final_unified.py

ì¶œë ¥:
- merged_sourceCode.csv: ë³‘í•©ëœ ì†ŒìŠ¤ì½”ë“œ
- pypi_typo_analysis5.csv: ë¶„ì„ ê²°ê³¼ ë°ì´í„°
- package_vulnerability_analysis.csv: LSTM ë¶„ì„ ê²°ê³¼
- pypi_malicious_reason_report.txt: ìµœì¢… íŒë‹¨ ë¦¬í¬íŠ¸
"""

import os
import csv
import re
import zipfile
import pickle
import sys
import numpy as np
import pandas as pd
import gc
import math
import time
import requests
from typing import Optional, Dict, List, Tuple, Any
from collections import Counter
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from google.cloud import bigquery
from google.oauth2 import service_account
from google.api_core import exceptions as gcp_exceptions

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸° ë° ì„±ëŠ¥ ìµœì í™”
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # TensorFlow ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # oneDNN ìµœì í™” ë¹„í™œì„±í™”
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'  # GPU ë©”ëª¨ë¦¬ ì ì§„ì  í• ë‹¹
os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'  # GPU ìŠ¤ë ˆë“œ ëª¨ë“œ ìµœì í™”
import warnings
warnings.filterwarnings('ignore')  # ëª¨ë“  ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°

# TensorFlow GPU ë©”ëª¨ë¦¬ ìµœì í™”
try:
    import tensorflow as tf
    # GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ë©”ëª¨ë¦¬ ì¦ê°€ í—ˆìš©
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
                tf.config.experimental.set_virtual_device_configuration(
                    gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)]
                )
        except RuntimeError:
            pass  # ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš° ë¬´ì‹œ
except ImportError:
    pass

# LSTM ê´€ë ¨ import
try:
    import chardet
    HAS_CHARDET = True
except ImportError:
    HAS_CHARDET = False

# ì¼€ë¼ìŠ¤ import - í˜¸í™˜ì„±ì„ ìœ„í•œ ë‹¤ì¤‘ ë°©ì‹ ì‹œë„
try:
    from tensorflow.keras import backend as K
    from tensorflow import keras
except ImportError:
    try:
        import keras
        from keras import backend as K
    except ImportError:
        print("ì¼€ë¼ìŠ¤ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜¸í™˜ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        K = None
        keras = None

# preprocess import ì‹œ ì¶œë ¥ ë©”ì‹œì§€ ì„ì‹œ ìˆ¨ê¸°ê¸°
import sys
from io import StringIO

# stdoutì„ ì„ì‹œë¡œ ë¦¬ë””ë ‰ì…˜í•˜ì—¬ Word2Vec ë¡œë“œ ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
old_stdout = sys.stdout
sys.stdout = StringIO()
try:
    from preprocess import tokenize_python, embed_sequences, w2v_model
finally:
    sys.stdout = old_stdout

# Levenshtein distance import (ì¡°ìš©íˆ ì²˜ë¦¬)
try:
    from Levenshtein import distance as levenshtein_distance
except ImportError:
    # ì¡°ìš©íˆ ëŒ€ì²´ í•¨ìˆ˜ ì‚¬ìš©
    def levenshtein_distance(a, b):
        return abs(len(a) - len(b))  # ê°„ë‹¨í•œ ëŒ€ì²´ í•¨ìˆ˜

# Get the current directory
current_dir = os.path.dirname(os.path.abspath(__file__))
model_save_dir = os.path.join(current_dir, 'model')
result_dir = os.path.join(current_dir, 'result')

# Global variables for models
model_mal = None
label_encoder_mal = None
xgboost_model = None

class FinalUnifiedAnalyzer:
    def __init__(self):
        self.current_dir = current_dir
        self.model_save_dir = model_save_dir
        self.result_dir = result_dir
        self.meta_datas = []
        self.df = None
        self.lstm_results = None
        
    def read_python_file_with_encoding(self, file_path):
        """ë‹¤ì–‘í•œ ì¸ì½”ë”©ì„ ì‹œë„í•˜ì—¬ Python íŒŒì¼ ì½ê¸°"""
        # ì‹œë„í•  ì¸ì½”ë”© ë¦¬ìŠ¤íŠ¸ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
        encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'latin-1', 'iso-8859-1', 'utf-16', 'ascii']
        
        # chardetìœ¼ë¡œ ì¸ì½”ë”© ê°ì§€ ì‹œë„
        if HAS_CHARDET:
            try:
                with open(file_path, 'rb') as f:
                    raw_data = f.read(10000)  # ì²˜ìŒ 10KBë§Œ ì½ì–´ì„œ ê°ì§€
                    detected = chardet.detect(raw_data)
                    if detected['encoding'] and detected['confidence'] > 0.7:
                        detected_encoding = detected['encoding']
                        # ê°ì§€ëœ ì¸ì½”ë”©ì„ ë¦¬ìŠ¤íŠ¸ ë§¨ ì•ìœ¼ë¡œ
                        if detected_encoding not in encodings:
                            encodings.insert(0, detected_encoding)
                        elif encodings.index(detected_encoding) > 0:
                            encodings.remove(detected_encoding)
                            encodings.insert(0, detected_encoding)
                        print(f"[ì¸ì½”ë”© ê°ì§€] {file_path}: {detected_encoding} (ì‹ ë¢°ë„: {detected['confidence']:.2f})")
            except Exception as e:
                print(f"[ì¸ì½”ë”© ê°ì§€ ì‹¤íŒ¨] {file_path}: {e}")
        
        # ê° ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ ì½ê¸° ì‹œë„
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding, errors='replace') as f:
                    content = f.read()
                    # ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆëŠ”ì§€ í™•ì¸ (replace ëª¨ë“œì—ì„œë„ ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì¸ì§€)
                    if content.strip() and not ('ï¿½' in content and content.count('ï¿½') > len(content) * 0.1):
                        return content
            except (UnicodeDecodeError, UnicodeError):
                continue
            except Exception as e:
                print(f"âš ï¸ {file_path} ì½ê¸° ì˜¤ë¥˜ ({encoding}): {e}")
                continue
        
        # ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨ ì‹œ ë°”ì´ë„ˆë¦¬ ëª¨ë“œë¡œ ì½ì–´ì„œ ASCII ë¶€ë¶„ë§Œ ì¶”ì¶œ
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read()
                # ASCII ë¬¸ìë§Œ ì¶”ì¶œí•˜ì—¬ ê¸°ë³¸ì ì¸ ì½”ë“œ êµ¬ì¡° ìœ ì§€
                ascii_content = ''.join(chr(b) if 32 <= b <= 126 or b in [9, 10, 13] else ' ' for b in raw_data)
                if ascii_content.strip():
                    print(f"âš ï¸ {file_path}: ASCII ëª¨ë“œë¡œ ì½ê¸° (ì¼ë¶€ ë¬¸ì ì†ì‹¤ ê°€ëŠ¥)")
                    return ascii_content
        except Exception as e:
            print(f"âŒ {file_path}: ëª¨ë“  ì½ê¸° ë°©ë²• ì‹¤íŒ¨ - {e}")
        
        return None

    def remove_comments(self, code):
        """ì†ŒìŠ¤ ì½”ë“œì—ì„œ ì£¼ì„ ì œê±°"""
        # ì—¬ëŸ¬ ì¤„ ì£¼ì„ ì œê±°
        code = re.sub(r"'''(.*?)'''", '', code, flags=re.DOTALL)
        code = re.sub(r'"""(.*?)"""', '', code, flags=re.DOTALL)
        # í•œ ì¤„ ì£¼ì„ ì œê±°
        code = re.sub(r'#.*', '', code)
        return code

    def process_directory(self, root_path):
        """ë””ë ‰í† ë¦¬ë¥¼ ì²˜ë¦¬í•˜ì—¬ ì†ŒìŠ¤ì½”ë“œë¥¼ ì¶”ì¶œí•˜ê³  ë³‘í•©"""
        rows = []

        # root_path ë‚´ë¶€ì˜ ëª¨ë“  í•˜ìœ„ ë””ë ‰í„°ë¦¬ íƒìƒ‰
        for dir_name in os.listdir(root_path):
            dir_path = os.path.join(root_path, dir_name)
            if os.path.isdir(dir_path):
                merged_code = ''
                for root, _, files in os.walk(dir_path):
                    for file in files:
                        if file.endswith('.py'):
                            file_path = os.path.join(root, file)
                            raw_code = self.read_python_file_with_encoding(file_path)
                            if raw_code:
                                cleaned_code = self.remove_comments(raw_code)
                                merged_code += cleaned_code + '\n'
                if merged_code.strip():
                    rows.append([dir_name, merged_code.strip()])
        return rows

    def save_to_csv(self, data, output_file='merged_sourceCode.csv'):
        """ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        # result í´ë”ì— ì €ì¥í•˜ë„ë¡ ê²½ë¡œ ìˆ˜ì •
        if not os.path.isabs(output_file):
            os.makedirs(self.result_dir, exist_ok=True)
            output_file = os.path.join(self.result_dir, output_file)
        
        try:
            with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                # LSTM ë¶„ì„ê³¼ í˜¸í™˜ë˜ë„ë¡ ì»¬ëŸ¼ëª… í†µì¼
                writer.writerow(['package', 'code'])
                for row in data:
                    # ë°ì´í„° ê²€ì¦ í›„ ì €ì¥
                    if len(row) >= 2 and row[1]:
                        writer.writerow([row[0], row[1]])
            print(f"âœ… CSV íŒŒì¼ ì €ì¥ ì„±ê³µ: {output_file}")
        except Exception as e:
            print(f"âŒ CSV íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            # ëŒ€ì²´ ì¸ì½”ë”©ìœ¼ë¡œ ì¬ì‹œë„
            try:
                with open(output_file, 'w', newline='', encoding='cp949') as f:
                    writer = csv.writer(f)
                    writer.writerow(['package', 'code'])
                    for row in data:
                        if len(row) >= 2 and row[1]:
                            writer.writerow([row[0], row[1]])
                print(f"âœ… CSV íŒŒì¼ ì €ì¥ ì„±ê³µ (CP949): {output_file}")
            except Exception as e2:
                print(f"âŒ CSV íŒŒì¼ ì €ì¥ ìµœì¢… ì‹¤íŒ¨: {e2}")

    def extract_zip_and_process_source(self):
        """ZIP íŒŒì¼ ì••ì¶• í•´ì œ ë° ì†ŒìŠ¤ì½”ë“œ ì²˜ë¦¬ (ê°•í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬)"""
        zip_dir = "./python-packages-1757531529324.zip"
        extract_dir = "./extracted_files"
        
        if not os.path.exists(zip_dir):
            print(f"Warning: ZIP íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {zip_dir}")
            return None
        
        try:
            # ì••ì¶• í•´ì œ
            print("ğŸ”„ ZIP íŒŒì¼ ì••ì¶• í•´ì œ ì¤‘...")
            with zipfile.ZipFile(zip_dir, 'r') as zip_ref:
                zip_ref.extractall(extract_dir)
            print(f"âœ… ZIP íŒŒì¼ ì••ì¶• í•´ì œ ì™„ë£Œ: {extract_dir}")
        except Exception as e:
            print(f"âŒ ZIP íŒŒì¼ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}")
            return None
        
        # ì†ŒìŠ¤ì½”ë“œ ì²˜ë¦¬
        root_path = './extracted_files/source'
        if os.path.exists(root_path):
            print(f"ğŸ” ì†ŒìŠ¤ì½”ë“œ ë””ë ‰í† ë¦¬ ìŠ¤ìº”: {root_path}")
            data = self.process_directory(root_path)
            
            if data:
                self.save_to_csv(data)
                print(f"âœ… ì†ŒìŠ¤ì½”ë“œ ì²˜ë¦¬ ì™„ë£Œ: {len(data)}ê°œ íŒ¨í‚¤ì§€")
                
                # í†µê³„ ì •ë³´ ì¶œë ¥
                total_code_length = sum(len(row[1]) for row in data)
                avg_code_length = total_code_length / len(data) if data else 0
                print(f"ğŸ“Š í†µê³„ - ì´ ì½”ë“œ ê¸¸ì´: {total_code_length:,} ë¬¸ì, í‰ê· : {avg_code_length:,.0f} ë¬¸ì/íŒ¨í‚¤ì§€")
                
                return data
            else:
                print("âš ï¸ ì²˜ë¦¬ëœ ì†ŒìŠ¤ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
        else:
            print(f"Warning: ì†ŒìŠ¤ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {root_path}")
            return None

    def parse_name_email(self, text):
        """ì´ë¦„ê³¼ ì´ë©”ì¼ íŒŒì‹±"""
        match = re.match(r"(.*)<(.*@.*)>", text)
        if match:
            name = match.group(1).strip()
            email = match.group(2).strip()
            return name, email
        return None, None

    def parse_metadata(self, file_path):
        """ë©”íƒ€ë°ì´í„° íŒŒì‹± (ë‹¤ì–‘í•œ ì¸ì½”ë”© ì§€ì›)"""
        target_keys = {
            "name", "summary", "author", "author-email", "version",
            "maintainer", "maintainer-email"
        }
        metadata = {}
        
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ë‹¤ì–‘í•œ ì¸ì½”ë”©ìœ¼ë¡œ ì½ê¸° ì‹œë„
        content = self.read_metadata_file_with_encoding(file_path)
        if not content:
            return metadata

        try:
            for line in content.split('\n'):
                line = line.strip()
                if ':' in line:
                    key, value = map(str.strip, line.split(':', 1))
                    key_lower = key.lower()
                    
                    if key_lower in target_keys:
                        metadata[key_lower] = value

            # authorê°€ ì—†ê±°ë‚˜ ê°’ì´ ë¹„ì–´ ìˆì„ ê²½ìš°
            if not metadata.get("author"):
                if metadata.get("author-email"):
                    name, email = self.parse_name_email(metadata["author-email"])
                    if name and email:
                        metadata["author"] = name
                        metadata["author-email"] = email

            # author_emailì´ ì—†ê±°ë‚˜ ê°’ì´ ë¹„ì–´ ìˆì„ ê²½ìš°
            if not metadata.get("author-email"):
                if metadata.get("maintainer-email") and metadata["maintainer-email"].strip():
                    metadata["author-email"] = metadata["maintainer-email"]

            # authorê°€ ì—†ê±°ë‚˜ ê°’ì´ ë¹„ì–´ ìˆì„ ê²½ìš° â†’ maintainerë¡œ ëŒ€ì²´
            if not metadata.get("author"):
                if metadata.get("maintainer") and metadata["maintainer"].strip():
                    metadata["author"] = metadata["maintainer"]
                    
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜ {file_path}: {e}")
            
        return metadata

    def read_metadata_file_with_encoding(self, file_path):
        """ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ë‹¤ì–‘í•œ ì¸ì½”ë”©ìœ¼ë¡œ ì½ê¸°"""
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ì€ ë³´í†µ ASCII/UTF-8ì´ì§€ë§Œ ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„
        encodings = ['utf-8', 'utf-8-sig', 'ascii', 'latin-1', 'cp949', 'euc-kr', 'iso-8859-1']
        
        # chardetìœ¼ë¡œ ì¸ì½”ë”© ê°ì§€ ì‹œë„
        if HAS_CHARDET:
            try:
                with open(file_path, 'rb') as f:
                    raw_data = f.read()
                    detected = chardet.detect(raw_data)
                    if detected['encoding'] and detected['confidence'] > 0.6:
                        detected_encoding = detected['encoding']
                        # ê°ì§€ëœ ì¸ì½”ë”©ì„ ë¦¬ìŠ¤íŠ¸ ë§¨ ì•ìœ¼ë¡œ
                        if detected_encoding not in encodings:
                            encodings.insert(0, detected_encoding)
                        elif encodings.index(detected_encoding) > 0:
                            encodings.remove(detected_encoding)
                            encodings.insert(0, detected_encoding)
            except Exception:
                pass
        
        # ê° ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ ì½ê¸° ì‹œë„
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding, errors='replace') as f:
                    content = f.read()
                    # replace ëª¨ë“œì—ì„œë„ ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì¸ì§€ í™•ì¸
                    if content.strip() and not ('\ufffd' in content and content.count('\ufffd') > len(content) * 0.1):
                        return content
            except Exception:
                continue
        
        print(f"âš ï¸ {file_path}: ë©”íƒ€ë°ì´í„° ì½ê¸° ì‹¤íŒ¨ - ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨")
        return None

    def extract_and_parse_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° íŒŒì‹± (ê°•í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬)"""
        extract_dir = "./extracted_files"
        metadata_dir = os.path.join(extract_dir, "metadata")
        
        if not os.path.exists(metadata_dir):
            print(f"Warning: ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {metadata_dir}")
            return []
        
        print(f"ğŸ” ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬ ìŠ¤ìº”: {metadata_dir}")
        meta_datas = []
        failed_files = []
        
        metadata_files = [f for f in os.listdir(metadata_dir) if f.endswith(".txt")]
        print(f"ğŸ“ ë°œê²¬ëœ ë©”íƒ€ë°ì´í„° íŒŒì¼: {len(metadata_files)}ê°œ")
        
        for file in metadata_files:
            metadata_path = os.path.join(metadata_dir, file)
            try:
                metadata = self.parse_metadata(metadata_path)
                if metadata and metadata.get('name'):  # ìµœì†Œí•œ ì´ë¦„ì´ ìˆëŠ”ì§€ í™•ì¸
                    meta_datas.append(metadata)
                else:
                    failed_files.append(f"{file} (íŒŒì‹±ëœ ë°ì´í„° ì—†ìŒ)")
            except Exception as e:
                failed_files.append(f"{file} ({str(e)})")
                print(f"âš ï¸ ë©”íƒ€ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file} - {e}")
        
        self.meta_datas = meta_datas
        
        # ê²°ê³¼ ìš”ì•½
        print(f"âœ… ë©”íƒ€ë°ì´í„° íŒŒì‹± ì™„ë£Œ: {len(meta_datas)}ê°œ ì„±ê³µ")
        if failed_files:
            print(f"âš ï¸ ì‹¤íŒ¨í•œ íŒŒì¼: {len(failed_files)}ê°œ")
            for fail in failed_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
                print(f"   - {fail}")
            if len(failed_files) > 5:
                print(f"   ... ì™¸ {len(failed_files) - 5}ê°œ ë”")
        
        # ë©”íƒ€ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
        if meta_datas:
            complete_metadata = sum(1 for md in meta_datas if all(md.get(key) for key in ['name', 'version', 'summary']))
            print(f"ğŸ“Š ì™„ì „í•œ ë©”íƒ€ë°ì´í„°: {complete_metadata}/{len(meta_datas)}ê°œ")
        
        return meta_datas

    def get_pepy_downloads(self, package_name, api_key):
        """PePy.tech APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìš´ë¡œë“œ ìˆ˜ ì¡°íšŒ"""
        url = f"https://api.pepy.tech/api/v2/projects/{package_name}"
        headers = {"X-API-Key": api_key}

        try:
            response = requests.get(url, headers=headers)
            if response.status_code == 200:
                data = response.json()
                return data.get("total_downloads", -1)
            else:
                return -1
        except Exception as e:
            return -1

    def get_download_count_bq(self, package_name, service_account_json):
        """BigQueryë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìš´ë¡œë“œ ìˆ˜ ì¡°íšŒ"""
        try:
            client = bigquery.Client.from_service_account_json(service_account_json)

            query = """
            SELECT COUNT(*) AS total_downloads
            FROM `bigquery-public-data.pypi.file_downloads`
            WHERE file.project = @pkg
            """

            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("pkg", "STRING", package_name)
                ]
            )

            query_job = client.query(query, job_config=job_config)
            result = query_job.result()

            for row in result:
                return int(row["total_downloads"])
            return 0
        except Exception as e:
            return -1

    def download_unified(self, package_name):
        """í†µí•©ëœ ë‹¤ìš´ë¡œë“œ ìˆ˜ ì¡°íšŒ"""
        """
        download_count = self.get_pepy_downloads(package_name, "0SRbc/jRFsHYxOShwIQ/N0jtrKf1syMW")
        if download_count == -1:
            download_count = self.get_download_count_bq(package_name, "./plated-mantis-471407-m4-b14f1b3e761d.json")    
        return download_count
        """
        return 0

    def shannon_entropy(self, s):
        """ë¬¸ìì—´ì˜ Shannon ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        if not s:
            return 0
        prob = [v / len(s) for v in Counter(s).values()]
        return -sum(p * math.log2(p) for p in prob)

    def is_valid_version(self, v):
        """ë²„ì „ í˜•ì‹ ê²€ì¦"""
        return bool(re.match(r"^\d+\.\d+\.\d+$", str(v).strip()))

    def get_pypi_top_packages(self):
        """PyPI ìƒìœ„ íŒ¨í‚¤ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            url = "https://hugovk.github.io/top-pypi-packages/top-pypi-packages-30-days.json"
            response = requests.get(url)
            data = response.json()
            return [pkg['project'] for pkg in data['rows']]
        except Exception as e:
            print(f"ì¸ê¸° íŒ¨í‚¤ì§€ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def extract_core_name(self, name):
        """í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ (ì ‘ë‘/ì ‘ë¯¸ì–´ ì œê±°)"""
        return re.split(r"[-_.]", name.lower())[0]

    def is_typo_like(self, pkg_name, legit_list):
        """ì˜¤íƒ€ ê¸°ë°˜ ìœ ì‚¬ì„± íŒë³„"""
        name = self.extract_core_name(pkg_name)
        for legit in legit_list:
            legit_core = self.extract_core_name(legit)
            if levenshtein_distance(name, legit_core) == 1 and abs(len(name) - len(legit_core)) <= 1:
                return True
        return False

    def preprocess_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬"""
        # ë‹¤ìš´ë¡œë“œ ìˆ˜ ìˆ˜ì§‘
        for meta_data in self.meta_datas:
            package_name = meta_data.get("name")
            if package_name:
                download_count = self.download_unified(package_name)
                meta_data["download"] = download_count

        # DataFrame ë³€í™˜
        df = pd.DataFrame(self.meta_datas)

        # ê¸°ë³¸ ì „ì²˜ë¦¬
        df["download"] = df["download"].fillna(0).astype(int)
        df["download_log"] = df["download"].apply(lambda x: np.log1p(x))

        scaler = StandardScaler()
        df["download_scaled"] = scaler.fit_transform(df[["download_log"]])

        # ì„¤ëª… ë¶„ì„
        df["summary"] = df["summary"].fillna("")
        df["summary_length"] = df["summary"].apply(len)
        df["summary_too_short"] = df["summary_length"] < 10
        df["summary_too_long"] = df["summary_length"] > 300
        df["summary_entropy"] = df["summary"].apply(self.shannon_entropy)
        df["summary_low_entropy"] = df["summary_entropy"] < 3.5

        # ë²„ì „ ê²€ì¦
        df["version_valid"] = df["version"].apply(self.is_valid_version)

        # ì˜¤íƒ€ ê¸°ë°˜ íƒì§€
        pypi_packages = self.get_pypi_top_packages()
        df["is_typo_like"] = df["name"].apply(lambda x: self.is_typo_like(x, pypi_packages))

        # ì¶”ê°€ í”¼ì²˜
        df["download_too_low"] = df["download_log"] < df["download_log"].quantile(0.05)
        df["download_too_high"] = df["download_log"] > df["download_log"].quantile(0.95)
        df["is_disposable"] = False

        # MinMaxScalerë¡œ download_log ì •ê·œí™”
        scaler2 = MinMaxScaler()
        df["download_log_scaled"] = 1 - scaler2.fit_transform(df[["download_log"]])

        self.df = df
        return df

    # LSTM ë¶„ì„ ê´€ë ¨ ë©”ì„œë“œë“¤
    def detect_encoding(self, file_path):
        """íŒŒì¼ ì¸ì½”ë”© ê°ì§€"""
        if HAS_CHARDET:
            try:
                with open(file_path, 'rb') as f:
                    sample = f.read(10000)
                    result = chardet.detect(sample)
                    if result['confidence'] > 0.7:
                        return [result['encoding']]
            except Exception as e:
                print(f"ì¸ì½”ë”© ê°ì§€ ì˜¤ë¥˜: {e}")
        
        return ['cp949', 'euc-kr', 'utf-8', 'utf-16', 'latin-1', 'iso-8859-1']

    def read_csv_data(self, csv_file_path):
        """CSV íŒŒì¼ ì½ê¸° (ì¸ì½”ë”© ìë™ ê°ì§€)"""
        print(f"CSV íŒŒì¼ ì½ê¸° ì‹œë„: {csv_file_path}")
        
        encodings = self.detect_encoding(csv_file_path)
        
        for encoding in encodings:
            try:
                print(f"ì¸ì½”ë”© ì‹œë„: {encoding}")
                df = pd.read_csv(csv_file_path, encoding=encoding)
                
                if 'package' not in df.columns or 'code' not in df.columns:
                    # Directoryì™€ MergedCodeWithoutComments ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš° ë³€í™˜
                    if 'Directory' in df.columns and 'MergedCodeWithoutComments' in df.columns:
                        df = df.rename(columns={'Directory': 'package', 'MergedCodeWithoutComments': 'code'})
                    else:
                        print(f"í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {list(df.columns)}")
                        return None
                        
                print(f"CSV íŒŒì¼ ë¡œë“œ ì„±ê³µ: {csv_file_path} ({encoding})")
                print(f"í–‰ ìˆ˜: {len(df)}, ì»¬ëŸ¼: {list(df.columns)}")
                return df
                
            except UnicodeDecodeError:
                continue
            except Exception as e:
                print(f"CSV ì½ê¸° ì˜¤ë¥˜ ({encoding}): {e}")
                continue
        
        print(f"ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨: {encodings}")
        return None

    def load_lstm_models(self):
        """LSTM ëª¨ë¸ê³¼ ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ - ë°˜ë“œì‹œ ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©"""
        global model_mal, label_encoder_mal
        
        try:
            model_path = os.path.join(self.model_save_dir, 'model_mal.pkl')
            
            # TensorFlow/Keras í˜¸í™˜ì„±ì„ ìœ„í•œ ì„¤ì •
            import tensorflow as tf
            from tensorflow import keras
            import pickle
            try:
                import dill
                HAS_DILL = True
            except ImportError:
                HAS_DILL = False
            import sys
            
            print("ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì¤‘...")
            
            # Keras í˜¸í™˜ì„± ì²˜ë¦¬ë¥¼ ìœ„í•œ ëª¨ë“ˆ ë§¤í•‘
            def setup_keras_compatibility():
                """ì¼€ë¼ìŠ¤ í˜¸í™˜ì„±ì„ ìœ„í•œ ëª¨ë“ˆ ì„¤ì •"""
                compatibility_mappings = {
                    'keras.src.models.sequential': 'keras.models',
                    'keras.src.models.model': 'keras.models', 
                    'keras.src.layers': 'keras.layers',
                    'keras.src.layers.core': 'keras.layers',
                    'keras.src.layers.dense': 'keras.layers',
                    'keras.src.layers.rnn': 'keras.layers',
                    'keras.src.layers.rnn.lstm': 'keras.layers',
                    'keras.src.layers.dropout': 'keras.layers',
                    'keras.src.optimizers': 'keras.optimizers',
                    'keras.src.optimizers.adam': 'keras.optimizers',
                    'keras.src.losses': 'keras.losses',
                    'keras.src.metrics': 'keras.metrics',
                    'keras.src.activations': 'keras.activations',
                    'keras.src.regularizers': 'keras.regularizers',
                    'keras.src.constraints': 'keras.constraints',
                    'keras.src.initializers': 'keras.initializers',
                    'keras.src.callbacks': 'keras.callbacks',
                    'keras.src.utils': 'keras.utils',
                    'keras.src.engine': 'keras.engine',
                    'keras.src.engine.sequential': 'keras.models',
                    'keras.src.saving': 'keras.utils'
                }
                
                old_modules = {}
                for old_path, new_path in compatibility_mappings.items():
                    if old_path not in sys.modules:
                        try:
                            # ìƒˆ ëª¨ë“ˆì„ ê°€ì ¸ì™€ì„œ ì´ì „ ê²½ë¡œì— ë§¤í•‘
                            parts = new_path.split('.')
                            module = __import__(parts[0])
                            for part in parts[1:]:
                                if hasattr(module, part):
                                    module = getattr(module, part)
                                else:
                                    break
                            
                            sys.modules[old_path] = module
                            old_modules[old_path] = True
                            
                        except (ImportError, AttributeError) as e:
                            print(f"ëª¨ë“ˆ ë§¤í•‘ ì‹¤íŒ¨: {old_path} -> {new_path}: {e}")
                            pass
                            
                return old_modules
            
            # ë°©ë²• 1: í˜¸í™˜ì„± ì„¤ì • í›„ í‘œì¤€ pickle ë¡œë“œ
            print("ë°©ë²• 1: í˜¸í™˜ì„± ë§¤í•‘ + í‘œì¤€ pickle")
            old_modules = setup_keras_compatibility()
            
            try:
                with open(model_path, 'rb') as f:
                    model_mal = pickle.load(f)
                print("âœ… í˜¸í™˜ì„± ë§¤í•‘ìœ¼ë¡œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                
            except Exception as e:
                print(f"âŒ í˜¸í™˜ì„± ë§¤í•‘ + pickle ì‹¤íŒ¨: {str(e)}")
                
                # ë°©ë²• 2: Sequential í´ë˜ìŠ¤ íŒ¨ì¹˜
                print("ë°©ë²• 2: Sequential í´ë˜ìŠ¤ ì§ì ‘ íŒ¨ì¹˜")
                try:
                    from keras.models import Sequential
                    
                    # Sequential í´ë˜ìŠ¤ì— _unpickle_model ë©”ì„œë“œ ì¶”ê°€
                    if not hasattr(Sequential, '_unpickle_model'):
                        def _unpickle_model(cls, state):
                            model = cls()
                            model.__dict__.update(state)
                            return model
                        Sequential._unpickle_model = classmethod(_unpickle_model)
                    
                    with open(model_path, 'rb') as f:
                        model_mal = pickle.load(f)
                    print("âœ… Sequential íŒ¨ì¹˜ë¡œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                    
                except Exception as e:
                    print(f"âŒ Sequential íŒ¨ì¹˜ ì‹¤íŒ¨: {str(e)}")
                    
                    # ë°©ë²• 3: dill ì‹œë„
                    print("ë°©ë²• 3: dill ë¡œë“œ")
                    try:
                        if not HAS_DILL:
                            raise ImportError("dill íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                        with open(model_path, 'rb') as f:
                            model_mal = dill.load(f)
                        print("âœ… dillë¡œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                        
                    except Exception as e:
                        print(f"âŒ dill ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                        
                        # ë°©ë²• 4: ì¼€ë¼ìŠ¤ ë„¤ì´í‹°ë¸Œ ë¡œë“œ ì‹œë„
                        print("ë°©ë²• 4: Keras ë„¤ì´í‹°ë¸Œ ë¡œë“œ")
                        try:
                            # H5 ë˜ëŠ” SavedModel í˜•ì‹ìœ¼ë¡œ ì €ì¥ëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
                            h5_path = model_path.replace('.pkl', '.h5')
                            savedmodel_path = model_path.replace('.pkl', '_savedmodel')
                            
                            if os.path.exists(h5_path):
                                model_mal = keras.models.load_model(h5_path)
                                print("âœ… H5 í˜•ì‹ìœ¼ë¡œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                            elif os.path.exists(savedmodel_path):
                                model_mal = keras.models.load_model(savedmodel_path)
                                print("âœ… SavedModel í˜•ì‹ìœ¼ë¡œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                            else:
                                raise FileNotFoundError("H5 ë˜ëŠ” SavedModel íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                
                        except Exception as e:
                            print(f"âŒ Keras ë„¤ì´í‹°ë¸Œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                            
                            # ìµœí›„ ë°©ë²•: ì§ì ‘ ë°”ì´íŠ¸ ì¡°ì‘
                            print("ë°©ë²• 5: ì§ì ‘ ë°”ì´íŠ¸ ì¡°ì‘ ì‹œë„")
                            try:
                                with open(model_path, 'rb') as f:
                                    data = f.read()
                                
                                # pickle í—¤ë”ì—ì„œ keras.srcë¥¼ kerasë¡œ êµì²´
                                modified_data = data.replace(b'keras.src.', b'keras.')
                                
                                # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ë¡œë“œ
                                temp_path = model_path + '.temp'
                                with open(temp_path, 'wb') as f:
                                    f.write(modified_data)
                                
                                with open(temp_path, 'rb') as f:
                                    model_mal = pickle.load(f)
                                
                                os.remove(temp_path)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                                print("âœ… ë°”ì´íŠ¸ ì¡°ì‘ìœ¼ë¡œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                                
                            except Exception as e:
                                print(f"âŒ ëª¨ë“  ë°©ë²• ì‹¤íŒ¨: {str(e)}")
                                raise Exception("ê¸°ì¡´ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ì‹œë„ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
            # ëª¨ë“ˆ ì •ë¦¬
            for module_name in old_modules:
                if module_name in sys.modules:
                    del sys.modules[module_name]
            
            # ëª¨ë¸ ë¡œë“œ í™•ì¸
            if model_mal is None:
                raise Exception("ëª¨ë¸ ë¡œë“œ í›„ì—ë„ model_malì´ Noneì…ë‹ˆë‹¤.")
                
            print("âœ… ê¸°ì¡´ LSTM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            print(f"ëª¨ë¸ íƒ€ì…: {type(model_mal)}")
            
            # GPU ìµœì í™” ì„¤ì •
            try:
                if tf.config.list_physical_devices('GPU'):
                    gpus = tf.config.experimental.list_physical_devices('GPU')
                    if gpus:
                        for gpu in gpus:
                            tf.config.experimental.set_memory_growth(gpu, True)
            except Exception:
                pass
            
        except Exception as e:
            print(f"LSTM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
        
        # ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ
        try:
            encoder_path = os.path.join(self.model_save_dir, 'label_encoder_mal.pkl')
            with open(encoder_path, 'rb') as f:
                label_encoder_mal = pickle.load(f)
            print("ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ ì„±ê³µ")
            return True
        except Exception as e:
            print(f"ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def analyze_single_code(self, source_code, package_name):
        """ë‹¨ì¼ ì½”ë“œ LSTM ë¶„ì„"""
        try:
            tokenized_code = tokenize_python(source_code)
            
            if not tokenized_code:
                return {
                    'vulnerability_status': 'Error',
                    'cwe_label': 'parsing_error',
                    'confidence': 0.0
                }
            
            if w2v_model is None:
                return {
                    'vulnerability_status': 'Error',
                    'cwe_label': 'word2vec_error',
                    'confidence': 0.0
                }
            
            # ë²¡í„° ì„ë² ë”©
            padded_code = embed_sequences([tokenized_code], w2v_model, max_length=100)
            
            if padded_code is None or len(padded_code) == 0:
                return {
                    'vulnerability_status': 'Error',
                    'cwe_label': 'embedding_error',
                    'confidence': 0.0
                }
            
            # LSTM ì˜ˆì¸¡
            prediction = model_mal.predict(padded_code, verbose=0)
            
            if prediction is None or len(prediction) == 0:
                return {
                    'vulnerability_status': 'Error',
                    'cwe_label': 'prediction_error',
                    'confidence': 0.0
                }
            
            # ê²°ê³¼ í•´ì„
            predicted_class = np.argmax(prediction, axis=1)[0]
            confidence = float(np.max(prediction))
            
            # ë¼ë²¨ ì¸ì½”ë”ë¡œ í´ë˜ìŠ¤ ì´ë¦„ ë³€í™˜
            if label_encoder_mal is not None:
                try:
                    cwe_label = label_encoder_mal.inverse_transform([predicted_class])[0]
                except Exception as e:
                    print(f"ë¼ë²¨ ë””ì½”ë”© ì˜¤ë¥˜: {e}")
                    cwe_label = f'class_{predicted_class}'
            else:
                cwe_label = f'class_{predicted_class}'
            
            # ì·¨ì•½ì  ìƒíƒœ ê²°ì •
            vulnerability_status = 'Vulnerable' if confidence > 0.5 else 'Safe'
            
            return {
                'vulnerability_status': vulnerability_status,
                'cwe_label': cwe_label,
                'confidence': confidence
            }
            
        except Exception as e:
            print(f"ì½”ë“œ ë¶„ì„ ì˜¤ë¥˜ ({package_name}): {e}")
            return {
                'vulnerability_status': 'Error',
                'cwe_label': 'analysis_error',
                'confidence': 0.0
            }

    def analyze_lstm_codes(self, source_csv='merged_sourceCode.csv'):
        """CSV íŒŒì¼ì˜ ëª¨ë“  ì½”ë“œë¥¼ LSTMìœ¼ë¡œ ë¶„ì„ (ë©”ëª¨ë¦¬ ìµœì í™” í¬í•¨)"""
        csv_path = os.path.join(self.result_dir, source_csv)  # result í´ë”ì—ì„œ ì°¾ë„ë¡ ìˆ˜ì •
        
        if not os.path.exists(csv_path):
            print(f"ì†ŒìŠ¤ì½”ë“œ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
            return None
            
        df = self.read_csv_data(csv_path)
        if df is None:
            return None
        
        print(f"\n=== LSTM ë¶„ì„ ì‹œì‘: {len(df)}ê°œ íŒ¨í‚¤ì§€ ===")
        start_time = time.time()
        
        results = []
        
        for idx, row in df.iterrows():
            package_name = row['package']
            source_code = row['code']
            
            print(f"LSTM ë¶„ì„ ì¤‘ ({idx+1}/{len(df)}): {package_name}")
            
            if pd.isna(source_code) or str(source_code).strip() == '':
                result_row = {
                    'package': package_name,
                    'vulnerability_status': 'Error',
                    'cwe_label': 'Empty Code',
                    'confidence': 0.0
                }
            else:
                analysis_result = self.analyze_single_code(str(source_code), package_name)
                result_row = {
                    'package': package_name,
                    'vulnerability_status': analysis_result['vulnerability_status'],
                    'cwe_label': analysis_result['cwe_label'],
                    'confidence': analysis_result['confidence']
                }
            
            results.append(result_row)
            
            # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ (10ê°œ ë¶„ì„ë§ˆë‹¤)
            if (idx + 1) % 10 == 0:
                try:
                    gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
                    # Keras ì„¸ì…˜ ì •ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)
                    try:
                        K.clear_session()
                        print(f"[ë©”ëª¨ë¦¬ ì •ë¦¬] {idx + 1}ê°œ ë¶„ì„ ì™„ë£Œ í›„ ì„¸ì…˜ ì •ë¦¬")
                    except Exception:
                        pass
                except Exception:
                    pass
        
        end_time = time.time()
        total_time = end_time - start_time
        
        result_df = pd.DataFrame(results)
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
        try:
            gc.collect()
            K.clear_session()
            print("[ìµœì¢… ì •ë¦¬] ë©”ëª¨ë¦¬ ë° ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ")
        except Exception:
            pass
        
        print(f"\n=== LSTM ë¶„ì„ ì™„ë£Œ ===")
        print(f"ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"íŒ¨í‚¤ì§€ë‹¹ í‰ê·  ì‹œê°„: {total_time/len(df):.2f}ì´ˆ")
        
        # ê²°ê³¼ ì €ì¥
        os.makedirs(self.result_dir, exist_ok=True)
        output_path = os.path.join(self.result_dir, 'package_vulnerability_analysis.csv')
        result_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"LSTM ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_path}")
        
        self.lstm_results = result_df
        return result_df

    def integrate_lstm_results(self):
        """LSTM ê²°ê³¼ë¥¼ ë©”ì¸ DataFrameì— í†µí•©"""
        if self.lstm_results is None or self.df is None:
            print("LSTM ê²°ê³¼ ë˜ëŠ” ë©”ì¸ DataFrameì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        # LSTM ê²°ê³¼ë¥¼ ë©”ì¸ DataFrameê³¼ ë³‘í•©
        # vulnerability_statusë¥¼ ìˆ«ìë¡œ ë³€í™˜
        vulnerability_map = {'Vulnerable': 1, 'Not Vulnerable': 0, 'Error': -1}
        self.lstm_results['vulnerability_status_numeric'] = self.lstm_results['vulnerability_status'].map(vulnerability_map)
        
        # CWE ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜ (ê°„ë‹¨í•œ ë°©ì‹)
        cwe_map = {'Benign': 0, 'Empty Code': -1, 'parsing_error': -1, 'model_error': -1, 
                   'embedding_error': -1, 'label_decode_error': -1, 'analysis_error': -1}
        
        # CWE ê°’ë“¤ì„ ìˆ«ìë¡œ ë§¤í•‘ (ê¸°íƒ€ëŠ” 1ë¡œ ì„¤ì •)
        self.lstm_results['cwe_label_numeric'] = self.lstm_results['cwe_label'].apply(
            lambda x: cwe_map.get(x, 1) if x in cwe_map else 1
        )
        
        # íŒ¨í‚¤ì§€ ì´ë¦„ì„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
        merged_df = pd.merge(self.df, 
                            self.lstm_results[['package', 'vulnerability_status_numeric', 'cwe_label_numeric', 'confidence']], 
                            left_on='name', 
                            right_on='package', 
                            how='left')
        
        # ë³‘í•©ë˜ì§€ ì•Šì€ í•­ëª©ì€ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
        merged_df['vulnerability_status_numeric'] = merged_df['vulnerability_status_numeric'].fillna(0)
        merged_df['cwe_label_numeric'] = merged_df['cwe_label_numeric'].fillna(0)
        merged_df['confidence'] = merged_df['confidence'].fillna(0.0)
        
        # ìƒˆë¡œìš´ í”¼ì²˜ ìƒì„± (ë…¸ì´ì¦ˆ ì¶”ê°€ëœ ë²„ì „)
        merged_df['vulnerability_status_noisy'] = merged_df['vulnerability_status_numeric']
        merged_df['cwe_label_noisy'] = merged_df['cwe_label_numeric'] 
        merged_df['threat_level_noisy'] = merged_df.apply(self.combined_threat, axis=1)
        merged_df['download_log_scaled_noisy'] = merged_df['download_log_scaled']
        
        self.df = merged_df
        
        # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥ (result í´ë”ì—)
        os.makedirs(self.result_dir, exist_ok=True)
        output_path = os.path.join(self.result_dir, 'pypi_typo_analysis5.csv')
        self.df.to_csv(output_path, index=False)
        print(f"í†µí•©ëœ ë¶„ì„ ë°ì´í„° ì €ì¥: {output_path}")
        
        return True

    def combined_threat(self, row):
        """ìœ„í˜‘ ìˆ˜ì¤€ ê³„ì‚°"""
        vuln_status = row.get('vulnerability_status_numeric', 0)
        cwe_label = row.get('cwe_label_numeric', 0)
        
        if vuln_status == 1 and cwe_label == 1:
            return 2
        elif vuln_status == 1 or cwe_label == 1:
            return 1
        else:
            return 0

    def load_xgboost_model(self):
        """XGBoost ëª¨ë¸ ë¡œë“œ"""
        global xgboost_model
        
        try:
            model_path = os.path.join(self.current_dir, "xgboost_model.pkl")
            with open(model_path, "rb") as f:
                xgboost_model = pickle.load(f)
            print("XGBoost ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            return True
        except Exception as e:
            print(f"XGBoost ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def predict_malicious(self):
        """XGBoost ëª¨ë¸ë¡œ ì•…ì„± íŒ¨í‚¤ì§€ ì˜ˆì¸¡"""
        if self.df is None:
            print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        if xgboost_model is None:
            print("XGBoost ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        # í”¼ì²˜ ì„ íƒ
        features = [
            "is_disposable", 
            "summary_length", "summary_too_short", "summary_too_long",
            "summary_entropy", "summary_low_entropy", "version_valid",
            "is_typo_like",
            "download_log_scaled_noisy",
            "vulnerability_status_noisy", "threat_level_noisy", "cwe_label_noisy"
        ]
        
        # ëˆ„ë½ëœ í”¼ì²˜ê°€ ìˆëŠ”ì§€ í™•ì¸
        available_features = [f for f in features if f in self.df.columns]
        missing_features = [f for f in features if f not in self.df.columns]
        
        if missing_features:
            print(f"ëˆ„ë½ëœ í”¼ì²˜: {missing_features}")
            # ëˆ„ë½ëœ í”¼ì²˜ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›€
            for feature in missing_features:
                self.df[feature] = 0
        
        X = self.df[features]
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        try:
            self.df["is_malicious"] = xgboost_model.predict(X)
            print("ì•…ì„± íŒ¨í‚¤ì§€ ì˜ˆì¸¡ ì™„ë£Œ")
            print(self.df[["name", "is_malicious"]].head(10))
            return True
        except Exception as e:
            print(f"ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def get_malicious_reasons(self, row):
        """ì•…ì„± íŒë‹¨ ì´ìœ  ìƒì„±"""
        reasons = []
        if row.get("summary_too_short"): reasons.append("ìš”ì•½ì´ ë„ˆë¬´ ì§§ìŒ")
        if row.get("summary_too_long"): reasons.append("ìš”ì•½ì´ ë„ˆë¬´ ê¹€")
        if row.get("summary_low_entropy"): reasons.append("ìš”ì•½ì´ ìë™ ìƒì„±ëœ ê²ƒìœ¼ë¡œ ì˜ì‹¬ë¨")
        if row.get("download_too_low"): reasons.append("ë‹¤ìš´ë¡œë“œ ìˆ˜ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ë‚®ìŒ")
        if row.get("download_too_high"): reasons.append("ë‹¤ìš´ë¡œë“œ ìˆ˜ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ìŒ")
        if not row.get("version_valid"): reasons.append("ë²„ì „ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŒ")
        if row.get("is_disposable"): reasons.append("ì¼íšŒìš© ì´ë©”ì¼ ì‚¬ìš© ì˜ì‹¬")
        if row.get("is_typo_like"): reasons.append("ì¸ê¸° íŒ¨í‚¤ì§€ ì´ë¦„ê³¼ ìœ ì‚¬í•œ ì˜¤íƒ€ ê¸°ë°˜ ì´ë¦„")
        if row.get("vulnerability_status_noisy", 0) == 1: reasons.append("LSTM ë¶„ì„ì—ì„œ ì·¨ì•½ì  ë°œê²¬")
        return reasons

    def get_normal_reasons(self, row):
        """ì •ìƒ íŒë‹¨ ì´ìœ  ìƒì„±"""
        reasons = []
        if not row.get("summary_too_short"): reasons.append("ìš”ì•½ ê¸¸ì´ ì ì ˆí•¨")
        if not row.get("summary_too_long"): reasons.append("ìš”ì•½ì´ ë„ˆë¬´ ê¸¸ì§€ ì•ŠìŒ")
        if not row.get("summary_low_entropy"): reasons.append("ìš”ì•½ì´ ì‚¬ëŒì´ ì‘ì„±í•œ ê²ƒìœ¼ë¡œ ë³´ì„")
        if not row.get("download_too_low"): reasons.append("ë‹¤ìš´ë¡œë“œ ìˆ˜ê°€ ì¶©ë¶„í•¨")
        if not row.get("download_too_high"): reasons.append("ë‹¤ìš´ë¡œë“œ ìˆ˜ê°€ ê³¼ë„í•˜ì§€ ì•ŠìŒ")
        if row.get("version_valid"): reasons.append("ë²„ì „ í˜•ì‹ì´ ì˜¬ë°”ë¦„")
        if not row.get("is_disposable"): reasons.append("ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì´ë©”ì¼ ì‚¬ìš©")
        if not row.get("is_typo_like"): reasons.append("ì´ë¦„ì´ ì¸ê¸° íŒ¨í‚¤ì§€ì™€ ìœ ì‚¬í•˜ì§€ ì•ŠìŒ")
        if row.get("vulnerability_status_noisy", 0) == 0: reasons.append("LSTM ë¶„ì„ì—ì„œ ì·¨ì•½ì  ì—†ìŒ")
        return reasons

    def generate_final_report(self):
        """ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        if self.df is None or 'is_malicious' not in self.df.columns:
            print("ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ì–´ì„œ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        os.makedirs(self.result_dir, exist_ok=True)
        report_path = os.path.join(self.result_dir, "pypi_malicious_reason_report.txt")
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("=== Python íŒ¨í‚¤ì§€ ë³´ì•ˆ ë¶„ì„ ìµœì¢… ë¦¬í¬íŠ¸ ===\n\n")
            
            total_packages = len(self.df)
            malicious_count = self.df['is_malicious'].sum()
            normal_count = total_packages - malicious_count
            
            f.write(f"ì´ ë¶„ì„ íŒ¨í‚¤ì§€ ìˆ˜: {total_packages}\n")
            f.write(f"ì•…ì„± íŒ¨í‚¤ì§€: {malicious_count}ê°œ\n")
            f.write(f"ì •ìƒ íŒ¨í‚¤ì§€: {normal_count}ê°œ\n")
            f.write(f"ì•…ì„± ë¹„ìœ¨: {malicious_count/total_packages*100:.2f}%\n\n")
            
            for _, row in self.df.iterrows():
                pkg_name = row.get("name", "unknown")
                label = row.get("is_malicious", 0)

                if label == 1:
                    reasons = self.get_malicious_reasons(row)
                    status = "âŒ ì•…ì„±"
                else:
                    reasons = self.get_normal_reasons(row)
                    status = "âœ… ì •ìƒ"

                reason_text = " / ".join(reasons) if reasons else "íŒë‹¨ ê¸°ì¤€ ì—†ìŒ"
                f.write(f"{status} ğŸ“¦ {pkg_name}\n")
                f.write(f"â†’ íŒë‹¨ ì´ìœ : {reason_text}\n\n")

        print(f"ğŸ“„ ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        return True

    def save_comprehensive_results(self):
        """ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ í¬í•¨í•œ í†µí•© CSV íŒŒì¼ ìƒì„±"""
        if self.df is None:
            print("ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ì„œ í†µí•© CSVë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.result_dir, exist_ok=True)
        
        # í†µí•© ê²°ê³¼ DataFrame ì¤€ë¹„
        comprehensive_df = self.df.copy()
        
        # LSTM ê²°ê³¼ì™€ ë³‘í•© (ì´ë¯¸ í†µí•©ë˜ì–´ ìˆì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ í‘œì‹œ)
        if self.lstm_results is not None:
            # LSTM ê²°ê³¼ë¥¼ ë” ëª…í™•í•˜ê²Œ í‘œì‹œ
            lstm_merge_df = self.lstm_results[['package', 'vulnerability_status', 'cwe_label', 'confidence']].rename(columns={
                'vulnerability_status': 'lstm_vulnerability_status',
                'cwe_label': 'lstm_cwe_label', 
                'confidence': 'lstm_confidence'
            })
            
            comprehensive_df = pd.merge(comprehensive_df, lstm_merge_df, 
                                       left_on='name', right_on='package', 
                                       how='left', suffixes=('', '_lstm'))
            
            # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
            if 'package_lstm' in comprehensive_df.columns:
                comprehensive_df = comprehensive_df.drop('package_lstm', axis=1)
        
        # ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ì»¬ëŸ¼ ì´ë¦„ ëª…í™•í™”
        if 'is_malicious' in comprehensive_df.columns:
            comprehensive_df = comprehensive_df.rename(columns={'is_malicious': 'xgboost_prediction'})
        
        # ì¤‘ìš”í•œ ì»¬ëŸ¼ë“¤ì„ ì•ìª½ìœ¼ë¡œ ì¬ë°°ì¹˜
        priority_columns = [
            'name',  # íŒ¨í‚¤ì§€ ì´ë¦„
            'xgboost_prediction',  # XGBoost ìµœì¢… ì˜ˆì¸¡
            'lstm_vulnerability_status',  # LSTM ì·¨ì•½ì  ìƒíƒœ
            'lstm_cwe_label',  # LSTM CWE ë¼ë²¨
            'lstm_confidence',  # LSTM ì‹ ë¢°ë„
            'summary',  # íŒ¨í‚¤ì§€ ì„¤ëª…
            'author',  # ì‘ì„±ì
            'author-email',  # ì‘ì„±ì ì´ë©”ì¼
            'version',  # ë²„ì „
            'download',  # ë‹¤ìš´ë¡œë“œ ìˆ˜
            'download_log',  # ë¡œê·¸ ë³€í™˜ëœ ë‹¤ìš´ë¡œë“œ ìˆ˜
        ]
        
        # ìš°ì„ ìˆœìœ„ ì»¬ëŸ¼ë“¤ì´ ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ ì„ íƒ
        available_priority_cols = [col for col in priority_columns if col in comprehensive_df.columns]
        
        # ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ë“¤
        remaining_cols = [col for col in comprehensive_df.columns if col not in available_priority_cols]
        
        # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬
        final_columns = available_priority_cols + remaining_cols
        comprehensive_df = comprehensive_df[final_columns]
        
        # í†µí•© CSV íŒŒì¼ ì €ì¥
        comprehensive_csv_path = os.path.join(self.result_dir, 'comprehensive_analysis_results.csv')
        comprehensive_df.to_csv(comprehensive_csv_path, index=False, encoding='utf-8-sig')
        
        print(f"ğŸ“Š í†µí•© ë¶„ì„ ê²°ê³¼ CSV ì €ì¥: {comprehensive_csv_path}")
        print(f"   - ì´ íŒ¨í‚¤ì§€ ìˆ˜: {len(comprehensive_df)}")
        print(f"   - ì´ ì»¬ëŸ¼ ìˆ˜: {len(comprehensive_df.columns)}")
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        if 'xgboost_prediction' in comprehensive_df.columns:
            malicious_count = comprehensive_df['xgboost_prediction'].sum()
            print(f"   - XGBoost ì˜ˆì¸¡ ì•…ì„±: {malicious_count}ê°œ")
            print(f"   - XGBoost ì˜ˆì¸¡ ì •ìƒ: {len(comprehensive_df) - malicious_count}ê°œ")
        
        if 'lstm_vulnerability_status' in comprehensive_df.columns:
            lstm_vulnerable = (comprehensive_df['lstm_vulnerability_status'] == 'Vulnerable').sum()
            print(f"   - LSTM ì·¨ì•½ì  ë°œê²¬: {lstm_vulnerable}ê°œ")
        
        # ì£¼ìš” ì»¬ëŸ¼ ëª©ë¡ ì¶œë ¥
        print("   - ì£¼ìš” ì»¬ëŸ¼ë“¤:")
        for i, col in enumerate(available_priority_cols[:10]):  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
            print(f"     {i+1}. {col}")
        
        if len(available_priority_cols) > 10:
            print(f"     ... ì™¸ {len(final_columns) - 10}ê°œ ì»¬ëŸ¼")
        
        return comprehensive_csv_path

    def cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        global model_mal, label_encoder_mal, xgboost_model
        
        try:
            K.clear_session()
        except:
            pass
        
        # ì „ì—­ ë³€ìˆ˜ ì •ë¦¬
        for obj_name in ['model_mal', 'label_encoder_mal', 'xgboost_model']:
            if obj_name in globals():
                try:
                    del globals()[obj_name]
                except:
                    pass
        
        gc.collect()
        print("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ì•ˆì •ì„± ê°•í™”)"""
    print("=== Python íŒ¨í‚¤ì§€ ë³´ì•ˆ ë¶„ì„ ë„êµ¬ (Final Unified) ===\n")
    
    analyzer = FinalUnifiedAnalyzer()
    successful_steps = []
    
    try:
        # 0. ì´ˆê¸° í™˜ê²½ í™•ì¸ ë° ì„¤ì •
        print("0ï¸âƒ£ ì´ˆê¸° í™˜ê²½ ì„¤ì • ë° í™•ì¸...")
        try:
            # TensorFlow ì´ˆê¸°í™” í™•ì¸
            import tensorflow as tf
            print(f"[í™˜ê²½ í™•ì¸] TensorFlow ë²„ì „: {tf.__version__}")
            if tf.config.list_physical_devices('GPU'):
                print("[í™˜ê²½ í™•ì¸] GPU ì‚¬ìš© ê°€ëŠ¥")
            else:
                print("[í™˜ê²½ í™•ì¸] CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        except Exception as env_error:
            print(f"[í™˜ê²½ ê²½ê³ ] TensorFlow ì„¤ì • ì´ìŠˆ: {env_error}")
        
        # 1. ZIP íŒŒì¼ í•´ì œ ë° ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ
        print("\n1ï¸âƒ£ ZIP íŒŒì¼ í•´ì œ ë° ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ...")
        try:
            source_data = analyzer.extract_zip_and_process_source()
            if source_data is None:
                print("âŒ ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ ì‹¤íŒ¨ - ZIP íŒŒì¼ ë˜ëŠ” ì†ŒìŠ¤ ê²½ë¡œ í™•ì¸ í•„ìš”")
                return
            successful_steps.append("ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ")
            print(f"âœ… ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ ì„±ê³µ: {len(source_data)}ê°œ íŒ¨í‚¤ì§€")
        except Exception as e:
            print(f"âŒ ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return
        
        # 2. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° íŒŒì‹±
        print("\n2ï¸âƒ£ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° íŒŒì‹±...")
        try:
            meta_data = analyzer.extract_and_parse_metadata()
            if not meta_data:
                print("âŒ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ - metadata í´ë” í™•ì¸ í•„ìš”")
                return
            successful_steps.append("ë©”íƒ€ë°ì´í„° ì¶”ì¶œ")
            print(f"âœ… ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì„±ê³µ: {len(meta_data)}ê°œ íŒ¨í‚¤ì§€")
        except Exception as e:
            print(f"âŒ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return
        
        # 3. ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬
        print("\n3ï¸âƒ£ ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬...")
        try:
            df = analyzer.preprocess_metadata()
            if df is None:
                print("âŒ ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨")
                return
            successful_steps.append("ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬")
            print(f"âœ… ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬ ì„±ê³µ: {len(df)}ê°œ íŒ¨í‚¤ì§€")
        except Exception as e:
            print(f"âŒ ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return
        
        # 4. LSTM ëª¨ë¸ ë¡œë“œ (ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜)
        print("\n4ï¸âƒ£ LSTM ëª¨ë¸ ë¡œë“œ...")
        lstm_load_success = False
        for attempt in range(3):
            try:
                print(f"[ì‹œë„ {attempt + 1}/3] ëª¨ë¸ ë¡œë“œ ì¤‘...")
                if analyzer.load_lstm_models():
                    lstm_load_success = True
                    successful_steps.append("LSTM ëª¨ë¸ ë¡œë“œ")
                    print("âœ… LSTM ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
                    break
                else:
                    print(f"[ì‹œë„ {attempt + 1}/3] ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ì¬ì‹œë„...")
                    time.sleep(1)
            except Exception as e:
                print(f"[ì‹œë„ {attempt + 1}/3] ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
                if attempt < 2:
                    print("ì¬ì‹œë„ ì¤‘...")
                    time.sleep(2)
                    
        if not lstm_load_success:
            print("âŒ LSTM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - model_mal.pkl ë° label_encoder_mal.pkl íŒŒì¼ í™•ì¸")
            return
        
        # 5. LSTM ì½”ë“œ ë¶„ì„
        print("\n5ï¸âƒ£ LSTM ì½”ë“œ ë¶„ì„...")
        try:
            lstm_results = analyzer.analyze_lstm_codes()
            if lstm_results is None:
                print("âŒ LSTM ë¶„ì„ ì‹¤íŒ¨ - ì†ŒìŠ¤ì½”ë“œ ë°ì´í„° í™•ì¸ í•„ìš”")
                return
            successful_steps.append("LSTM ë¶„ì„")
            print("âœ… LSTM ë¶„ì„ ì„±ê³µ")
        except Exception as e:
            print(f"âŒ LSTM ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return
        
        # 6. LSTM ê²°ê³¼ í†µí•©
        print("\n6ï¸âƒ£ LSTM ê²°ê³¼ í†µí•©...")
        try:
            if not analyzer.integrate_lstm_results():
                print("âŒ ê²°ê³¼ í†µí•© ì‹¤íŒ¨")
                return
            successful_steps.append("LSTM ê²°ê³¼ í†µí•©")
            print("âœ… LSTM ê²°ê³¼ í†µí•© ì„±ê³µ")
        except Exception as e:
            print(f"âŒ LSTM ê²°ê³¼ í†µí•© ì¤‘ ì˜¤ë¥˜: {e}")
            return
        
        # 7. XGBoost ëª¨ë¸ ë¡œë“œ
        print("\n7ï¸âƒ£ XGBoost ëª¨ë¸ ë¡œë“œ...")
        try:
            if not analyzer.load_xgboost_model():
                print("âŒ XGBoost ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                return
            successful_steps.append("XGBoost ëª¨ë¸ ë¡œë“œ")
            print("âœ… XGBoost ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"âŒ XGBoost ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return
        
        # 8. ìµœì¢… ì•…ì„± ì˜ˆì¸¡
        print("\n8ï¸âƒ£ ìµœì¢… ì•…ì„± íŒ¨í‚¤ì§€ ì˜ˆì¸¡...")
        try:
            if not analyzer.predict_malicious():
                print("âŒ ì˜ˆì¸¡ ì‹¤íŒ¨")
                return
            successful_steps.append("ì•…ì„± ì˜ˆì¸¡")
            print("âœ… ì•…ì„± ì˜ˆì¸¡ ì„±ê³µ")
        except Exception as e:
            print(f"âŒ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {e}")
            return
        
        # 9. ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
        print("\n9ï¸âƒ£ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±...")
        try:
            if not analyzer.generate_final_report():
                print("âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨")
                return
            successful_steps.append("ë¦¬í¬íŠ¸ ìƒì„±")
            print("âœ… ë¦¬í¬íŠ¸ ìƒì„± ì„±ê³µ")
        except Exception as e:
            print(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return
        
        # 10. í†µí•© CSV íŒŒì¼ ìƒì„±
        print("\nğŸ”Ÿ í†µí•© ë¶„ì„ ê²°ê³¼ CSV ìƒì„±...")
        try:
            comprehensive_csv = analyzer.save_comprehensive_results()
            if not comprehensive_csv:
                print("âŒ í†µí•© CSV ìƒì„± ì‹¤íŒ¨")
                return
            successful_steps.append("í†µí•© CSV ìƒì„±")
            print("âœ… í†µí•© CSV ìƒì„± ì„±ê³µ")
        except Exception as e:
            print(f"âŒ í†µí•© CSV ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return
        
        print("\n" + "="*60)
        print("ğŸ‰ ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("="*60)
        print(f"\nâœ… ì„±ê³µí•œ ë‹¨ê³„ë“¤: {', '.join(successful_steps)}")
        print("\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤ (./result í´ë”):")
        print("   â”œâ”€â”€ merged_sourceCode.csv: ë³‘í•©ëœ ì†ŒìŠ¤ì½”ë“œ")
        print("   â”œâ”€â”€ pypi_typo_analysis5.csv: í†µí•© ë¶„ì„ ë°ì´í„°")
        print("   â”œâ”€â”€ package_vulnerability_analysis.csv: LSTM ë¶„ì„ ê²°ê³¼")
        print("   â”œâ”€â”€ comprehensive_analysis_results.csv: ëª¨ë“  ê²°ê³¼ í†µí•© CSV")
        print("   â””â”€â”€ pypi_malicious_reason_report.txt: ìµœì¢… íŒë‹¨ ë¦¬í¬íŠ¸")
        print("="*60)
        
    except KeyboardInterrupt:
        print(f"\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        if successful_steps:
            print(f"âœ… ì¤‘ë‹¨ ì „ê¹Œì§€ ì™„ë£Œëœ ë‹¨ê³„: {', '.join(successful_steps)}")
    except Exception as e:
        print(f"\nâŒ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        if successful_steps:
            print(f"âœ… ì˜¤ë¥˜ ë°œìƒ ì „ê¹Œì§€ ì™„ë£Œëœ ë‹¨ê³„: {', '.join(successful_steps)}")
        print("\nğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
        import traceback
        traceback.print_exc()
    finally:
        # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
        print("\nğŸ”§ ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘...")
        try:
            analyzer.cleanup()
            print("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except Exception as cleanup_error:
            print(f"âš ï¸ ì •ë¦¬ ì¤‘ ê²½ê³ : {cleanup_error}")
        print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()

