"""
í†µí•©ëœ Python íŒ¨í‚¤ì§€ ë³´ì•ˆ ë¶„ì„ ë„êµ¬ (Final Unified Version)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì„ í†µí•©í•©ë‹ˆë‹¤:
1. ë©”íƒ€ë°ì´í„° íŒŒì¼ íŒŒì‹± í›„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì €ì¥
2. ëª¨ë“  ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ ë° ì „ì²˜ë¦¬
3. LSTM ê¸°ë°˜ ì½”ë“œ ì·¨ì•½ì  ë¶„ì„
4. XGBoost ëª¨ë¸ì„ ì´ìš©í•œ ìµœì¢… ì•…ì„± íŒ¨í‚¤ì§€ íŒë‹¨
5. ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±

ì‚¬ìš©ë²•:
    python final_unified.py

ì¶œë ¥:
- merged_sourceCode.csv: ë³‘í•©ëœ ì†ŒìŠ¤ì½”ë“œ
- pypi_typo_analysis5.csv: ë¶„ì„ ê²°ê³¼ ë°ì´í„°
- package_vulnerability_analysis.csv: LSTM ë¶„ì„ ê²°ê³¼
- pypi_malicious_reason_report.txt: ìµœì¢… íŒë‹¨ ë¦¬í¬íŠ¸
"""

import os
import csv
import re
import zipfile
import pickle
import sys
import numpy as np
import pandas as pd
import gc
import math
import time
import requests
from typing import Optional, Dict, List, Tuple, Any
from collections import Counter
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from google.cloud import bigquery
from google.oauth2 import service_account
from google.api_core import exceptions as gcp_exceptions

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # TensorFlow ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # oneDNN ìµœì í™” ë¹„í™œì„±í™”
import warnings
warnings.filterwarnings('ignore')  # ëª¨ë“  ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°

# LSTM ê´€ë ¨ import
try:
    import chardet
    HAS_CHARDET = True
except ImportError:
    HAS_CHARDET = False

# ì¼€ë¼ìŠ¤ import - í˜¸í™˜ì„±ì„ ìœ„í•œ ë‹¤ì¤‘ ë°©ì‹ ì‹œë„
try:
    from tensorflow.keras import backend as K
    from tensorflow import keras
except ImportError:
    try:
        import keras
        from keras import backend as K
    except ImportError:
        print("ì¼€ë¼ìŠ¤ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜¸í™˜ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        K = None
        keras = None

# preprocess import ì‹œ ì¶œë ¥ ë©”ì‹œì§€ ì„ì‹œ ìˆ¨ê¸°ê¸°
import sys
from io import StringIO

# stdoutì„ ì„ì‹œë¡œ ë¦¬ë””ë ‰ì…˜í•˜ì—¬ Word2Vec ë¡œë“œ ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
old_stdout = sys.stdout
sys.stdout = StringIO()
try:
    from preprocess import tokenize_python, embed_sequences, w2v_model
finally:
    sys.stdout = old_stdout

# Levenshtein distance import (ì¡°ìš©íˆ ì²˜ë¦¬)
try:
    from Levenshtein import distance as levenshtein_distance
except ImportError:
    # ì¡°ìš©íˆ ëŒ€ì²´ í•¨ìˆ˜ ì‚¬ìš©
    def levenshtein_distance(a, b):
        return abs(len(a) - len(b))  # ê°„ë‹¨í•œ ëŒ€ì²´ í•¨ìˆ˜

# Get the current directory
current_dir = os.path.dirname(os.path.abspath(__file__))
model_save_dir = os.path.join(current_dir, 'model')
result_dir = os.path.join(current_dir, 'result')

# Global variables for models
model_mal = None
label_encoder_mal = None
xgboost_model = None

class FinalUnifiedAnalyzer:
    def __init__(self):
        self.current_dir = current_dir
        self.model_save_dir = model_save_dir
        self.result_dir = result_dir
        self.meta_datas = []
        self.df = None
        self.lstm_results = None
        
    def remove_comments(self, code):
        """ì†ŒìŠ¤ ì½”ë“œì—ì„œ ì£¼ì„ ì œê±°"""
        # ì—¬ëŸ¬ ì¤„ ì£¼ì„ ì œê±°
        code = re.sub(r"'''(.*?)'''", '', code, flags=re.DOTALL)
        code = re.sub(r'"""(.*?)"""', '', code, flags=re.DOTALL)
        # í•œ ì¤„ ì£¼ì„ ì œê±°
        code = re.sub(r'#.*', '', code)
        return code.strip()

    def extract_zip_and_process_source(self):
        """ZIP íŒŒì¼ í•´ì œ ë° ì†ŒìŠ¤ì½”ë“œ ì²˜ë¦¬"""
        print("1ï¸âƒ£ ZIP íŒŒì¼ í•´ì œ ë° ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ...")
        
        zip_file_path = 'python-packages-1757531529324.zip'
        extract_path = './extracted_files'
        
        # ZIP íŒŒì¼ í•´ì œ
        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)
        
        source_path = os.path.join(extract_path, 'source')
        
        # ì†ŒìŠ¤ì½”ë“œ ì²˜ë¦¬ ë° CSV ì €ì¥
        source_data = self.process_directory(source_path)
        self.save_to_csv(source_data, 'merged_sourceCode.csv')
        
        print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {len(source_data)}ê°œ ë””ë ‰í„°ë¦¬ ì²˜ë¦¬ë¨")

    def process_directory(self, root_path):
        """ë””ë ‰í„°ë¦¬ ë‚´ Python íŒŒì¼ë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬"""
        data = []
        
        for dir_name in os.listdir(root_path):
            dir_path = os.path.join(root_path, dir_name)
            if os.path.isdir(dir_path):
                merged_code = ""
                
                # ëª¨ë“  .py íŒŒì¼ì„ ì°¾ì•„ì„œ ë³‘í•©
                for root, dirs, files in os.walk(dir_path):
                    for file in files:
                        if file.endswith('.py'):
                            file_path = os.path.join(root, file)
                            try:
                                with open(file_path, 'r', encoding='utf-8') as f:
                                    content = f.read()
                                    # ì£¼ì„ ì œê±° í›„ ë³‘í•©
                                    merged_code += self.remove_comments(content) + "\n"
                            except Exception as e:
                                print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")
                                continue
                
                if merged_code.strip():
                    data.append([dir_name, merged_code.strip()])
        
        return data

    def save_to_csv(self, data, output_file='merged_sourceCode.csv'):
        """ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥"""
        os.makedirs(self.result_dir, exist_ok=True)
        output_path = os.path.join(self.result_dir, output_file)
        
        with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            if output_file == 'merged_sourceCode.csv':
                writer.writerow(['package', 'code'])  # í—¤ë” ë³€ê²½
                for row in data:
                    writer.writerow([row[0], row[1]])  # Directory -> packageë¡œ ë§¤í•‘
            else:
                writer.writerow(['Directory', 'MergedCodeWithoutComments'])
                writer.writerows(data)

    def extract_and_parse_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° íŒŒì‹±"""
        print("2ï¸âƒ£ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° íŒŒì‹±...")
        
        extract_path = './extracted_files'
        metadata_path = os.path.join(extract_path, 'metadata')
        
        self.meta_datas = []
        
        for filename in os.listdir(metadata_path):
            if filename.endswith('.txt'):
                filepath = os.path.join(metadata_path, filename)
                metadata = self.parse_metadata_file(filepath)
                if metadata:
                    self.meta_datas.append(metadata)
        
        print(f"âœ… ë©”íƒ€ë°ì´í„° íŒŒì‹± ì™„ë£Œ: {len(self.meta_datas)}ê°œ")

    def parse_metadata_file(self, filepath):
        """ê°œë³„ ë©”íƒ€ë°ì´í„° íŒŒì¼ íŒŒì‹±"""
        metadata = {}
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # íŒ¨í‚¤ì§€ëª… ì¶”ì¶œ (íŒŒì¼ëª…ì—ì„œ)
            package_name = os.path.basename(filepath).replace('.txt', '')
            metadata['name'] = package_name
            
            # ê¸°ë³¸ í•„ë“œë“¤ ì¶”ì¶œ
            fields = ['summary', 'version', 'author', 'author-email', 'maintainer',
                     'maintainer-email', 'license', 'platform', 'classifier',
                     'requires-dist', 'project-url', 'description']
            
            for field in fields:
                pattern = rf'^{re.escape(field)}:\s*(.+)$'
                matches = re.findall(pattern, content, re.MULTILINE | re.IGNORECASE)
                if matches:
                    if field in ['classifier', 'requires-dist', 'project-url']:
                        metadata[field] = matches  # ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
                    else:
                        metadata[field] = matches[0].strip()
                else:
                    metadata[field] = None
            
            return metadata
            
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜ {filepath}: {e}")
            return None

    def preprocess_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬"""
        print("3ï¸âƒ£ ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬...")
        
        if not self.meta_datas:
            print("âŒ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # DataFrame ìƒì„±
        self.df = pd.DataFrame(self.meta_datas)
        
        # ë‹¤ìš´ë¡œë“œ ìˆ˜ ì¡°íšŒ
        self.df = self.get_download_counts(self.df)
        
        # ì¶”ê°€ í”¼ì²˜ ìƒì„±
        self.df = self.engineer_features(self.df)
        
        print(f"âœ… ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(self.df)}ê°œ íŒ¨í‚¤ì§€")

    def get_download_counts(self, df):
        """BigQueryë¥¼ í†µí•œ ë‹¤ìš´ë¡œë“œ ìˆ˜ ì¡°íšŒ"""
        try:
            # Google Cloud ì¸ì¦ ì„¤ì •
            credentials = service_account.Credentials.from_service_account_file(
                'plated-mantis-471407-m4-b14f1b3e761d.json'
            )
            client = bigquery.Client(credentials=credentials, project=credentials.project_id)
            
            download_counts = []
            
            for _, row in df.iterrows():
                package_name = row['name']
                
                query = f"""
                SELECT file.project, COUNT(*) as download_count
                FROM `bigquery-public-data.pypi.file_downloads`
                WHERE file.project = '{package_name}'
                  AND DATE(timestamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
                GROUP BY file.project
                """
                
                try:
                    query_job = client.query(query)
                    results = query_job.result()
                    
                    count = 0
                    for result in results:
                        count = result.download_count
                        break
                    
                    download_counts.append(count)
                    
                except Exception as e:
                    print(f"ë‹¤ìš´ë¡œë“œ ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨ {package_name}: {e}")
                    download_counts.append(0)
            
            df['download'] = download_counts
            
        except Exception as e:
            print(f"BigQuery ì„¤ì • ì˜¤ë¥˜: {e}")
            df['download'] = 0
        
        return df

    def engineer_features(self, df):
        """í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"""
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ í”¼ì²˜
        df['summary_length'] = df['summary'].fillna('').astype(str).apply(len)
        df['version_parts'] = df['version'].fillna('').astype(str).apply(lambda x: len(x.split('.')))
        
        # ì´ë©”ì¼ ë„ë©”ì¸ ì¶”ì¶œ
        def extract_domain(email):
            if pd.isna(email) or '@' not in str(email):
                return 'unknown'
            return str(email).split('@')[-1]
        
        df['author_domain'] = df['author-email'].apply(extract_domain)
        
        # ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê³„ì‚° (íŒ¨í‚¤ì§€ëª… ìœ ì‚¬ë„)
        popular_packages = ['requests', 'numpy', 'pandas', 'flask', 'django', 'tensorflow']
        
        def min_levenshtein_distance(name, popular_list):
            if pd.isna(name):
                return 100
            min_dist = float('inf')
            for popular in popular_list:
                dist = levenshtein_distance(str(name).lower(), popular.lower())
                min_dist = min(min_dist, dist)
            return min_dist
        
        df['min_levenshtein_distance'] = df['name'].apply(
            lambda x: min_levenshtein_distance(x, popular_packages)
        )
        
        return df

    # LSTM ë¶„ì„ ê´€ë ¨ ë©”ì„œë“œë“¤
    def detect_encoding(self, file_path):
        """íŒŒì¼ ì¸ì½”ë”© ê°ì§€"""
        detected_encodings = []
        
        # chardetì„ ì´ìš©í•œ ì¸ì½”ë”© ê°ì§€
        if HAS_CHARDET:
            try:
                with open(file_path, 'rb') as f:
                    sample = f.read(10000)
                    result = chardet.detect(sample)
                    if result and result['encoding'] and result['confidence'] > 0.7:
                        detected_encodings.append(result['encoding'])
                        print(f"ê°ì§€ëœ ì¸ì½”ë”©: {result['encoding']} (ì‹ ë¢°ë„: {result['confidence']:.2f})")
            except Exception as e:
                print(f"ì¸ì½”ë”© ê°ì§€ ì˜¤ë¥˜: {e}")
        
        # ê¸°ë³¸ ì¸ì½”ë”© ë¦¬ìŠ¤íŠ¸ (Windows í™˜ê²½ ìš°ì„ )
        default_encodings = ['utf-8', 'cp949', 'euc-kr', 'utf-8-sig', 'latin-1', 'iso-8859-1', 'utf-16']
        
        # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œ ìœ ì§€
        all_encodings = detected_encodings.copy()
        for enc in default_encodings:
            if enc not in all_encodings:
                all_encodings.append(enc)
        
        return all_encodings

    def read_csv_data(self, csv_file_path):
        """CSV íŒŒì¼ ì½ê¸° (ì¸ì½”ë”© ìë™ ê°ì§€)"""
        print(f"CSV íŒŒì¼ ì½ê¸° ì‹œë„: {csv_file_path}")
        
        encodings = self.detect_encoding(csv_file_path)
        
        for encoding in encodings:
            try:
                print(f"ì¸ì½”ë”© ì‹œë„: {encoding}")
                df = pd.read_csv(csv_file_path, encoding=encoding)
                
                if 'package' not in df.columns or 'code' not in df.columns:
                    # Directoryì™€ MergedCodeWithoutComments ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš° ë³€í™˜
                    if 'Directory' in df.columns and 'MergedCodeWithoutComments' in df.columns:
                        df = df.rename(columns={'Directory': 'package', 'MergedCodeWithoutComments': 'code'})
                    else:
                        print(f"í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {list(df.columns)}")
                        return None
                        
                print(f"CSV íŒŒì¼ ë¡œë“œ ì„±ê³µ: {csv_file_path} ({encoding})")
                print(f"í–‰ ìˆ˜: {len(df)}, ì»¬ëŸ¼: {list(df.columns)}")
                return df
                
            except UnicodeDecodeError:
                continue
            except Exception as e:
                print(f"CSV ì½ê¸° ì˜¤ë¥˜ ({encoding}): {e}")
                continue
        
        print(f"ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨: {encodings}")
        return None

    def load_lstm_models(self):
        """LSTM ëª¨ë¸ê³¼ ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ - ë°˜ë“œì‹œ ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©"""
        global model_mal, label_encoder_mal
        
        try:
            model_path = os.path.join(self.model_save_dir, 'model_mal.pkl')
            
            # TensorFlow/Keras í˜¸í™˜ì„±ì„ ìœ„í•œ ì„¤ì •
            import tensorflow as tf
            from tensorflow import keras
            import pickle
            import dill
            import sys
            
            print("ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì¤‘...")
            
            # Keras í˜¸í™˜ì„± ì²˜ë¦¬ë¥¼ ìœ„í•œ ëª¨ë“ˆ ë§¤í•‘
            def setup_keras_compatibility():
                """ì¼€ë¼ìŠ¤ í˜¸í™˜ì„±ì„ ìœ„í•œ ëª¨ë“ˆ ì„¤ì •"""
                compatibility_mappings = {
                    'keras.src.models.sequential': 'keras.models',
                    'keras.src.models.model': 'keras.models', 
                    'keras.src.layers': 'keras.layers',
                    'keras.src.layers.core': 'keras.layers',
                    'keras.src.layers.dense': 'keras.layers',
                    'keras.src.layers.rnn': 'keras.layers',
                    'keras.src.layers.rnn.lstm': 'keras.layers',
                    'keras.src.layers.dropout': 'keras.layers',
                    'keras.src.optimizers': 'keras.optimizers',
                    'keras.src.optimizers.adam': 'keras.optimizers',
                    'keras.src.losses': 'keras.losses',
                    'keras.src.metrics': 'keras.metrics',
                    'keras.src.activations': 'keras.activations',
                    'keras.src.regularizers': 'keras.regularizers',
                    'keras.src.constraints': 'keras.constraints',
                    'keras.src.initializers': 'keras.initializers',
                    'keras.src.callbacks': 'keras.callbacks',
                    'keras.src.utils': 'keras.utils',
                    'keras.src.engine': 'keras.engine',
                    'keras.src.engine.sequential': 'keras.models',
                    'keras.src.saving': 'keras.utils'
                }
                
                old_modules = {}
                for old_path, new_path in compatibility_mappings.items():
                    if old_path not in sys.modules:
                        try:
                            # ìƒˆ ëª¨ë“ˆì„ ê°€ì ¸ì™€ì„œ ì´ì „ ê²½ë¡œì— ë§¤í•‘
                            parts = new_path.split('.')
                            module = __import__(parts[0])
                            for part in parts[1:]:
                                if hasattr(module, part):
                                    module = getattr(module, part)
                                else:
                                    break
                            
                            sys.modules[old_path] = module
                            old_modules[old_path] = True
                            
                        except (ImportError, AttributeError) as e:
                            print(f"ëª¨ë“ˆ ë§¤í•‘ ì‹¤íŒ¨: {old_path} -> {new_path}: {e}")
                            pass
                            
                return old_modules
            
            # ë°©ë²• 1: í˜¸í™˜ì„± ì„¤ì • í›„ í‘œì¤€ pickle ë¡œë“œ
            print("ë°©ë²• 1: í˜¸í™˜ì„± ë§¤í•‘ + í‘œì¤€ pickle")
            old_modules = setup_keras_compatibility()
            
            try:
                with open(model_path, 'rb') as f:
                    model_mal = pickle.load(f)
                print("âœ… í˜¸í™˜ì„± ë§¤í•‘ìœ¼ë¡œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                
            except Exception as e:
                print(f"âŒ í˜¸í™˜ì„± ë§¤í•‘ + pickle ì‹¤íŒ¨: {str(e)}")
                
                # ë°©ë²• 2: Sequential í´ë˜ìŠ¤ íŒ¨ì¹˜
                print("ë°©ë²• 2: Sequential í´ë˜ìŠ¤ ì§ì ‘ íŒ¨ì¹˜")
                try:
                    from keras.models import Sequential
                    
                    # Sequential í´ë˜ìŠ¤ì— _unpickle_model ë©”ì„œë“œ ì¶”ê°€
                    if not hasattr(Sequential, '_unpickle_model'):
                        def _unpickle_model(cls, state):
                            model = cls()
                            model.__dict__.update(state)
                            return model
                        Sequential._unpickle_model = classmethod(_unpickle_model)
                    
                    with open(model_path, 'rb') as f:
                        model_mal = pickle.load(f)
                    print("âœ… Sequential íŒ¨ì¹˜ë¡œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                    
                except Exception as e:
                    print(f"âŒ Sequential íŒ¨ì¹˜ ì‹¤íŒ¨: {str(e)}")
                    
                    # ë°©ë²• 3: dill ì‹œë„
                    print("ë°©ë²• 3: dill ë¡œë“œ")
                    try:
                        with open(model_path, 'rb') as f:
                            model_mal = dill.load(f)
                        print("âœ… dillë¡œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                        
                    except Exception as e:
                        print(f"âŒ dill ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                        
                        # ë°©ë²• 4: ì¼€ë¼ìŠ¤ ë„¤ì´í‹°ë¸Œ ë¡œë“œ ì‹œë„
                        print("ë°©ë²• 4: Keras ë„¤ì´í‹°ë¸Œ ë¡œë“œ")
                        try:
                            # H5 ë˜ëŠ” SavedModel í˜•ì‹ìœ¼ë¡œ ì €ì¥ëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
                            h5_path = model_path.replace('.pkl', '.h5')
                            savedmodel_path = model_path.replace('.pkl', '_savedmodel')
                            
                            if os.path.exists(h5_path):
                                model_mal = keras.models.load_model(h5_path)
                                print("âœ… H5 í˜•ì‹ìœ¼ë¡œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                            elif os.path.exists(savedmodel_path):
                                model_mal = keras.models.load_model(savedmodel_path)
                                print("âœ… SavedModel í˜•ì‹ìœ¼ë¡œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                            else:
                                raise FileNotFoundError("H5 ë˜ëŠ” SavedModel íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                
                        except Exception as e:
                            print(f"âŒ Keras ë„¤ì´í‹°ë¸Œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                            
                            # ìµœí›„ ë°©ë²•: ì§ì ‘ ë°”ì´íŠ¸ ì¡°ì‘
                            print("ë°©ë²• 5: ì§ì ‘ ë°”ì´íŠ¸ ì¡°ì‘ ì‹œë„")
                            try:
                                with open(model_path, 'rb') as f:
                                    data = f.read()
                                
                                # pickle í—¤ë”ì—ì„œ keras.srcë¥¼ kerasë¡œ êµì²´
                                modified_data = data.replace(b'keras.src.', b'keras.')
                                
                                # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ë¡œë“œ
                                temp_path = model_path + '.temp'
                                with open(temp_path, 'wb') as f:
                                    f.write(modified_data)
                                
                                with open(temp_path, 'rb') as f:
                                    model_mal = pickle.load(f)
                                
                                os.remove(temp_path)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                                print("âœ… ë°”ì´íŠ¸ ì¡°ì‘ìœ¼ë¡œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                                
                            except Exception as e:
                                print(f"âŒ ëª¨ë“  ë°©ë²• ì‹¤íŒ¨: {str(e)}")
                                raise Exception("ê¸°ì¡´ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ì‹œë„ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
            # ëª¨ë“ˆ ì •ë¦¬
            for module_name in old_modules:
                if module_name in sys.modules:
                    del sys.modules[module_name]
            
            # ëª¨ë¸ ë¡œë“œ í™•ì¸
            if model_mal is None:
                raise Exception("ëª¨ë¸ ë¡œë“œ í›„ì—ë„ model_malì´ Noneì…ë‹ˆë‹¤.")
                
            print("âœ… ê¸°ì¡´ LSTM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            print(f"ëª¨ë¸ íƒ€ì…: {type(model_mal)}")
            
            # GPU ìµœì í™” ì„¤ì •
            try:
                if tf.config.list_physical_devices('GPU'):
                    gpus = tf.config.experimental.list_physical_devices('GPU')
                    if gpus:
                        for gpu in gpus:
                            tf.config.experimental.set_memory_growth(gpu, True)
            except Exception:
                pass
            
        except Exception as e:
            print(f"LSTM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
        
        # ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ
        try:
            encoder_path = os.path.join(self.model_save_dir, 'label_encoder_mal.pkl')
            with open(encoder_path, 'rb') as f:
                label_encoder_mal = pickle.load(f)
            print("ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ ì„±ê³µ")
            return True
        except Exception as e:
            print(f"ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def analyze_single_code(self, source_code, package_name):
        """ë‹¨ì¼ ì½”ë“œ LSTM ë¶„ì„"""
        try:
            tokenized_code = tokenize_python(source_code)
            
            if not tokenized_code:
                return {
                    'vulnerability_status': 'Error',
                    'cwe_label': 'parsing_error',
                    'confidence': 0.0
                }
            
            if w2v_model is None:
                return {
                    'vulnerability_status': 'Error',
                    'cwe_label': 'word2vec_error',
                    'confidence': 0.0
                }
            
            # ë²¡í„° ì„ë² ë”©
            padded_code = embed_sequences([tokenized_code], w2v_model, max_length=100)
            
            if padded_code is None or len(padded_code) == 0:
                return {
                    'vulnerability_status': 'Error',
                    'cwe_label': 'embedding_error',
                    'confidence': 0.0
                }
            
            # LSTM ì˜ˆì¸¡
            prediction = model_mal.predict(padded_code, verbose=0)
            
            if prediction is None or len(prediction) == 0:
                return {
                    'vulnerability_status': 'Error',
                    'cwe_label': 'prediction_error',
                    'confidence': 0.0
                }
            
            # ê²°ê³¼ í•´ì„
            predicted_class = np.argmax(prediction, axis=1)[0]
            confidence = float(np.max(prediction))
            
            # ë¼ë²¨ ì¸ì½”ë”ë¡œ í´ë˜ìŠ¤ ì´ë¦„ ë³€í™˜
            if label_encoder_mal is not None:
                try:
                    cwe_label = label_encoder_mal.inverse_transform([predicted_class])[0]
                except Exception as e:
                    print(f"ë¼ë²¨ ë””ì½”ë”© ì˜¤ë¥˜: {e}")
                    cwe_label = f'class_{predicted_class}'
            else:
                cwe_label = f'class_{predicted_class}'
            
            # ì·¨ì•½ì  ìƒíƒœ ê²°ì •
            vulnerability_status = 'Vulnerable' if confidence > 0.5 else 'Safe'
            
            return {
                'vulnerability_status': vulnerability_status,
                'cwe_label': cwe_label,
                'confidence': confidence
            }
            
        except Exception as e:
            print(f"ì½”ë“œ ë¶„ì„ ì˜¤ë¥˜ ({package_name}): {e}")
            return {
                'vulnerability_status': 'Error',
                'cwe_label': 'analysis_error',
                'confidence': 0.0
            }

    def analyze_lstm_codes(self, source_csv='merged_sourceCode.csv'):
        """LSTMì„ ì´ìš©í•œ ì†ŒìŠ¤ì½”ë“œ ë¶„ì„"""
        print("5ï¸âƒ£ LSTM ì½”ë“œ ë¶„ì„...")
        
        csv_path = os.path.join(self.result_dir, source_csv)
        
        if not os.path.exists(csv_path):
            print(f"âŒ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
            return None
        
        # CSV íŒŒì¼ ì½ê¸°
        df = self.read_csv_data(csv_path)
        if df is None:
            print("âŒ LSTM ë¶„ì„ ì‹¤íŒ¨")
            return None
        
        print(f"\n=== LSTM ë¶„ì„ ì‹œì‘: {len(df)}ê°œ íŒ¨í‚¤ì§€ ===")
        
        results = []
        start_time = time.time()
        
        for idx, row in df.iterrows():
            package_name = row['package']
            source_code = row['code']
            
            print(f"LSTM ë¶„ì„ ì¤‘ ({idx + 1}/{len(df)}): {package_name}")
            
            # ê°œë³„ ì½”ë“œ ë¶„ì„
            result = self.analyze_single_code(source_code, package_name)
            result['package_name'] = package_name
            results.append(result)
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        print(f"\n=== LSTM ë¶„ì„ ì™„ë£Œ ===")
        print(f"ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        print(f"íŒ¨í‚¤ì§€ë‹¹ í‰ê·  ì‹œê°„: {elapsed_time/len(df):.2f}ì´ˆ")
        
        # ê²°ê³¼ DataFrame ìƒì„±
        results_df = pd.DataFrame(results)
        
        # ê²°ê³¼ ì €ì¥
        output_path = os.path.join(self.result_dir, 'package_vulnerability_analysis.csv')
        results_df.to_csv(output_path, index=False)
        print(f"LSTM ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_path}")
        
        self.lstm_results = results_df
        return results_df

    def merge_lstm_results(self):
        """LSTM ê²°ê³¼ë¥¼ ë©”ì¸ ë°ì´í„°ì™€ í†µí•©"""
        print("6ï¸âƒ£ LSTM ê²°ê³¼ í†µí•©...")
        
        if self.lstm_results is None or self.df is None:
            print("âŒ í†µí•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # íŒ¨í‚¤ì§€ëª… ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
        self.df = self.df.merge(
            self.lstm_results, 
            left_on='name', 
            right_on='package_name', 
            how='left'
        )
        
        # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
        if 'package_name' in self.df.columns:
            self.df = self.df.drop('package_name', axis=1)
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        lstm_columns = ['vulnerability_status', 'cwe_label', 'confidence']
        for col in lstm_columns:
            if col in self.df.columns:
                self.df[col] = self.df[col].fillna('Unknown')
        
        # í†µí•© ë°ì´í„° ì €ì¥
        output_path = os.path.join(self.result_dir, 'pypi_typo_analysis5.csv')
        self.df.to_csv(output_path, index=False)
        print(f"í†µí•©ëœ ë¶„ì„ ë°ì´í„° ì €ì¥: {output_path}")

    def load_xgboost_model(self):
        """XGBoost ëª¨ë¸ ë¡œë“œ"""
        print("7ï¸âƒ£ XGBoost ëª¨ë¸ ë¡œë“œ...")
        
        global xgboost_model
        
        try:
            model_path = os.path.join(self.current_dir, 'xgboost_model.pkl')
            with open(model_path, 'rb') as f:
                xgboost_model = pickle.load(f)
            print("XGBoost ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            return True
        except Exception as e:
            print(f"XGBoost ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def predict_malicious_packages(self):
        """ìµœì¢… ì•…ì„± íŒ¨í‚¤ì§€ ì˜ˆì¸¡"""
        print("8ï¸âƒ£ ìµœì¢… ì•…ì„± íŒ¨í‚¤ì§€ ì˜ˆì¸¡...")
        
        if xgboost_model is None or self.df is None:
            print("âŒ ëª¨ë¸ ë˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # ì˜ˆì¸¡ì— í•„ìš”í•œ í”¼ì²˜ ì¤€ë¹„
            feature_columns = [
                'download', 'summary_length', 'version_parts', 
                'min_levenshtein_distance', 'confidence'
            ]
            
            # ê²°ì¸¡ê°’ ì²˜ë¦¬
            prediction_df = self.df.copy()
            for col in feature_columns:
                if col in prediction_df.columns:
                    prediction_df[col] = pd.to_numeric(prediction_df[col], errors='coerce').fillna(0)
                else:
                    prediction_df[col] = 0
            
            X = prediction_df[feature_columns]
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            predictions = xgboost_model.predict(X)
            
            # ê²°ê³¼ ì €ì¥
            self.df['xgboost_prediction'] = predictions
            
            # ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½
            result_summary = pd.DataFrame({
                'name': self.df['name'],
                'is_malicious': predictions
            })
            
            print("ì•…ì„± íŒ¨í‚¤ì§€ ì˜ˆì¸¡ ì™„ë£Œ")
            print(result_summary)
            
        except Exception as e:
            print(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
            self.df['xgboost_prediction'] = 0

    def generate_final_report(self):
        """ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("9ï¸âƒ£ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±...")
        
        if self.df is None:
            print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        report_path = os.path.join(self.result_dir, 'pypi_malicious_reason_report.txt')
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("=== Python íŒ¨í‚¤ì§€ ë³´ì•ˆ ë¶„ì„ ë¦¬í¬íŠ¸ ===\n\n")
                f.write(f"ë¶„ì„ ì¼ì‹œ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ì´ ë¶„ì„ íŒ¨í‚¤ì§€ ìˆ˜: {len(self.df)}\n\n")
                
                # ì•…ì„± íŒ¨í‚¤ì§€ ì˜ˆì¸¡ ê²°ê³¼
                if 'xgboost_prediction' in self.df.columns:
                    malicious_count = sum(self.df['xgboost_prediction'] == 1)
                    f.write(f"ì•…ì„± íŒ¨í‚¤ì§€ ì˜ˆì¸¡: {malicious_count}ê°œ\n")
                    f.write(f"ì •ìƒ íŒ¨í‚¤ì§€ ì˜ˆì¸¡: {len(self.df) - malicious_count}ê°œ\n\n")
                
                # ê°œë³„ íŒ¨í‚¤ì§€ ë¶„ì„ ê²°ê³¼
                f.write("=== ê°œë³„ íŒ¨í‚¤ì§€ ë¶„ì„ ê²°ê³¼ ===\n\n")
                
                for _, row in self.df.iterrows():
                    f.write(f"ğŸ“¦ íŒ¨í‚¤ì§€ëª…: {row['name']}\n")
                    f.write(f"   ë²„ì „: {row.get('version', 'Unknown')}\n")
                    f.write(f"   ì‘ì„±ì: {row.get('author', 'Unknown')}\n")
                    f.write(f"   ë‹¤ìš´ë¡œë“œ ìˆ˜: {row.get('download', 0)}\n")
                    
                    if 'xgboost_prediction' in row:
                        prediction = "ì•…ì„±" if row['xgboost_prediction'] == 1 else "ì •ìƒ"
                        f.write(f"   ìµœì¢… íŒë‹¨: {prediction}\n")
                    
                    if 'vulnerability_status' in row:
                        f.write(f"   LSTM ì·¨ì•½ì  ë¶„ì„: {row['vulnerability_status']}\n")
                        f.write(f"   CWE ë¶„ë¥˜: {row.get('cwe_label', 'Unknown')}\n")
                        f.write(f"   ì‹ ë¢°ë„: {row.get('confidence', 0):.3f}\n")
                    
                    f.write("-" * 50 + "\n")
            
            print(f"ğŸ“„ ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
            
        except Exception as e:
            print(f"ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")

    def save_comprehensive_results(self):
        """ì¢…í•© ë¶„ì„ ê²°ê³¼ CSV ì €ì¥"""
        print("ğŸ”Ÿ í†µí•© ë¶„ì„ ê²°ê³¼ CSV ìƒì„±...")
        
        if self.df is None:
            print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # ì»¬ëŸ¼ëª… ì •ë¦¬
            columns_mapping = {
                'vulnerability_status': 'lstm_vulnerability_status',
                'cwe_label': 'lstm_cwe_label',
                'confidence': 'lstm_confidence'
            }
            
            result_df = self.df.rename(columns=columns_mapping)
            
            # ìµœì¢… ê²°ê³¼ ì €ì¥
            output_path = os.path.join(self.result_dir, 'comprehensive_analysis_results.csv')
            result_df.to_csv(output_path, index=False)
            
            # í†µê³„ ì •ë³´ ì¶œë ¥
            print(f"ğŸ“Š í†µí•© ë¶„ì„ ê²°ê³¼ CSV ì €ì¥: {output_path}")
            print(f"   - ì´ íŒ¨í‚¤ì§€ ìˆ˜: {len(result_df)}")
            print(f"   - ì´ ì»¬ëŸ¼ ìˆ˜: {len(result_df.columns)}")
            
            if 'xgboost_prediction' in result_df.columns:
                malicious_count = sum(result_df['xgboost_prediction'] == 1)
                print(f"   - XGBoost ì˜ˆì¸¡ ì•…ì„±: {malicious_count}ê°œ")
                print(f"   - XGBoost ì˜ˆì¸¡ ì •ìƒ: {len(result_df) - malicious_count}ê°œ")
            
            if 'lstm_vulnerability_status' in result_df.columns:
                vulnerable_count = sum(result_df['lstm_vulnerability_status'] == 'Vulnerable')
                print(f"   - LSTM ì·¨ì•½ì  ë°œê²¬: {vulnerable_count}ê°œ")
            
            # ì£¼ìš” ì»¬ëŸ¼ ëª©ë¡ ì¶œë ¥
            important_cols = ['name', 'xgboost_prediction', 'lstm_vulnerability_status', 
                            'lstm_cwe_label', 'lstm_confidence', 'summary', 'author', 
                            'author-email', 'version', 'download']
            
            available_cols = [col for col in important_cols if col in result_df.columns]
            print("   - ì£¼ìš” ì»¬ëŸ¼ë“¤:")
            for i, col in enumerate(available_cols[:10], 1):
                print(f"     {i}. {col}")
            
            if len(available_cols) > 10:
                print(f"     ... ì™¸ {len(result_df.columns) - 10}ê°œ ì»¬ëŸ¼")
            
        except Exception as e:
            print(f"ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {e}")

    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        print("ğŸ”§ ë©”ëª¨ë¦¬ ì •ë¦¬...")
        
        global model_mal, label_encoder_mal, xgboost_model
        
        # TensorFlow ì„¸ì…˜ ì •ë¦¬
        try:
            if K is not None:
                K.clear_session()
        except Exception:
            pass
        
        # ëª¨ë¸ ê°ì²´ ì •ë¦¬
        for obj_name in ['model_mal', 'label_encoder_mal', 'xgboost_model']:
            try:
                globals()[obj_name] = None
            except Exception:
                pass
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        print("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

    def run_full_analysis(self):
        """ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("=== Python íŒ¨í‚¤ì§€ ë³´ì•ˆ ë¶„ì„ ë„êµ¬ (Final Unified) ===\n")
        
        try:
            # 1. ZIP íŒŒì¼ í•´ì œ ë° ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ
            self.extract_zip_and_process_source()
            
            # 2. ë©”íƒ€ë°ì´í„° íŒŒì‹±
            self.extract_and_parse_metadata()
            
            # 3. ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬
            self.preprocess_metadata()
            
            # 4. LSTM ëª¨ë¸ ë¡œë“œ
            print("4ï¸âƒ£ LSTM ëª¨ë¸ ë¡œë“œ...")
            if not self.load_lstm_models():
                print("âŒ LSTM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                return
            
            # 5. LSTM ì½”ë“œ ë¶„ì„
            self.analyze_lstm_codes()
            
            # 6. ê²°ê³¼ í†µí•©
            self.merge_lstm_results()
            
            # 7. XGBoost ëª¨ë¸ ë¡œë“œ
            if not self.load_xgboost_model():
                return
            
            # 8. ìµœì¢… ì˜ˆì¸¡
            self.predict_malicious_packages()
            
            # 9. ë¦¬í¬íŠ¸ ìƒì„±
            self.generate_final_report()
            
            # 10. ì¢…í•© ê²°ê³¼ ì €ì¥
            self.save_comprehensive_results()
            
            print("\nâœ… ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("\nìƒì„±ëœ íŒŒì¼ë“¤ (./result í´ë”):")
            print("- result/merged_sourceCode.csv: ë³‘í•©ëœ ì†ŒìŠ¤ì½”ë“œ")
            print("- result/pypi_typo_analysis5.csv: í†µí•© ë¶„ì„ ë°ì´í„°") 
            print("- result/package_vulnerability_analysis.csv: LSTM ë¶„ì„ ê²°ê³¼")
            print("- result/comprehensive_analysis_results.csv: ëª¨ë“  ê²°ê³¼ í†µí•© CSV")
            print("- result/pypi_malicious_reason_report.txt: ìµœì¢… íŒë‹¨ ë¦¬í¬íŠ¸")
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.cleanup_memory()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = FinalUnifiedAnalyzer()
    analyzer.run_full_analysis()

if __name__ == "__main__":
    main()