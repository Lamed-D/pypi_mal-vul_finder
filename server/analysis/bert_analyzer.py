"""
BERT ê¸°ë°˜ í†µí•© ë¶„ì„ê¸°
CodeBERT ëª¨ë¸ì„ ì‚¬ìš©í•œ ì·¨ì•½ì  ë° ì•…ì„±ì½”ë“œ ë¶„ì„
"""

import asyncio
import os
import sys
import time

try:  # pragma: no cover - optional dependency guard
    import torch  # type: ignore
except ImportError:  # pragma: no cover
    torch = None  # type: ignore

import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional

try:  # pragma: no cover - optional dependency guard
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
except ImportError:  # pragma: no cover
    AutoTokenizer = None  # type: ignore
    AutoModelForSequenceClassification = None  # type: ignore

# ì„œë²„ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
server_dir = Path(__file__).parents[1]
sys.path.insert(0, str(server_dir))

def load_labels(path: str) -> Optional[List[str]]:
    """í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ë¼ë²¨ëª…ì„ ì¤„ ë‹¨ìœ„ë¡œ ë¡œë“œ. íŒŒì¼ì´ ì—†ìœ¼ë©´ None.

    Args:
        path: ë¼ë²¨ íŒŒì¼ ê²½ë¡œ

    Returns:
        ë¼ë²¨ëª… ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” None
    """
    if not os.path.exists(path):
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            labels = [line.strip() for line in f if line.strip()]
        return labels or None
    except Exception:
        return None

class BERTAnalyzer:
    """BERT ê¸°ë°˜ í†µí•© ë¶„ì„ê¸°"""
    
    def __init__(self, models_dir: str):
        """
        BERT ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            models_dir: ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        if torch is None or AutoTokenizer is None or AutoModelForSequenceClassification is None:
            raise RuntimeError(
                "BERT analysis requires 'torch' and 'transformers' packages; install them to enable this feature."
            )

        self.models_dir = Path(models_dir)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
        # 1) ì„œë²„ ë‚´ë¶€ models ê²½ë¡œ ìš°ì„ 
        mal_server = self.models_dir / "bert_mal" / "codebert"
        vul_server = self.models_dir / "bert_vul" / "codebert"
        # 2) ì„œë²„ì— ì—†ìœ¼ë©´ ì›ë³¸ ë ˆí¬ ê²½ë¡œë¡œ í´ë°±
        mal_original = Path(__file__).parents[2] / "codebert_mal" / "model" / "codebert"
        vul_original = Path(__file__).parents[2] / "codebert_test2" / "model" / "codebert"
        self.mal_model_path = mal_server if mal_server.exists() else mal_original
        self.vul_model_path = vul_server if vul_server.exists() else vul_original
        
        # CWE ë¼ë²¨ ë¡œë”©
        cwe_labels_path = self.models_dir / "bert_vul" / "cwe_labels.txt"
        self.cwe_label_names = load_labels(str(cwe_labels_path))
        
        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self.mal_tokenizer = None
        self.mal_model = None
        self.vul_tokenizer = None
        self.vul_model = None
        
        # ì„¤ì •
        self.max_length = 512
        # ì›ë³¸ íŒŒì´í”„ë¼ì¸ê³¼ ì¼ì¹˜í•˜ë„ë¡ stride ì¡°ì •
        self.stride = 64
        self.batch_size = 8
        self.threshold = 0.5
        # ê¸ì • í´ë˜ìŠ¤ ì¸ë±ìŠ¤ (ì´ì§„: 1, ë‹¤ì¤‘: id2label ê¸°ë°˜ ì¶”ì •)
        self.mal_positive_index = 1
        self.vul_positive_index = 1
        
        print(f"ğŸ”§ BERT Analyzer initialized on {self.device}")
        print(f"ğŸ“ Malicious model path: {self.mal_model_path}")
        print(f"ğŸ“ Vulnerability model path: {self.vul_model_path}")
        print(f"ğŸ“ Models directory: {self.models_dir}")
        print(f"âš™ï¸  Params - max_length={self.max_length}, stride={self.stride}, batch_size={self.batch_size}, threshold={self.threshold}")
    
    def load_malicious_model(self):
        """ì•…ì„±ì½”ë“œ ë¶„ì„ ëª¨ë¸ ë¡œë“œ"""
        try:
            if self.mal_model_path.exists():
                self.mal_tokenizer = AutoTokenizer.from_pretrained(str(self.mal_model_path))
                self.mal_model = AutoModelForSequenceClassification.from_pretrained(
                    str(self.mal_model_path), ignore_mismatched_sizes=True
                )
                self.mal_model.to(self.device)
                self.mal_model.eval()
                self.mal_positive_index = self._resolve_positive_index(self.mal_model, ["mal", "malicious"])  
                id2label = getattr(self.mal_model.config, "id2label", None)
                print(f"âœ… Malicious BERT model loaded successfully (positive_index={self.mal_positive_index}, id2label={id2label})")
            else:
                print(f"âš ï¸ Malicious model not found at {self.mal_model_path}")
        except Exception as e:
            print(f"âŒ Error loading malicious model: {e}")
    
    def load_vulnerability_model(self):
        """ì·¨ì•½ì  ë¶„ì„ ëª¨ë¸ ë¡œë“œ"""
        try:
            if self.vul_model_path.exists():
                self.vul_tokenizer = AutoTokenizer.from_pretrained(str(self.vul_model_path))
                self.vul_model = AutoModelForSequenceClassification.from_pretrained(
                    str(self.vul_model_path), ignore_mismatched_sizes=True
                )
                self.vul_model.to(self.device)
                self.vul_model.eval()
                self.vul_positive_index = self._resolve_positive_index(self.vul_model, ["vul", "vulnerable", "vulnerability"])  
                id2label = getattr(self.vul_model.config, "id2label", None)
                print(f"âœ… Vulnerability BERT model loaded successfully (positive_index={self.vul_positive_index}, id2label={id2label})")
            else:
                print(f"âš ï¸ Vulnerability model not found at {self.vul_model_path}")
        except Exception as e:
            print(f"âŒ Error loading vulnerability model: {e}")
    
    def analyze_malicious(self, content: str, file_path: str) -> Dict[str, Any]:
        """ì•…ì„±ì½”ë“œ ë¶„ì„"""
        if not self.mal_model or not self.mal_tokenizer:
            return {"is_malicious": False, "error": "Malicious model not loaded"}
        
        try:
            start_time = time.time()
            
            # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì²­í¬ ìƒì„±
            chunks = self._create_chunks(content)
            print(f"ğŸ§© [MAL] Created {len(chunks)} chunks (max_length={self.max_length}, stride={self.stride}) for {file_path}")
            
            if not chunks:
                return {"is_malicious": False, "malicious_probability": 0.0, "malicious_status": "Safe", "malicious_label": "Safe"}
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì˜ˆì¸¡
            probabilities = []
            for i in range(0, len(chunks), self.batch_size):
                batch = chunks[i:i + self.batch_size]
                batch_probs = self._predict_batch(batch, self.mal_tokenizer, self.mal_model, self.mal_positive_index)
                probabilities.extend(batch_probs)
            
            # íŒŒì¼ ìˆ˜ì¤€ í™•ë¥  ê³„ì‚° (ì¤‘ì•™ ê°€ì¤‘ì¹˜ + ìµœëŒ“ê°’)
            file_probability = self._aggregate_probabilities(probabilities)
            print(f"ğŸ“ˆ [MAL] Aggregated probability={file_probability:.4f} (pos_idx={self.mal_positive_index})")
            
            # ì•…ì„± ì—¬ë¶€ íŒë‹¨
            is_malicious = file_probability > self.threshold
            malicious_status = "malicious" if is_malicious else "Safe"
            malicious_label = "malicious" if is_malicious else "Safe"
            
            analysis_time = time.time() - start_time
            
            return {
                "is_malicious": is_malicious,
                "malicious_probability": file_probability,
                "malicious_status": malicious_status,
                "malicious_label": malicious_label,
                "analysis_time": analysis_time
            }
            
        except Exception as e:
            print(f"âŒ Error in malicious analysis: {e}")
            return {"is_malicious": False, "error": str(e)}
    
    def analyze_vulnerability(self, content: str, file_path: str) -> Dict[str, Any]:
        """ì·¨ì•½ì  ë¶„ì„"""
        if not self.vul_model or not self.vul_tokenizer:
            return {"is_vulnerable": False, "error": "Vulnerability model not loaded"}
        
        try:
            start_time = time.time()
            
            # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì²­í¬ ìƒì„±
            chunks = self._create_chunks(content)
            print(f"ğŸ§© [VUL] Created {len(chunks)} chunks (max_length={self.max_length}, stride={self.stride}) for {file_path}")
            
            if not chunks:
                return {"is_vulnerable": False, "vulnerability_probability": 0.0, "vulnerability_status": "Safe", "vulnerability_label": "Safe", "cwe_label": "Safe"}
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì˜ˆì¸¡
            probabilities = []
            for i in range(0, len(chunks), self.batch_size):
                batch = chunks[i:i + self.batch_size]
                batch_probs = self._predict_batch(batch, self.vul_tokenizer, self.vul_model, self.vul_positive_index)
                probabilities.extend(batch_probs)
            
            # íŒŒì¼ ìˆ˜ì¤€ í™•ë¥  ê³„ì‚°
            file_probability = self._aggregate_probabilities(probabilities)
            print(f"ğŸ“ˆ [VUL] Aggregated probability={file_probability:.4f} (pos_idx={self.vul_positive_index})")
            
            # ì·¨ì•½ì  ì—¬ë¶€ íŒë‹¨
            is_vulnerable = file_probability > self.threshold
            vulnerability_status = "Vulnerable" if is_vulnerable else "Safe"
            vulnerability_label = "Vulnerable" if is_vulnerable else "Safe"
            
            # CWE ë¼ë²¨ ê²°ì • (ì›ë³¸ ì½”ë“œ ë°©ì‹ ì°¸ê³ )
            if is_vulnerable:
                # ëª¨ë¸ì˜ id2label í™•ì¸
                id2label = getattr(self.vul_model.config, "id2label", None)
                if isinstance(id2label, dict):
                    id2label = {int(k): v for k, v in id2label.items()}
                
                # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
                max_prob_idx = np.argmax(probabilities) if probabilities else 0
                
                # CWE ë¼ë²¨ ë§¤í•‘ (ì›ë³¸ ì½”ë“œì˜ idx_to_name í•¨ìˆ˜ ë¡œì§)
                if self.cwe_label_names and 0 <= max_prob_idx < len(self.cwe_label_names):
                    cwe_label = self.cwe_label_names[max_prob_idx]
                elif id2label and max_prob_idx in id2label:
                    cwe_label = id2label[max_prob_idx]
                else:
                    cwe_label = f"class_{max_prob_idx}"
            else:
                cwe_label = "Safe"
            
            analysis_time = time.time() - start_time
            
            return {
                "is_vulnerable": is_vulnerable,
                "vulnerability_probability": file_probability,
                "vulnerability_status": vulnerability_status,
                "vulnerability_label": vulnerability_label,
                "cwe_label": cwe_label,
                "analysis_time": analysis_time
            }
            
        except Exception as e:
            print(f"âŒ Error in vulnerability analysis: {e}")
            return {"is_vulnerable": False, "error": str(e)}
    
    def _create_chunks(self, content: str) -> List[str]:
        """ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì²­í¬ ìƒì„±"""
        if len(content) <= self.max_length:
            return [content]
        
        chunks = []
        start = 0
        while start < len(content):
            end = start + self.max_length
            chunk = content[start:end]
            chunks.append(chunk)
            start += self.max_length - self.stride
            
            if start >= len(content):
                break
        
        return chunks
    
    def _predict_batch(self, chunks: List[str], tokenizer, model, positive_index: int) -> List[float]:
        """ë°°ì¹˜ ì˜ˆì¸¡"""
        try:
            # í† í¬ë‚˜ì´ì§•
            inputs = tokenizer(
                chunks,
                padding=True,
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt"
            )
            
            # GPUë¡œ ì´ë™
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ì˜ˆì¸¡
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                if logits.shape[-1] == 1:
                    probs = torch.sigmoid(logits).squeeze(-1).cpu().numpy()
                else:
                    probabilities = torch.softmax(logits, dim=-1)
                    probs = probabilities[:, positive_index].cpu().numpy()
            
            return probs.tolist()
            
        except Exception as e:
            print(f"âŒ Error in batch prediction: {e}")
            return [0.0] * len(chunks)

    def _resolve_positive_index(self, model, keywords: List[str]) -> int:
        """ëª¨ë¸ configì—ì„œ ê¸ì • í´ë˜ìŠ¤ ì¸ë±ìŠ¤ë¥¼ ì¶”ë¡  (í‚¤ì›Œë“œ ìš°ì„ , ì´ì§„=1, ë‹¤ì¤‘=ë§ˆì§€ë§‰)."""
        try:
            config = getattr(model, "config", None)
            id2label = getattr(config, "id2label", None)
            if isinstance(id2label, dict) and len(id2label) > 0:
                for key, name in id2label.items():
                    try:
                        idx = int(key)
                    except Exception:
                        continue
                    if isinstance(name, str) and any(kw in name.lower() for kw in keywords):
                        return idx
            num_labels = getattr(config, "num_labels", None)
            if num_labels == 1:
                return 0
            if num_labels == 2:
                return 1
            return int(num_labels - 1) if num_labels else 1
        except Exception:
            return 1
    
    def _aggregate_probabilities(self, probabilities: List[float]) -> float:
        """í™•ë¥  ì§‘ê³„ (ì¤‘ì•™ ê°€ì¤‘ì¹˜ + ìµœëŒ“ê°’)"""
        if not probabilities:
            return 0.0
        
        # ì¤‘ì•™ ê°€ì¤‘ì¹˜ ê³„ì‚° (n=1 ì¼€ì´ìŠ¤ ë³´í˜¸)
        n = len(probabilities)
        if n == 1:
            weighted_avg = probabilities[0]
        else:
            weights = []
            for i in range(n):
                # ì¤‘ì•™ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜
                distance_from_center = abs(i - (n-1)/2)
                denom = ((n-1)/2) if (n-1) != 0 else 1.0
                weight = 1.0 - (distance_from_center / denom)
                weights.append(max(0.1, weight))  # ìµœì†Œ 0.1
            # ê°€ì¤‘ í‰ê· 
            weighted_avg = sum(p * w for p, w in zip(probabilities, weights)) / max(1e-8, sum(weights))
        
        # ìµœëŒ“ê°’
        max_prob = max(probabilities)
        
        # ê²°í•© (ê°€ì¤‘ í‰ê·  70% + ìµœëŒ“ê°’ 30%)
        final_prob = 0.7 * weighted_avg + 0.3 * max_prob
        
        return float(final_prob)
    
    def analyze_single_file(self, content: str, file_path: str, mode: str = "both") -> Dict[str, Any]:
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„"""
        start_time = time.time()
        
        # ëª¨ë“œì— ë”°ë¼ ë¶„ì„ ìˆ˜í–‰
        vul_result = self.analyze_vulnerability(content, file_path) if mode in ("both", "vul") else {"is_vulnerable": False}
        mal_result = self.analyze_malicious(content, file_path) if mode in ("both", "mal") else {"is_malicious": False}
        
        analysis_time = time.time() - start_time
        
        return {
            "file_path": file_path,
            "vulnerability_analysis": vul_result,
            "malicious_analysis": mal_result,
            "analysis_time": analysis_time,
            "is_safe": not (vul_result.get("is_vulnerable", False) or mal_result.get("is_malicious", False))
        }
    
    async def analyze_files_multiprocess(self, session_id: str, files: List[Dict[str, Any]], mode: str = "both") -> Dict[str, Any]:
        """ë‹¤ì¤‘ í”„ë¡œì„¸ìŠ¤ë¡œ íŒŒì¼ë“¤ ë¶„ì„"""
        return await asyncio.to_thread(self._analyze_files_sync, session_id, files, mode)

    def _analyze_files_sync(self, session_id: str, files: List[Dict[str, Any]], mode: str = "both") -> Dict[str, Any]:
        try:
            print(f"ğŸ” Starting BERT multiprocess analysis for {len(files)} files (mode: {mode})")

            # ëª¨ë¸ ë¡œë“œ
            if mode in ("both", "mal"):
                self.load_malicious_model()
            if mode in ("both", "vul"):
                self.load_vulnerability_model()

            results: List[Dict[str, Any]] = []
            total_files = len(files)

            for i, file_info in enumerate(files):
                try:
                    print(f"ğŸ“„ Analyzing file {i+1}/{total_files}: {file_info['name']}")

                    if not file_info.get("content"):
                        print(f"âš ï¸ Empty content for file {file_info['path']}")
                        continue

                    result = self.analyze_single_file(file_info["content"], file_info["path"], mode=mode)
                    result["session_id"] = session_id
                    result["file_name"] = file_info["name"]
                    result["file_size"] = file_info["size"]

                    results.append(result)

                except Exception as e:
                    print(f"âŒ Error analyzing file {file_info.get('name', 'unknown')}: {e}")
                    continue

            print(f"âœ… BERT analysis completed: {len(results)} files processed")

            return {
                "status": "completed",
                "results": results,
                "total_files": len(results),
                "analysis_type": "BERT"
            }

        except Exception as e:
            print(f"âŒ BERT multiprocess analysis failed: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "results": [],
                "total_files": 0,
                "analysis_type": "BERT"
            }
    
    def get_active_tasks_count(self) -> int:
        """í™œì„± ì‘ì—… ìˆ˜ ë°˜í™˜ (BERTëŠ” ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤)"""
        return 0
    
    def shutdown_executor(self):
        """ì‹¤í–‰ì ì¢…ë£Œ (BERTëŠ” ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ì´ë¯€ë¡œ ë¹ˆ êµ¬í˜„)"""
        pass
