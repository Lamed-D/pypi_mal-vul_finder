"""
ML íŒ¨í‚¤ì§€ ë¶„ì„ê¸° - LSTM + XGBoost í†µí•© ë¶„ì„
============================================

ì´ ëª¨ë“ˆì€ safepy_3_malicious_MLì˜ ê¸°ëŠ¥ì„ serverì— í†µí•©í•˜ì—¬
LSTM ê¸°ë°˜ ì·¨ì•½ì  ë¶„ì„ê³¼ XGBoost ê¸°ë°˜ ì•…ì„± íŒ¨í‚¤ì§€ íŒë³„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- LSTM ëª¨ë¸ì„ ì‚¬ìš©í•œ ì·¨ì•½ì  ë¶„ì„
- XGBoost ëª¨ë¸ì„ ì‚¬ìš©í•œ ì•…ì„± íŒ¨í‚¤ì§€ íŒë³„
- ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
- í†µí•© ë¶„ì„ ê²°ê³¼ ìƒì„±

ë¶„ì„ í”„ë¡œì„¸ìŠ¤:
1. ZIP íŒŒì¼ì—ì„œ íŒ¨í‚¤ì§€ ë°ì´í„° ì¶”ì¶œ
2. ë©”íƒ€ë°ì´í„° íŒŒì‹± ë° ì „ì²˜ë¦¬
3. LSTM ëª¨ë¸ë¡œ ì·¨ì•½ì  ë¶„ì„
4. XGBoost ëª¨ë¸ë¡œ ì•…ì„± íŒ¨í‚¤ì§€ íŒë³„
5. í†µí•© ê²°ê³¼ ìƒì„± ë° DB ì €ì¥
"""

import os
import csv
import re
import zipfile
import pickle
try:  # pragma: no cover - optional dependency guard
    import numpy as np
except ImportError:  # pragma: no cover
    np = None  # type: ignore

try:  # pragma: no cover
    import pandas as pd
except ImportError:  # pragma: no cover
    pd = None  # type: ignore
import time
import math
try:  # pragma: no cover
    import requests
except ImportError:  # pragma: no cover
    requests = None  # type: ignore
from typing import Optional, Dict, List, Tuple, Any
from collections import Counter
try:  # pragma: no cover
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
except ImportError:  # pragma: no cover
    StandardScaler = None  # type: ignore
    MinMaxScaler = None  # type: ignore
from pathlib import Path
import shutil

# TensorFlow ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
import warnings
warnings.filterwarnings('ignore')

# LSTM ê´€ë ¨ import
try:
    import chardet
    HAS_CHARDET = True
except ImportError:
    HAS_CHARDET = False

try:  # pragma: no cover
    from tensorflow.keras import backend as K
except ImportError:  # pragma: no cover
    K = None  # type: ignore

# preprocess import ì‹œ ì¶œë ¥ ë©”ì‹œì§€ ì„ì‹œ ìˆ¨ê¸°ê¸°
import sys
from io import StringIO

# stdoutì„ ì„ì‹œë¡œ ë¦¬ë””ë ‰ì…˜í•˜ì—¬ Word2Vec ë¡œë“œ ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
old_stdout = sys.stdout
sys.stdout = StringIO()
try:
    # safepy_3_malicious_MLì˜ preprocess ëª¨ë“ˆ import
    sys.path.append(str(Path(__file__).parents[2] / "safepy_3_malicious_ML"))
    from preprocess import tokenize_python, embed_sequences, w2v_model
except ImportError:  # pragma: no cover
    tokenize_python = None  # type: ignore
    embed_sequences = None  # type: ignore
    w2v_model = None  # type: ignore
finally:
    sys.stdout = old_stdout

# Levenshtein distance import
try:
    from Levenshtein import distance as levenshtein_distance
except ImportError:
    def levenshtein_distance(a, b):
        return abs(len(a) - len(b))

class MLPackageAnalyzer:
    """ML íŒ¨í‚¤ì§€ ë¶„ì„ê¸° - LSTM + XGBoost í†µí•©"""
    
    def __init__(self, models_dir: str = None):
        """ML íŒ¨í‚¤ì§€ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        missing_dependencies = []
        if np is None:
            missing_dependencies.append("numpy")
        if pd is None:
            missing_dependencies.append("pandas")
        if StandardScaler is None or MinMaxScaler is None:
            missing_dependencies.append("scikit-learn")
        if K is None:
            missing_dependencies.append("tensorflow")
        if requests is None:
            missing_dependencies.append("requests")
        if tokenize_python is None or embed_sequences is None or w2v_model is None:
            missing_dependencies.append("safepy_3_malicious_ML preprocess")

        if missing_dependencies:
            raise RuntimeError(
                "MLPackageAnalyzer requires additional dependencies: "
                + ", ".join(missing_dependencies)
                + ". Install optional ML components to enable this feature."
            )

        # ì„œë²„ ë‚´ë¶€ ëª¨ë¸ ë””ë ‰í† ë¦¬ë§Œ ì‚¬ìš© (ì™„ì „ ë…ë¦½ì )
        self.server_models_root = Path(__file__).parents[1] / "models" / "ml_package"
        self.model_save_dir = self.server_models_root / "model"
        self.w2v_dir = self.server_models_root / "w2v"
        
        # ì„œë²„ ë‚´ë¶€ ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
        self._verify_server_models()
        
        # ëª¨ë¸ë“¤
        self.lstm_model = None
        self.label_encoder = None
        self.xgboost_model = None
        self.w2v_model = None
        
        # ë¶„ì„ ê²°ê³¼
        self.meta_datas = []
        self.df = None
        self.lstm_results = None
        
        # ëª¨ë¸ ë¡œë“œ
        self._load_models()


    def _verify_server_models(self) -> None:
        """ì„œë²„ ë‚´ë¶€ ëª¨ë¸ íŒŒì¼ë“¤ì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        required_files = [
            self.model_save_dir / 'model_mal.pkl',
            self.model_save_dir / 'label_encoder_mal.pkl',
            self.server_models_root / 'xgboost_model.pkl',
            self.w2v_dir / 'word2vec_withString10-6-100.model'
        ]
        
        missing_files = []
        for file_path in required_files:
            if not file_path.exists():
                missing_files.append(str(file_path))
        
        if missing_files:
            raise RuntimeError(
                f"í•„ìˆ˜ ëª¨ë¸ íŒŒì¼ë“¤ì´ ì„œë²„ì— ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_files)}\n"
                f"ì„œë²„ë¥¼ ì™„ì „íˆ ë…ë¦½ì ìœ¼ë¡œ ë§Œë“¤ë ¤ë©´ ì´ íŒŒì¼ë“¤ì„ server/models/ml_package/ ë””ë ‰í† ë¦¬ì— ë³µì‚¬í•´ì•¼ í•©ë‹ˆë‹¤."
            )
        
        print("âœ… ì„œë²„ ë‚´ë¶€ ëª¨ë¸ íŒŒì¼ë“¤ì´ ëª¨ë‘ ì¡´ì¬í•©ë‹ˆë‹¤.")
    
    def _load_models(self):
        """ëª¨ë“  ML ëª¨ë¸ ë¡œë“œ"""
        try:
            # LSTM ëª¨ë¸ ë¡œë“œ
            lstm_model_path = self.model_save_dir / 'model_mal.pkl'
            if lstm_model_path.exists():
                with open(lstm_model_path, 'rb') as f:
                    self.lstm_model = pickle.load(f)
                print("âœ… LSTM ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            else:
                print(f"âŒ LSTM ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {lstm_model_path}")
            
            # ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ
            label_encoder_path = self.model_save_dir / 'label_encoder_mal.pkl'
            if label_encoder_path.exists():
                with open(label_encoder_path, 'rb') as f:
                    self.label_encoder = pickle.load(f)
                print("âœ… ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ ì„±ê³µ")
            else:
                print(f"âŒ ë¼ë²¨ ì¸ì½”ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {label_encoder_path}")
            
            # XGBoost ëª¨ë¸ ë¡œë“œ (ì„œë²„ ë‚´ë¶€ë§Œ ì‚¬ìš©)
            xgboost_model_path = self.server_models_root / 'xgboost_model.pkl'
            if xgboost_model_path.exists():
                with open(xgboost_model_path, 'rb') as f:
                    self.xgboost_model = pickle.load(f)
                print("âœ… XGBoost ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            else:
                print(f"âŒ XGBoost ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {xgboost_model_path}")
            
            # Word2Vec ëª¨ë¸ì€ preprocessì—ì„œ ì´ë¯¸ ë¡œë“œë¨
            if w2v_model is not None:
                self.w2v_model = w2v_model
                print("âœ… Word2Vec ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            else:
                print("âŒ Word2Vec ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def remove_comments(self, code):
        """ì†ŒìŠ¤ ì½”ë“œì—ì„œ ì£¼ì„ ì œê±°"""
        # ì—¬ëŸ¬ ì¤„ ì£¼ì„ ì œê±°
        code = re.sub(r"'''(.*?)'''", '', code, flags=re.DOTALL)
        code = re.sub(r'"""(.*?)"""', '', code, flags=re.DOTALL)
        # í•œ ì¤„ ì£¼ì„ ì œê±°
        code = re.sub(r'#.*', '', code)
        return code
    
    def process_directory(self, root_path):
        """ë””ë ‰í† ë¦¬ë¥¼ ì²˜ë¦¬í•˜ì—¬ ì†ŒìŠ¤ì½”ë“œë¥¼ ì¶”ì¶œí•˜ê³  ë³‘í•©"""
        rows = []
        
        # root_path ë‚´ë¶€ì˜ ëª¨ë“  í•˜ìœ„ ë””ë ‰í„°ë¦¬ íƒìƒ‰
        for dir_name in os.listdir(root_path):
            dir_path = os.path.join(root_path, dir_name)
            if os.path.isdir(dir_path):
                merged_code = ''
                for root, _, files in os.walk(dir_path):
                    for file in files:
                        if file.endswith('.py'):
                            file_path = os.path.join(root, file)
                            try:
                                with open(file_path, 'r', encoding='utf-8') as f:
                                    raw_code = f.read()
                                    cleaned_code = self.remove_comments(raw_code)
                                    merged_code += cleaned_code + '\n'
                            except Exception as e:
                                print(f"âš ï¸ {file_path} ì½ê¸° ì‹¤íŒ¨: {e}")
                if merged_code.strip():
                    rows.append([dir_name, merged_code.strip()])
        return rows
    
    def extract_zip_and_process_source(self, zip_file_path: str, extract_dir: str):
        """ZIP íŒŒì¼ ì••ì¶• í•´ì œ ë° ì†ŒìŠ¤ì½”ë“œ ì²˜ë¦¬"""
        if not os.path.exists(zip_file_path):
            print(f"Warning: ZIP íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {zip_file_path}")
            return None
        
        # ì••ì¶• í•´ì œ
        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
            zip_ref.extractall(extract_dir)
        
        # ì†ŒìŠ¤ì½”ë“œ ì²˜ë¦¬
        root_path = os.path.join(extract_dir, 'source')
        if os.path.exists(root_path):
            data = self.process_directory(root_path)
            print(f"âœ… ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ ì™„ë£Œ: {len(data)}ê°œ ë””ë ‰í„°ë¦¬ ì²˜ë¦¬ë¨")
            return data
        else:
            print(f"Warning: ì†ŒìŠ¤ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {root_path}")
            return None
    
    def parse_name_email(self, text):
        """ì´ë¦„ê³¼ ì´ë©”ì¼ íŒŒì‹±"""
        match = re.match(r"(.*)<(.*@.*)>", text)
        if match:
            name = match.group(1).strip()
            email = match.group(2).strip()
            return name, email
        return None, None
    
    def parse_metadata(self, file_path):
        """ë©”íƒ€ë°ì´í„° íŒŒì‹±"""
        target_keys = {
            "name", "summary", "author", "author-email", "version",
            "maintainer", "maintainer-email"
        }
        metadata = {}
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    line = line.strip()
                    if ':' in line:
                        key, value = map(str.strip, line.split(':', 1))
                        key_lower = key.lower()
                        
                        if key_lower in target_keys:
                            metadata[key_lower] = value
            
            # authorê°€ ì—†ê±°ë‚˜ ê°’ì´ ë¹„ì–´ ìˆì„ ê²½ìš°
            if not metadata.get("author"):
                if metadata.get("author-email"):
                    name, email = self.parse_name_email(metadata["author-email"])
                    if name and email:
                        metadata["author"] = name
                        metadata["author-email"] = email
            
            # author_emailì´ ì—†ê±°ë‚˜ ê°’ì´ ë¹„ì–´ ìˆì„ ê²½ìš°
            if not metadata.get("author-email"):
                if metadata.get("maintainer-email") and metadata["maintainer-email"].strip():
                    metadata["author-email"] = metadata["maintainer-email"]
            
            # authorê°€ ì—†ê±°ë‚˜ ê°’ì´ ë¹„ì–´ ìˆì„ ê²½ìš° â†’ maintainerë¡œ ëŒ€ì²´
            if not metadata.get("author"):
                if metadata.get("maintainer") and metadata["maintainer"].strip():
                    metadata["author"] = metadata["maintainer"]
                    
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜ {file_path}: {e}")
            
        return metadata
    
    def extract_and_parse_metadata(self, extract_dir: str):
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° íŒŒì‹±"""
        metadata_dir = os.path.join(extract_dir, "metadata")
        
        if not os.path.exists(metadata_dir):
            print(f"Warning: ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {metadata_dir}")
            return []
            
        meta_datas = []
        for file in os.listdir(metadata_dir):
            if file.endswith(".txt"):
                metadata_path = os.path.join(metadata_dir, file)
                metadata = self.parse_metadata(metadata_path)
                if metadata:
                    meta_datas.append(metadata)
        
        self.meta_datas = meta_datas
        print(f"âœ… ë©”íƒ€ë°ì´í„° íŒŒì‹± ì™„ë£Œ: {len(meta_datas)}ê°œ")
        return meta_datas
    
    def get_pepy_downloads(self, package_name, api_key):
        """PePy.tech APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìš´ë¡œë“œ ìˆ˜ ì¡°íšŒ"""
        url = f"https://api.pepy.tech/api/v2/projects/{package_name}"
        headers = {"X-API-Key": api_key}
        
        try:
            response = requests.get(url, headers=headers)
            if response.status_code == 200:
                data = response.json()
                return data.get("total_downloads", -1)
            else:
                return -1
        except Exception as e:
            return -1
    
    def download_unified(self, package_name):
        """í†µí•©ëœ ë‹¤ìš´ë¡œë“œ ìˆ˜ ì¡°íšŒ"""
        download_count = self.get_pepy_downloads(package_name, "0SRbc/jRFsHYxOShwIQ/N0jtrKf1syMW")
        if download_count == -1:
            download_count = 0  # API ì ‘ê·¼ì´ ë¶ˆê°€ëŠ¥í•  ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •
        return download_count
    
    def shannon_entropy(self, s):
        """ë¬¸ìì—´ì˜ Shannon ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        if not s:
            return 0
        prob = [v / len(s) for v in Counter(s).values()]
        return -sum(p * math.log2(p) for p in prob)
    
    def is_valid_version(self, v):
        """ë²„ì „ í˜•ì‹ ê²€ì¦"""
        return bool(re.match(r"^\d+\.\d+\.\d+$", str(v).strip()))
    
    def get_pypi_top_packages(self):
        """PyPI ìƒìœ„ íŒ¨í‚¤ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            url = "https://hugovk.github.io/top-pypi-packages/top-pypi-packages-30-days.json"
            response = requests.get(url)
            data = response.json()
            return [pkg['project'] for pkg in data['rows']]
        except Exception as e:
            print(f"ì¸ê¸° íŒ¨í‚¤ì§€ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def extract_core_name(self, name):
        """í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ (ì ‘ë‘/ì ‘ë¯¸ì–´ ì œê±°)"""
        return re.split(r"[-_.]", name.lower())[0]
    
    def is_typo_like(self, pkg_name, legit_list):
        """ì˜¤íƒ€ ê¸°ë°˜ ìœ ì‚¬ì„± íŒë³„"""
        name = self.extract_core_name(pkg_name)
        for legit in legit_list:
            legit_core = self.extract_core_name(legit)
            if levenshtein_distance(name, legit_core) == 1 and abs(len(name) - len(legit_core)) <= 1:
                return True
        return False
    
    def preprocess_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬"""
        # ë‹¤ìš´ë¡œë“œ ìˆ˜ ìˆ˜ì§‘
        for meta_data in self.meta_datas:
            package_name = meta_data.get("name")
            if package_name:
                download_count = self.download_unified(package_name)
                meta_data["download"] = download_count
        
        # DataFrame ë³€í™˜
        df = pd.DataFrame(self.meta_datas)
        
        # ê¸°ë³¸ ì „ì²˜ë¦¬
        df["download"] = df["download"].fillna(0).astype(int)
        df["download_log"] = df["download"].apply(lambda x: np.log1p(x))
        
        scaler = StandardScaler()
        df["download_scaled"] = scaler.fit_transform(df[["download_log"]])
        
        # ì„¤ëª… ë¶„ì„
        df["summary"] = df["summary"].fillna("")
        df["summary_length"] = df["summary"].apply(len)
        df["summary_too_short"] = df["summary_length"] < 10
        df["summary_too_long"] = df["summary_length"] > 300
        df["summary_entropy"] = df["summary"].apply(self.shannon_entropy)
        df["summary_low_entropy"] = df["summary_entropy"] < 3.5
        
        # ë²„ì „ ê²€ì¦
        df["version_valid"] = df["version"].apply(self.is_valid_version)
        
        # ì˜¤íƒ€ ê¸°ë°˜ íƒì§€
        pypi_packages = self.get_pypi_top_packages()
        df["is_typo_like"] = df["name"].apply(lambda x: self.is_typo_like(x, pypi_packages))
        
        # ì¶”ê°€ í”¼ì²˜
        df["download_too_low"] = df["download_log"] < df["download_log"].quantile(0.05)
        df["download_too_high"] = df["download_log"] > df["download_log"].quantile(0.95)
        df["is_disposable"] = False
        
        # MinMaxScalerë¡œ download_log ì •ê·œí™”
        scaler2 = MinMaxScaler()
        df["download_log_scaled"] = 1 - scaler2.fit_transform(df[["download_log"]])
        
        self.df = df
        return df
    
    def analyze_single_code(self, source_code, package_name):
        """ë‹¨ì¼ ì½”ë“œ LSTM ë¶„ì„"""
        try:
            tokenized_code = tokenize_python(source_code)
            
            if not tokenized_code:
                return {
                    'vulnerability_status': 'Error',
                    'cwe_label': 'parsing_error',
                    'confidence': 0.0
                }
            
            if self.w2v_model is None:
                return {
                    'vulnerability_status': 'Error',
                    'cwe_label': 'model_error',
                    'confidence': 0.0
                }
            
            embedded_code = embed_sequences([tokenized_code], self.w2v_model)
            
            if not embedded_code or len(embedded_code) == 0 or embedded_code[0].size == 0:
                return {
                    'vulnerability_status': 'Error',
                    'cwe_label': 'embedding_error',
                    'confidence': 0.0
                }
            
            # ì‹œí€€ìŠ¤ íŒ¨ë”©
            max_sequence_length = 100
            embedding_dim = self.w2v_model.vector_size
            padded_code = np.zeros((max_sequence_length, embedding_dim))
            
            embedded_sequence = embedded_code[0]
            if embedded_sequence.shape[0] > 0:
                if embedded_sequence.shape[0] < max_sequence_length:
                    padded_code[:embedded_sequence.shape[0], :] = embedded_sequence
                else:
                    padded_code = embedded_sequence[:max_sequence_length, :]
            
            padded_code = np.expand_dims(padded_code, axis=0)
            
            # ëª¨ë¸ ì˜ˆì¸¡
            prediction = self.lstm_model.predict(padded_code, verbose=0)
            
            if prediction.ndim == 2 and prediction.shape[1] == 1:
                confidence = float(prediction[0][0])
                predicted_index = int((prediction > 0.5).astype(int)[0][0])
            else:
                predicted_index = int(np.argmax(prediction, axis=1)[0])
                confidence = float(prediction[0][predicted_index])
            
            try:
                decoded_label = self.label_encoder.inverse_transform([predicted_index])[0]
            except Exception as e:
                return {
                    'vulnerability_status': 'Error',
                    'cwe_label': 'label_decode_error',
                    'confidence': confidence
                }
            
            benign_aliases = {"Benign", "benign", "Not Vulnerable", "Normal", "Safe", "0", 0}
            is_vulnerable = decoded_label not in benign_aliases
            
            vulnerability_status = 'Vulnerable' if is_vulnerable else 'Not Vulnerable'
            cwe_label = str(decoded_label) if is_vulnerable else 'Benign'
            
            return {
                'vulnerability_status': vulnerability_status,
                'cwe_label': cwe_label,
                'confidence': confidence
            }
            
        except Exception as e:
            print(f"ì½”ë“œ ë¶„ì„ ì˜¤ë¥˜ ({package_name}): {e}")
            return {
                'vulnerability_status': 'Error',
                'cwe_label': 'analysis_error',
                'confidence': 0.0
            }
    
    def analyze_lstm_codes(self, source_data):
        """ì†ŒìŠ¤ ë°ì´í„°ì˜ ëª¨ë“  ì½”ë“œë¥¼ LSTMìœ¼ë¡œ ë¶„ì„"""
        if not source_data:
            return None
            
        print(f"\n=== LSTM ë¶„ì„ ì‹œì‘: {len(source_data)}ê°œ íŒ¨í‚¤ì§€ ===")
        start_time = time.time()
        
        results = []
        
        for idx, (package_name, source_code) in enumerate(source_data):
            print(f"LSTM ë¶„ì„ ì¤‘ ({idx+1}/{len(source_data)}): {package_name}")
            
            if not source_code or str(source_code).strip() == '':
                result_row = {
                    'package': package_name,
                    'vulnerability_status': 'Error',
                    'cwe_label': 'Empty Code',
                    'confidence': 0.0
                }
            else:
                analysis_result = self.analyze_single_code(str(source_code), package_name)
                result_row = {
                    'package': package_name,
                    'vulnerability_status': analysis_result['vulnerability_status'],
                    'cwe_label': analysis_result['cwe_label'],
                    'confidence': analysis_result['confidence']
                }
            
            results.append(result_row)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        result_df = pd.DataFrame(results)
        
        print(f"\n=== LSTM ë¶„ì„ ì™„ë£Œ ===")
        print(f"ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"íŒ¨í‚¤ì§€ë‹¹ í‰ê·  ì‹œê°„: {total_time/len(source_data):.2f}ì´ˆ")
        
        self.lstm_results = result_df
        return result_df
    
    def integrate_lstm_results(self):
        """LSTM ê²°ê³¼ë¥¼ ë©”ì¸ DataFrameì— í†µí•©"""
        if self.lstm_results is None or self.df is None:
            print("LSTM ê²°ê³¼ ë˜ëŠ” ë©”ì¸ DataFrameì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        # LSTM ê²°ê³¼ë¥¼ ë©”ì¸ DataFrameê³¼ ë³‘í•©
        # vulnerability_statusë¥¼ ìˆ«ìë¡œ ë³€í™˜
        vulnerability_map = {'Vulnerable': 1, 'Not Vulnerable': 0, 'Error': -1}
        self.lstm_results['vulnerability_status_numeric'] = self.lstm_results['vulnerability_status'].map(vulnerability_map)
        
        # CWE ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜ (ê°„ë‹¨í•œ ë°©ì‹)
        cwe_map = {'Benign': 0, 'Empty Code': -1, 'parsing_error': -1, 'model_error': -1, 
                   'embedding_error': -1, 'label_decode_error': -1, 'analysis_error': -1}
        
        # CWE ê°’ë“¤ì„ ìˆ«ìë¡œ ë§¤í•‘ (ê¸°íƒ€ëŠ” 1ë¡œ ì„¤ì •)
        self.lstm_results['cwe_label_numeric'] = self.lstm_results['cwe_label'].apply(
            lambda x: cwe_map.get(x, 1) if x in cwe_map else 1
        )
        
        # íŒ¨í‚¤ì§€ ì´ë¦„ì„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
        merged_df = pd.merge(self.df, 
                            self.lstm_results[['package', 'vulnerability_status_numeric', 'cwe_label_numeric', 'confidence']], 
                            left_on='name', 
                            right_on='package', 
                            how='left')
        
        # ë³‘í•©ë˜ì§€ ì•Šì€ í•­ëª©ì€ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
        merged_df['vulnerability_status_numeric'] = merged_df['vulnerability_status_numeric'].fillna(0)
        merged_df['cwe_label_numeric'] = merged_df['cwe_label_numeric'].fillna(0)
        merged_df['confidence'] = merged_df['confidence'].fillna(0.0)
        
        # ìƒˆë¡œìš´ í”¼ì²˜ ìƒì„± (ë…¸ì´ì¦ˆ ì¶”ê°€ëœ ë²„ì „)
        merged_df['vulnerability_status_noisy'] = merged_df['vulnerability_status_numeric']
        merged_df['cwe_label_noisy'] = merged_df['cwe_label_numeric'] 
        merged_df['threat_level_noisy'] = merged_df.apply(self.combined_threat, axis=1)
        merged_df['download_log_noisy'] = merged_df['download_log']  # XGBoost ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” í”¼ì²˜ëª…
        
        self.df = merged_df
        return True
    
    def combined_threat(self, row):
        """ìœ„í˜‘ ìˆ˜ì¤€ ê³„ì‚°"""
        vuln_status = row.get('vulnerability_status_numeric', 0)
        cwe_label = row.get('cwe_label_numeric', 0)
        
        if vuln_status == 1 and cwe_label == 1:
            return 2
        elif vuln_status == 1 or cwe_label == 1:
            return 1
        else:
            return 0
    
    def predict_malicious(self):
        """XGBoost ëª¨ë¸ë¡œ ì•…ì„± íŒ¨í‚¤ì§€ ì˜ˆì¸¡"""
        if self.df is None:
            print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        if self.xgboost_model is None:
            print("XGBoost ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        # í”¼ì²˜ ì„ íƒ (XGBoost ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” í”¼ì²˜ëª…ê³¼ ì¼ì¹˜)
        features = [
            "is_disposable", 
            "summary_length", "summary_too_short", "summary_too_long",
            "summary_entropy", "summary_low_entropy", "version_valid",
            "is_typo_like",
            "download_log_noisy",  # XGBoost ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” í”¼ì²˜ëª…
            "vulnerability_status_noisy", "threat_level_noisy", "cwe_label_noisy"
        ]
        
        # ëˆ„ë½ëœ í”¼ì²˜ê°€ ìˆëŠ”ì§€ í™•ì¸
        available_features = [f for f in features if f in self.df.columns]
        missing_features = [f for f in features if f not in self.df.columns]
        
        if missing_features:
            print(f"ëˆ„ë½ëœ í”¼ì²˜: {missing_features}")
            # ëˆ„ë½ëœ í”¼ì²˜ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›€
            for feature in missing_features:
                self.df[feature] = 0
        
        X = self.df[features]
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        try:
            self.df["is_malicious"] = self.xgboost_model.predict(X)
            print("ì•…ì„± íŒ¨í‚¤ì§€ ì˜ˆì¸¡ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def generate_comprehensive_results(self):
        """í†µí•© ë¶„ì„ ê²°ê³¼ ìƒì„±"""
        if self.df is None or 'is_malicious' not in self.df.columns:
            print("ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ì–´ì„œ í†µí•© ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # í†µí•© ê²°ê³¼ DataFrame ì¤€ë¹„
        comprehensive_df = self.df.copy()
        
        # LSTM ê²°ê³¼ì™€ ë³‘í•©
        if self.lstm_results is not None:
            lstm_merge_df = self.lstm_results[['package', 'vulnerability_status', 'cwe_label', 'confidence']].rename(columns={
                'vulnerability_status': 'lstm_vulnerability_status',
                'cwe_label': 'lstm_cwe_label', 
                'confidence': 'lstm_confidence'
            })
            
            comprehensive_df = pd.merge(comprehensive_df, lstm_merge_df, 
                                       left_on='name', right_on='package', 
                                       how='left', suffixes=('', '_lstm'))
            
            # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
            if 'package_lstm' in comprehensive_df.columns:
                comprehensive_df = comprehensive_df.drop('package_lstm', axis=1)
        
        # ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ì»¬ëŸ¼ ì´ë¦„ ëª…í™•í™”
        if 'is_malicious' in comprehensive_df.columns:
            comprehensive_df = comprehensive_df.rename(columns={'is_malicious': 'xgboost_prediction'})
        
        # ì¤‘ìš”í•œ ì»¬ëŸ¼ë“¤ì„ ì•ìª½ìœ¼ë¡œ ì¬ë°°ì¹˜
        priority_columns = [
            'name',  # íŒ¨í‚¤ì§€ ì´ë¦„
            'xgboost_prediction',  # XGBoost ìµœì¢… ì˜ˆì¸¡
            'lstm_vulnerability_status',  # LSTM ì·¨ì•½ì  ìƒíƒœ
            'lstm_cwe_label',  # LSTM CWE ë¼ë²¨
            'lstm_confidence',  # LSTM ì‹ ë¢°ë„
            'summary',  # íŒ¨í‚¤ì§€ ì„¤ëª…
            'author',  # ì‘ì„±ì
            'author-email',  # ì‘ì„±ì ì´ë©”ì¼
            'version',  # ë²„ì „
            'download',  # ë‹¤ìš´ë¡œë“œ ìˆ˜
            'download_log',  # ë¡œê·¸ ë³€í™˜ëœ ë‹¤ìš´ë¡œë“œ ìˆ˜
        ]
        
        # ìš°ì„ ìˆœìœ„ ì»¬ëŸ¼ë“¤ì´ ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ ì„ íƒ
        available_priority_cols = [col for col in priority_columns if col in comprehensive_df.columns]
        
        # ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ë“¤
        remaining_cols = [col for col in comprehensive_df.columns if col not in available_priority_cols]
        
        # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬
        final_columns = available_priority_cols + remaining_cols
        comprehensive_df = comprehensive_df[final_columns]
        
        return comprehensive_df
    
    def analyze_package_zip(self, zip_file_path: str, extract_dir: str) -> Dict[str, Any]:
        """ZIP íŒŒì¼ì„ í†µí•œ íŒ¨í‚¤ì§€ ë¶„ì„ (ë©”ì¸ í•¨ìˆ˜)"""
        try:
            print("=== ML íŒ¨í‚¤ì§€ ë¶„ì„ ì‹œì‘ ===")
            start_time = time.time()
            
            # 1. ZIP íŒŒì¼ í•´ì œ ë° ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ
            print("1ï¸âƒ£ ZIP íŒŒì¼ í•´ì œ ë° ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ...")
            source_data = self.extract_zip_and_process_source(zip_file_path, extract_dir)
            if source_data is None:
                return {"error": "ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ ì‹¤íŒ¨"}
            
            # 2. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° íŒŒì‹±
            print("2ï¸âƒ£ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° íŒŒì‹±...")
            meta_data = self.extract_and_parse_metadata(extract_dir)
            if not meta_data:
                return {"error": "ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨"}
            
            # 3. ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬
            print("3ï¸âƒ£ ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬...")
            df = self.preprocess_metadata()
            if df is None:
                return {"error": "ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨"}
            
            # 4. LSTM ì½”ë“œ ë¶„ì„
            print("4ï¸âƒ£ LSTM ì½”ë“œ ë¶„ì„...")
            lstm_results = self.analyze_lstm_codes(source_data)
            if lstm_results is None:
                return {"error": "LSTM ë¶„ì„ ì‹¤íŒ¨"}
            
            # 5. LSTM ê²°ê³¼ í†µí•©
            print("5ï¸âƒ£ LSTM ê²°ê³¼ í†µí•©...")
            if not self.integrate_lstm_results():
                return {"error": "ê²°ê³¼ í†µí•© ì‹¤íŒ¨"}
            
            # 6. XGBoost ì•…ì„± ì˜ˆì¸¡
            print("6ï¸âƒ£ XGBoost ì•…ì„± íŒ¨í‚¤ì§€ ì˜ˆì¸¡...")
            if not self.predict_malicious():
                return {"error": "XGBoost ì˜ˆì¸¡ ì‹¤íŒ¨"}
            
            # 7. í†µí•© ê²°ê³¼ ìƒì„±
            print("7ï¸âƒ£ í†µí•© ë¶„ì„ ê²°ê³¼ ìƒì„±...")
            comprehensive_results = self.generate_comprehensive_results()
            if comprehensive_results is None:
                return {"error": "í†µí•© ê²°ê³¼ ìƒì„± ì‹¤íŒ¨"}
            
            end_time = time.time()
            total_time = end_time - start_time
            
            print(f"\nâœ… ML íŒ¨í‚¤ì§€ ë¶„ì„ ì™„ë£Œ! (ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ)")
            
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            results_list = comprehensive_results.to_dict('records')
            
            return {
                "success": True,
                "total_packages": len(results_list),
                "analysis_time": total_time,
                "results": results_list,
                "summary": {
                    "malicious_packages": sum(1 for r in results_list if r.get('xgboost_prediction', 0) == 1),
                    "vulnerable_packages": sum(1 for r in results_list if r.get('lstm_vulnerability_status') == 'Vulnerable'),
                    "safe_packages": sum(1 for r in results_list if r.get('xgboost_prediction', 0) == 0)
                }
            }
            
        except Exception as e:
            print(f"âŒ ML íŒ¨í‚¤ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if K is not None:
                try:
                    K.clear_session()
                except Exception:
                    pass
    
    def analyze_extracted_files(self, extract_dir: str, extracted_files: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì¶”ì¶œëœ íŒŒì¼ë“¤ì„ í†µí•œ íŒ¨í‚¤ì§€ ë¶„ì„ (ì„œë²„ í†µí•©ìš©)"""
        try:
            print("=== ML íŒ¨í‚¤ì§€ ë¶„ì„ ì‹œì‘ (ì¶”ì¶œëœ íŒŒì¼ ì‚¬ìš©) ===")
            start_time = time.time()
            
            # 1. ì¶”ì¶œëœ íŒŒì¼ë“¤ì—ì„œ ì†ŒìŠ¤ì½”ë“œ ë°ì´í„° ìƒì„±
            print("1ï¸âƒ£ ì¶”ì¶œëœ íŒŒì¼ë“¤ì—ì„œ ì†ŒìŠ¤ì½”ë“œ ë°ì´í„° ìƒì„±...")
            print(f"ğŸ“ ì´ {len(extracted_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
            source_data = []
            for i, file_info in enumerate(extracted_files):
                # file_serviceì—ì„œ ë°˜í™˜í•˜ëŠ” êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
                relative_path = file_info.get('path', '')
                file_name = file_info.get('name', '')
                content = file_info.get('content', '')
                
                if i < 5:  # ì²˜ìŒ 5ê°œ íŒŒì¼ë§Œ ë¡œê·¸ ì¶œë ¥
                    print(f"  ğŸ“„ íŒŒì¼ {i+1}: {relative_path} ({len(content)} chars)")
                
                if file_name.endswith('.py') and content:
                    try:
                        cleaned_code = self.remove_comments(content)
                        if cleaned_code.strip():
                            # íŒ¨í‚¤ì§€ëª… ì¶”ì¶œ (ë””ë ‰í† ë¦¬ êµ¬ì¡°ì—ì„œ)
                            path_parts = Path(relative_path).parts
                            package_name = path_parts[0] if len(path_parts) > 0 else Path(file_name).stem
                            source_data.append({
                                "package_name": package_name,
                                "merged_code": cleaned_code.strip()
                            })
                    except Exception as e:
                        print(f"âš ï¸ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {relative_path}: {e}")
                        continue
            
            print(f"âœ… {len(source_data)}ê°œ íŒ¨í‚¤ì§€ì˜ ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ ì™„ë£Œ")
            if not source_data:
                print("âŒ ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ ì‹¤íŒ¨: ìœ íš¨í•œ Python íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return {"error": "ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ ì‹¤íŒ¨"}
            
            # 2. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° íŒŒì‹±
            print("2ï¸âƒ£ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° íŒŒì‹±...")
            meta_data = self.extract_and_parse_metadata(extract_dir)
            if not meta_data:
                return {"error": "ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨"}
            
            # 3. ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬
            print("3ï¸âƒ£ ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬...")
            df = self.preprocess_metadata()
            if df is None:
                return {"error": "ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨"}
            
            # 4. LSTM ì½”ë“œ ë¶„ì„
            print("4ï¸âƒ£ LSTM ì½”ë“œ ë¶„ì„...")
            lstm_results = self.analyze_lstm_codes(source_data)
            if lstm_results is None:
                return {"error": "LSTM ë¶„ì„ ì‹¤íŒ¨"}
            
            # 5. LSTM ê²°ê³¼ í†µí•©
            print("5ï¸âƒ£ LSTM ê²°ê³¼ í†µí•©...")
            if not self.integrate_lstm_results():
                return {"error": "ê²°ê³¼ í†µí•© ì‹¤íŒ¨"}
            
            # 6. XGBoost ì•…ì„± ì˜ˆì¸¡
            print("6ï¸âƒ£ XGBoost ì•…ì„± íŒ¨í‚¤ì§€ ì˜ˆì¸¡...")
            if not self.predict_malicious():
                return {"error": "XGBoost ì˜ˆì¸¡ ì‹¤íŒ¨"}
            
            # 7. í†µí•© ê²°ê³¼ ìƒì„±
            print("7ï¸âƒ£ í†µí•© ë¶„ì„ ê²°ê³¼ ìƒì„±...")
            comprehensive_results = self.generate_comprehensive_results()
            if comprehensive_results is None:
                return {"error": "í†µí•© ê²°ê³¼ ìƒì„± ì‹¤íŒ¨"}
            
            end_time = time.time()
            total_time = end_time - start_time
            
            # ê²°ê³¼ë¥¼ ì„œë²„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            results_list = []
            for _, row in comprehensive_results.iterrows():
                result_item = {
                    "name": row.get("name", ""),
                    "summary": row.get("summary", ""),
                    "author": row.get("author", ""),
                    "author-email": row.get("author-email", ""),
                    "version": row.get("version", ""),
                    "download": row.get("download", 0),
                    "lstm_vulnerability_status": row.get("lstm_vulnerability_status", "Not Vulnerable"),
                    "lstm_cwe_label": row.get("lstm_cwe_label", "N/A"),
                    "lstm_confidence": row.get("lstm_malicious_probability", 0.0),
                    "xgboost_prediction": int(row.get("xgboost_prediction", 0)),
                    "xgboost_confidence": float(row.get("xgboost_confidence", 0.0)),
                    "final_malicious_status": bool(row.get("xgboost_prediction", 0)),
                    "threat_level": 2 if row.get("xgboost_prediction", 0) == 1 else 0,
                    "analysis_time": 0.0
                }
                results_list.append(result_item)
            
            print(f"âœ… ML ë¶„ì„ ì™„ë£Œ: {len(results_list)}ê°œ íŒ¨í‚¤ì§€ ë¶„ì„")
            
            return {
                "success": True,
                "analysis_time": total_time,
                "total_packages": len(results_list),
                "results": results_list,
                "summary": {
                    "malicious_packages": sum(1 for r in results_list if r.get('xgboost_prediction', 0) == 1),
                    "vulnerable_packages": sum(1 for r in results_list if r.get('lstm_vulnerability_status') == 'Vulnerable'),
                    "safe_packages": sum(1 for r in results_list if r.get('xgboost_prediction', 0) == 0)
                }
            }
            
        except Exception as e:
            print(f"âŒ ML íŒ¨í‚¤ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if K is not None:
                try:
                    K.clear_session()
                except Exception:
                    pass
